---
sidebar_position: 1
---
import useBaseUrl from '@docusaurus/useBaseUrl';

# Motivation

Current data quality frameworks often fall short of providing detailed explanations for specific row or column
data quality issues and are primarily designed for complete datasets, 
making integration into streaming workloads difficult.
They cannot quarantine invalid data and have compatibility issues with Databricks Runtime.

This project introduces a simple but powerful Python validation framework for assessing the data quality of PySpark DataFrames.
It enables real-time quality validation during data processing rather than relying solely on post-factum monitoring.
The validation output includes detailed information on why specific rows and columns have issues, 
allowing for quicker identification and resolution of data quality problems. The framework offers the ability to quarantine invalid data
and investigate quality issues before they escalate.

## What makes DQX unique?

✅ &nbsp;**Ease of use**: DQX is designed to be simple and intuitive, making it easy for various personas — such as data engineers, data scientists, analysts, and business users — to define data quality checks without extensive training.

✅ &nbsp;**Detailed information on data quality issues**: DQX provides granular insights into data quality problems, allowing users to understand the specific reasons behind data quality failures and reduce troubleshooting time to a minimum.

✅ &nbsp;**Streaming support**: DQX is built to work seamlessly with both batch and streaming data, enabling real-time data quality validation.

✅ &nbsp;**Proactive monitoring**: DQX allows you to monitor data quality before writing to the target table, ensuring that only valid data is persisted.

✅ &nbsp;**Custom quality rules**: Users can define their own data quality rules easily, making DQX flexible and adaptable to various data quality requirements and domains.

✅ &nbsp;**Integration with Databricks**: DQX is designed to work seamlessly with Databricks across all engines (Spark Core, Spark Structured Streaming, and Lakeflow Pipelines / DLT) and cluster types (standard and serverless).

## How DQX works

### Option 1: Apply checks and quarantine "bad" data.

Apply checks on the DataFrame and quarantine invalid records to ensure "bad" data is never written to the output.

<div className='bg-gray-100 p-4 rounded-lg'>
 <img src={useBaseUrl('/img/dqx_quarantine.png')} alt="Quarantine" />
</div>

### Option 2: Apply checks and annotate "bad" data.

Apply checks on the DataFrame and annotate invalid records as additional columns.

<div className='bg-gray-100 p-4 rounded-lg'>
 <img src={useBaseUrl('/img/dqx.png')} alt="DQX" />
</div>

### DQX usage in the Lakehouse Architecture

In the Lakehouse architecture, new data validation should happen during data entry into the curated layer to ensure bad data is not propagated to the subsequent layers.
With DQX, you can implement Dead-Letter pattern to quarantine invalid data and re-ingest it after curation to ensure data quality constraints are met.
The data quality can be monitored in real-time between layers, and the quarantine process can be automated.

<div className='bg-gray-100 p-4 rounded-lg'>
 <img src={useBaseUrl('/img/dqx_lakehouse.png')} alt="Lakehouse" />
</div>

## When to use DQX

* Use DQX if you need: pro-active monitoring (before data is written to a target table), streaming support, or custom quality rules.
* For monitoring data quality of already persisted data in Delta tables (post-factum monitoring), try
[Databricks Lakehouse Monitoring](https://docs.databricks.com/en/lakehouse-monitoring/index.html).
* DQX can be integrated with Lakeflow Pipelines (formerly DLT) to check data quality. However, your first choice for Lakeflow Pipelines should be [Expectations](https://docs.databricks.com/en/delta-live-tables/expectations.html#what-are-delta-live-tables-expectations). DQX can be used to profile data and generate Lakeflow Pipeline expectation candidates.

You can integrate DQX with other data transformation frameworks that support PySpark, such as [dbt](https://docs.getdbt.com/docs/build/python-models). This can be integrated by running DQX on tables produced by dbt models or as part of dbt project by running DQX in dbt Python models.
