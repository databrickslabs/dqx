---
sidebar_position: 8
---

import Admonition from '@theme/Admonition';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Summary Metrics

DQX provides comprehensive functionality to capture and store aggregate statistics about your data quality. This allows you to track data quality trends over time, monitor the health of your data pipelines, and gain insights into the overall quality of your datasets.

## Overview

Summary metrics in DQX capture both **built-in metrics** (automatically calculated) and **custom metrics** (user-defined SQL expressions) during data quality checking. These metrics are collected using Spark's built-in [Observation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Observation.html) functionality and can be persisted to tables for historical analysis.

### Built-in Metrics

DQX automatically captures the following built-in metrics for every data quality check execution:

| Metric Name              | Data Type                                       | Description                                        |
|--------------------------|-------------------------------------------------|----------------------------------------------------|
| `input_row_count`        | `int`                                           | Total number of input rows processed               |
| `error_row_count`        | `int`                                           | Number of rows that failed error-level checks      |
| `warning_row_count`      | `int`                                           | Number of rows that triggered warning-level checks |
| `valid_row_count`        | `int`                                           | Number of rows that passed all checks              |

### Custom Metrics

Users can define custom metrics with Spark SQL expressions. These metrics will be collected in addition to DQX's built-in metrics.

<Admonition type="warning" title="Avoid Expensive Operations">
Summary metrics are calculated on all records processed by DQX. Complex aggregations can degrade performance when processing large datasets. Be cautious with operations like `DISTINCT` on high-cardinality columns.
</Admonition>

Example of custom data quality summary metrics:
```sql
sum(array_size(_errors)) as total_errors
avg(array_size(_errors)) as errors_avg
count(case when array_size(_errors) > 1) as count_multiple_errors
```

## Usage Examples

### Accessing Metrics when Applying Checks

Engine methods (e.g. `apply_checks`, `apply_checks_by_metadata`) return a Spark Observation with 1 or more output DataFrames. Data quality metrics can be accessed from the Spark Observation after any action is performed on the output DataFrames.

<Admonition type="info" title="Streaming metrics">
Metrics are not directly accessible from the returned Spark Observation when data is processed with streaming. Use DQX's built-in methods to persist streaming metrics to an output table. See [Writing Metrics to a Table with Streaming](#writing-metrics-to-a-table-with-streaming) for more details.
</Admonition>

<Tabs>
  <TabItem value="Python" label="Python" default>
    ```python
    from databricks.labs.dqx.engine import DQEngine
    from databricks.labs.dqx.metrics_observer import DQMetricsObserver

    # Create observer
    observer = DQMetricsObserver(name="dq_metrics")

    # Create the engine with the optional observer
    engine = DQEngine(WorkspaceClient(), observer=observer)

    # Apply checks and get metrics
    checked_df, observation = engine.apply_checks_by_metadata(df, checks)
    
    # Trigger an action to populate metrics (e.g. count, save to a table)
    # Metrics are only populated if an action is triggered on the DataFrame such as count or saving to a table.
    row_count = checked_df.count()
    
    # Access metrics
    metrics = observation.get
    print(f"Input row count: {metrics['input_row_count']}")
    print(f"Error row count: {metrics['error_row_count']}")
    print(f"Warning row count: {metrics['warning_row_count']}")
    print(f"Valid row count: {metrics['valid_row_count']}")
    ```
  </TabItem>
</Tabs>

### Writing Metrics to a Table

Engine methods (e.g. `apply_checks_and_save_in_table`, `apply_checks_by_metadata_and_save_in_table`, and `save_results_in_table`) can write summary metrics into a table. Metrics can be written to a table in batch or streaming. You can write metrics for different datasets or workloads into a common metrics table to track data quality over time.

#### Writing Metrics to a Table in Batch

Summary metrics can be written to a table when calling `DQEngine` methods to apply checks and write output data. When the input data is read as a batch source, metrics will be collected and written in batch.

<Tabs>
  <TabItem value="Python" label="Python" default>
    ```python
    from databricks.labs.dqx import check_funcs
    from databricks.labs.dqx.engine import DQEngine
    from databricks.labs.dqx.metrics_observer import DQMetricsObserver
    from databricks.labs.dqx.rule import DQRowRule, DQDatasetRule
    from databricks.labs.dqx.config import InputConfig, OutputConfig
    from databricks.sdk import WorkspaceClient

    # Define the checks
    checks = [
      DQRowRule(
        criticality="warn",
        check_func=check_funcs.is_not_null,
        column="col3",
      ),
      DQDatasetRule(
        criticality="error",
        check_func=check_funcs.is_unique,
        columns=["col1", "col2"],
      ),
      DQRowRule(
            name="email_invalid_format",
            criticality="error",
            check_func=check_funcs.regex_match,
            column="email",
            check_func_kwargs={"regex": "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"},
        ),
    ]

    # Create the observer
    observer = DQMetricsObserver(name="dq_metrics")

    # Create the engine with the metrics observer
    engine = DQEngine(WorkspaceClient(), observer=observer)

    # Create the input config for a batch data source
    input_config = InputConfig("main.demo.input_data")

    # Create the output, quarantine, and metrics configs
    output_config = OutputConfig("main.demo.valid_data")
    quarantine_config = OutputConfig("main.demo.quarantine_data")
    metrics_config = OutputConfig("main.demo.metrics_data")

    # Read the data, apply the checks, write data to valid and quarantine tables, and write metrics to the metrics table
    engine.apply_checks_and_save_in_table(
        checks,
        input_config,
        output_config,
        quarantine_config,
        metrics_config
    )
    ```
  </TabItem>
</Tabs>

#### Writing Metrics to a Table with Streaming

Summary metrics can also be written in streaming. When the input data is read as a streaming source, metrics will be written for each streaming micro-batch:

<Tabs>
  <TabItem value="Python" label="Python" default>
    ```python
    from databricks.labs.dqx import check_funcs
    from databricks.labs.dqx.engine import DQEngine
    from databricks.labs.dqx.metrics_observer import DQMetricsObserver
    from databricks.labs.dqx.rule import DQRowRule, DQDatasetRule
    from databricks.labs.dqx.config import InputConfig, OutputConfig
    from databricks.sdk import WorkspaceClient

    # Define the checks
    checks = [
      DQRowRule(
        criticality="warn",
        check_func=check_funcs.is_not_null,
        column="col3",
      ),
      DQDatasetRule(
        criticality="error",
        check_func=check_funcs.is_unique,
        columns=["col1", "col2"],
      ),
      DQRowRule(
            name="email_invalid_format",
            criticality="error",
            check_func=check_funcs.regex_match,
            column="email",
            check_func_kwargs={"regex": "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"},
        ),
    ]

    # Create the observer
    observer = DQMetricsObserver(name="dq_metrics")

    # Create the engine with the metrics observer
    engine = DQEngine(WorkspaceClient(), observer=observer)

    # Create the input config for a streaming data source
    input_config = InputConfig("main.demo.input_data", is_streaming=True)

    # Create the output, quarantine, and metrics configs
    output_config = OutputConfig("main.demo.valid_data")
    quarantine_config = OutputConfig("main.demo.quarantine_data")
    metrics_config = OutputConfig("main.demo.metrics_data")

    # Read the data, apply the checks, write data to valid and quarantine tables, and write metrics to the metrics table
    # Output and quarantine data will be written in streaming and summary metrics will be written for each micro-batch
    engine.apply_checks_and_save_in_table(
        checks,
        input_config,
        output_config,
        quarantine_config,
        metrics_config
    )
    ```
  </TabItem>
</Tabs>

#### Saving Metrics to a Table

Summary metrics can be written to a table when calling `save_results_in_table`. After applying checks, pass the Spark Observation and output DataFrame(s) with the appropriate output configuration.

<Tabs>
  <TabItem value="Python" label="Python" default>
    ```python
    from databricks.labs.dqx import check_funcs
    from databricks.labs.dqx.engine import DQEngine
    from databricks.labs.dqx.metrics_observer import DQMetricsObserver
    from databricks.labs.dqx.rule import DQRowRule, DQDatasetRule
    from databricks.labs.dqx.config import OutputConfig
    from databricks.sdk import WorkspaceClient

    # Define the checks
    checks = [
      DQRowRule(
        criticality="warn",
        check_func=check_funcs.is_not_null,
        column="col3",
      ),
      DQDatasetRule(
        criticality="error",
        check_func=check_funcs.is_unique,
        columns=["col1", "col2"],
      ),
      DQRowRule(
            name="email_invalid_format",
            criticality="error",
            check_func=check_funcs.regex_match,
            column="email",
            check_func_kwargs={"regex": "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"},
        ),
    ]

    # Create the observer
    observer = DQMetricsObserver(name="dq_metrics")

    # Create the engine with the metrics observer
    engine = DQEngine(WorkspaceClient(), observer=observer)

    # Apply checks and get metrics
    valid_df, quarantine_df, observation = engine.apply_checks_by_metadata_and_split(df, checks)

    # Create the output, quarantine, and metrics configs
    output_config = OutputConfig("main.demo.valid_data")
    quarantine_config = OutputConfig("main.demo.quarantine_data")
    metrics_config = OutputConfig("main.demo.metrics_data")

    # Write the data to valid and quarantine tables, and write metrics to the metrics table
    engine.apply_checks_and_save_in_table(
        valid_df,
        quarantine_df,
        observation,
        output_config,
        quarantine_config,
        metrics_config
    )
    ```
  </TabItem>
</Tabs>

### Configuring Custom Metrics

Pass custom metrics as Spark SQL expressions when creating the `DQMetricsObserver`. Custom metrics should be defined as Spark SQL expressions with column aliases and will be accessible by their alias.

<Tabs>
  <TabItem value="Python" label="Python" default>
    ```python
    from databricks.labs.dqx.engine import DQEngine
    from databricks.labs.dqx.metrics_observer import DQMetricsObserver
    from databricks.labs.dqx.config import InputConfig, OutputConfig

    # Define custom metrics
    custom_metrics = [
        "sum(array_size(_errors)) as total_check_errors",
        "sum(array_size(_warnings)) as total_check_warnings",
    ]

    # Create the observer with custom metrics
    observer = DQMetricsObserver(
        name="business_metrics",
        custom_metrics=custom_metrics
    )

    # Create the engine with the optional observer
    engine = DQEngine(WorkspaceClient(), observer=observer)

    # Apply checks and get metrics
    checked_df, observation = engine.apply_checks_by_metadata(df, checks)
    
    # Trigger an action to populate metrics (e.g. count, save to a table)
    # Metrics are only populated if an action is triggered on the DataFrame such as count or saving to a table.
    row_count = checked_df.count()
    
    # Access metrics
    metrics = observation.get
    print(f"Input row count: {metrics['input_row_count']}")
    print(f"Error row count: {metrics['error_row_count']}")
    print(f"Warning row count: {metrics['warning_row_count']}")
    print(f"Valid row count: {metrics['valid_row_count']}")
    print(f"Total check errors: {metrics['total_check_errors']}")
    print(f"Total check warnings: {metrics['total_check_warnings']}")
    ```
  </TabItem>
</Tabs>

## Workflow Integration

### No-Code Approach (Workflows)

When using DQX workflows, summary metrics are automatically configured based on your installation [configuration file](/docs/installation/#configuration-file):

1. **Installation Configuration**: During installation, specify metrics table and custom metrics
2. **Automatic Observer Creation**: Workflows automatically create `DQMetricsObserver` when metrics are configured
3. **Metrics Persistence**: Metrics are automatically saved to the configured table after each workflow run

```bash
# Run quality checker workflow with metrics enabled
databricks labs dqx apply-checks --run-config "production"

# Run end-to-end workflow with metrics enabled  
databricks labs dqx e2e --run-config "production"
```

#### Configuration File Example

Metrics will be defined in the `metrics_config` section of your configuration file.

```yaml
run_configs:
- name: production
  input_config:
    location: main.raw.sales_data
    format: delta
  output_config:
    location: main.clean.sales_data
    format: delta
    mode: append
  quarantine_config:
    location: main.quarantine.sales_data
    format: delta
    mode: append
  metrics_config:  # Summary metrics configuration
    location: main.analytics.dq_metrics
    format: delta
    mode: append
  checks_location: main.config.quality_checks
  
# Global custom metrics (applied to all run configs)
custom_metrics:
  - "avg(amount) as average_transaction_amount"
  - "sum(case when region = 'US' then amount else 0 end) as us_revenue"
  - "count(distinct customer_id) as unique_customers"
```

## Metrics Table Schema

Metrics can be written automatically and centralized by specifying a metrics table. The metrics table will contain the following fields:

| Column Name     | Column Type           | Description                                               |
|-----------------|-----------------------|-----------------------------------------------------------|
| `run_ts`        | `TIMESTAMP`           | Run timestamp when the summary metrics were calculated    |
| `input_table`   | `STRING`              | Location of the input dataset                             |
| `metric_key`    | `STRING`              | Name of the metric                                        |
| `metric_value`  | `STRING`              | Value of the metric (as a string)                         |
| `user_metadata` | `MAP[STRING, STRING]` | User-defined, run-level metadata                          |

## Best Practices

### Performance Considerations

1. **Batch Metrics Collection**: Collect metrics during regular data processing after data is written
2. **Monitor Metrics Overhead**: Complex custom metrics may impact processing performance

### Monitoring and Alerting

1. **Track Trends**: Monitor metrics over time to identify data quality degradation
2. **Set Thresholds**: Establish acceptable ranges for error rates and warning counts
3. **Alert on Anomalies**: Set up alerts when metrics deviate significantly from historical patterns, e.g. by using Databricks SQL Alerts

The example query below shows how you can analyze metrics persisted to a table.
```sql
/* EXAMPLE: Identify quality degradation */
SELECT 
  date_trunc('day', run_ts) as run_date,
  avg(error_count * 100.0 / input_count) as avg_error_rate,
  avg(warning_count * 100.0 / input_count) as avg_warning_rate
FROM 
   main.analytics.dq_metrics 
WHERE 
   run_date >= current_date - INTERVAL 30 DAYS
GROUP BY 
   date_trunc('day', run_date)
ORDER BY 
   run_date DESC
```
