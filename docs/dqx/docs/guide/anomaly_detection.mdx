---
sidebar_position: 15
---

import Admonition from '@theme/Admonition';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Anomaly Detection (ML-based)

Detect unusual rows by learning what "normal" looks like across multiple columns, then flag rows that deviate.

## What it is (and why it helps)

Anomaly detection learns typical patterns from recent "good" data, then highlights rows that look unusual when you consider multiple columns together.
This complements rule-based checks:
- Rules catch the issues you can anticipate and describe.
- Anomaly detection surfaces unusual patterns you did not think to write rules for.

Example: amount, quantity, and discount are all valid individually, but the combination is rare for your data. Anomaly detection can flag that row.

## Why teams use it

It finds issues that look "valid" to rules but are still suspicious when you look at the full row. This is especially useful for:
- unexpected spikes or dips that are hard to encode as a rule
- unusual combinations across columns
- silent pipeline changes that shift behavior without breaking schemas

In practice, it helps reduce blind spots and speeds up triage because you can focus on the top‑ranked unusual rows first.

## Complements Databricks data quality monitoring

Databricks data quality monitoring focuses on table-level signals like freshness and completeness. DQX anomaly detection focuses on unusual rows and cross-column patterns.
Use both together to cover:
- **Data availability**: Is new data arriving as expected?
- **Data content quality**: Are the rows themselves unusual or inconsistent?

Example: monitoring tells you new data arrived as expected; DQX flags a spike in unusual orders inside it.

<Admonition type="tip" title="Use both together">
Use Databricks data quality monitoring for table health without looking inside the actual data (freshness/completeness) and DQX anomaly detection for discovering anomalies in the data.
</Admonition>

## What you get out of the box

Each row is scored and enriched with:
- **Severity percentile (0–100)**: how unusual the row is compared to training data.
- **Anomaly flag**: whether it crosses your chosen threshold.
- **Top contributors**: which fields most influenced the anomaly score.

## Prerequisites

Install DQX with anomaly support:

```bash
pip install 'databricks-labs-dqx[anomaly]'
```

## Quick start

Train a model on recent "good" data, then use it in checks:

<Admonition type="tip" title="Exclude identifier columns">
Avoid ID‑like columns (e.g., `order_id`, `user_id`) in anomaly training. IDs often look unique and can drown out real
patterns, which makes the model less useful. If you need to keep them in your table, use `exclude_columns` or provide
an explicit `columns` list of behavioral fields (amounts, counts, rates, timestamps).
</Admonition>

<Tabs>
  <TabItem value="Python" label="Python" default>
    ```python
    from databricks.labs.dqx.anomaly import AnomalyEngine, has_no_anomalies
    from databricks.labs.dqx.engine import DQEngine
    from databricks.sdk import WorkspaceClient

    ws = WorkspaceClient()
    anomaly_engine = AnomalyEngine(ws)

    model_name = anomaly_engine.train(
        df=spark.table("catalog.schema.orders"),
        model_name="catalog.schema.orders_monitor",
        columns=["amount", "quantity", "discount"],  # optional; omit to use all supported columns
        registry_table="catalog.schema.dqx_anomaly_models",
    )

    checks = [
        has_no_anomalies(
            model=model_name,
            registry_table="catalog.schema.dqx_anomaly_models",
        )
    ]

    dq_engine = DQEngine(ws, spark)
    scored_df = dq_engine.apply_checks(spark.table("catalog.schema.orders"), checks)
    ```
  </TabItem>
  <TabItem value="Workflows" label="Workflows">
    Configure anomaly training per run config:

    ```yaml
    run_configs:
      - name: orders
        input_config:
          location: catalog.schema.orders
        anomaly_config:
          columns: [amount, quantity, discount]  # optional; omit to use all supported columns
          model_name: catalog.schema.orders_monitor
          registry_table: catalog.schema.dqx_anomaly_models
    ```

    Define checks separately (YAML or Python). See [Anomaly Detection in Quality Checks](/docs/reference/quality_checks#anomaly-detection).
  </TabItem>
</Tabs>

## Investigate anomalies

Anomaly checks add results to `_errors`/`_warnings` and `_dq_info` with structured metadata. Feature contributions are enabled by default; disable them for faster scoring:

```python
has_no_anomalies(
    model=model_name,
    registry_table="catalog.schema.dqx_anomaly_models",
    include_contributions=False,
)
```

To review the most unusual rows:

```python
import pyspark.sql.functions as F

top = scored_df.orderBy(F.col("_dq_info.anomaly.score").desc()).limit(20)
top.select("order_id", "_dq_info.anomaly.score", "_dq_info.anomaly.contributions").show(truncate=False)
```

Scores are normalized into `severity_percentile` (0–100). The anomaly threshold is a percentile cutoff (default 95) that you should tune to your data and alert tolerance.

## How to choose a threshold

Think of the threshold as an alert volume control:
- **Lower** = more alerts (more sensitive).
- **Higher** = fewer alerts (more strict).

Start with the default (95), then tune based on how many rows you can reasonably review and how costly misses are.

## Practical examples (non‑technical)

- A sudden surge of high‑value orders in a region that usually has low spend.
- Orders placed at unusual hours for a business that operates during the day.
- A big jump in quantity for a category that normally sells in small units.

## When to retrain

Retrain when "normal" changes: seasonality, new pricing rules, new products, or major pipeline changes. A simple cadence (monthly or quarterly) is often enough.

If you want to quarantine anomalies for investigation, use DQX output/quarantine patterns (see [Applying Checks](/docs/guide/quality_checks_apply)).

<Admonition type="tip" title="When to use anomaly detection">
Use anomaly detection when you want to catch unusual combinations across columns that are hard to capture with static rules.
</Admonition>

## Next steps

For training options, parameters, and YAML examples, see [Anomaly Detection in Quality Checks](/docs/reference/quality_checks#anomaly-detection).
