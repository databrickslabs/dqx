---
sidebar_position: 15
---

# Anomaly Detection (ML-based)

Detect multivariate anomalies by training a model on "normal" data, then flagging rows that deviate. Current implementation uses [scikit-learn](https://scikit-learn.org/) IsolationForest (BSD license) with distributed Spark scoring via pandas UDFs; future releases can add more algorithms behind the same interface.

## Complements Databricks native monitoring

DQX's anomaly detection **works alongside** [Databricks native anomaly detection](https://learn.microsoft.com/en-gb/azure/databricks/data-quality-monitoring/anomaly-detection/) for comprehensive monitoring:

| **Feature** | **Databricks Native** | **DQX Anomaly Detection** |
|-------------|----------------------|---------------------------|
| **What it monitors** | Table freshness + row count completeness | Row-level data quality patterns |
| **Granularity** | Schema-level (all tables) | Table-specific (per-column groups) |
| **Questions answered** | "Did my data arrive on time?" | "Are these rows valid?" |
| **Setup** | Enable once per schema | Train model per table + columns |

**Together, they provide**:
- **Pipeline health** (Databricks native): Catch late updates, missing batches
- **Content quality** (DQX): Catch suspicious individual records, multivariate anomalies

**Recommended workflow**:
1. Enable Databricks native monitoring on your schema (automatic)
2. Add DQX anomaly checks for critical tables (model-based)
3. Use both signals: late data alerts + row-level anomalies

## Auto-Discovery (Recommended)

DQX can automatically identify which columns and segments benefit from anomaly detection:

**Python** (zero configuration):
```python
from databricks.labs.dqx.anomaly import AnomalyEngine
from databricks.sdk import WorkspaceClient

anomaly_engine = AnomalyEngine(WorkspaceClient())

# One-liner: auto-discovers everything
anomaly_engine.train(df=spark.table("catalog.schema.orders"), model_name="orders_monitor")

# Prints:
# "Auto-selected 3 columns: [amount, quantity, discount]"
# "Auto-detected 1 segment column: [region] (3 total segments)"
# "Training segment 1/3: region=US"
# "Training segment 2/3: region=EU"
# "Training segment 3/3: region=APAC"
```

**YAML** (workflow integration):
```yaml
run_configs:
  - name: orders
    input_config:
      location: catalog.schema.orders
    anomaly_config: {}  # Empty = auto-discovery
```

### Two Discovery Modes

DQX offers two auto-discovery modes:

**1. Heuristic Mode** (default - analyzes DataFrame on-the-fly):
```python
from databricks.labs.dqx.anomaly import AnomalyEngine
from databricks.sdk import WorkspaceClient

anomaly_engine = AnomalyEngine(WorkspaceClient())
# Computes statistics during training
anomaly_engine.train(df=spark.table("orders"), model_name="orders_monitor")
```
- **When to use**: Small-to-medium datasets, first-time exploration
- **Performance**: Analyzes schema and computes statistics during training
- **Benefit**: No setup required, works immediately

**2. Profiler Mode** (reuses DQX profiler results):
```python
# First, run DQX profiler once (optional, for large datasets)
from databricks.labs.dqx.profiler.profiler import DQProfiler
from databricks.sdk import WorkspaceClient

profiler = DQProfiler(WorkspaceClient())
summary_stats, dq_rules = profiler.profile(df)
# Save profiler output to a table for reuse
spark.createDataFrame([summary_stats]).write.mode("overwrite").saveAsTable("catalog.schema.profiler_output")

# Then, anomaly training reuses cached statistics
from databricks.labs.dqx.anomaly import AnomalyEngine

anomaly_engine = AnomalyEngine(WorkspaceClient())
anomaly_engine.train(
    df=df,
    model_name="orders_monitor",
    profiler_table="catalog.schema.profiler_output"  # Faster!
)
```
- **When to use**: Large datasets (>1B rows), multiple model training runs
- **Performance**: Reuses pre-computed statistics (no re-computation)
- **Benefit**: Consistent stats across models, faster discovery

**Both modes support the same multi-type discovery:**
- Numeric, categorical, datetime, boolean columns
- Priority-based selection (numeric first, then boolean, categorical, datetime)
- Max 10 columns with intelligent ranking

**What gets auto-discovered**:
- **Columns** (max 10, prioritized by type):
  1. Numeric columns with variance (stddev > 0, null_rate < 50%)
  2. Boolean columns (null_rate < 50%)
  3. Low-cardinality categorical (â‰¤20 values, null_rate < 50%)
  4. Datetime columns (null_rate < 50%)
  5. High-cardinality categorical (21-100 values, null_rate < 50%)
- **Segments**: Categorical columns with 2-50 distinct values (null_rate < 10%)
- **Exclusions**: ID patterns (columns ending in "id", "key", "uuid", or "guid"), arrays, maps, structs
- **Naming**: Automatic based on table and selected columns

**Manual overrides available** for advanced users (see Column Selection Best Practices below).

## Column Selection Best Practices

### Avoiding ID Fields

**Why ID fields are problematic:**

ID columns (`user_id`, `transaction_id`, `session_uuid`) have high cardinality and no meaningful patterns, leading to:
- **Model overfitting**: Memorizes specific IDs instead of learning behavioral patterns
- **Poor generalization**: New IDs at inference time are always flagged as anomalies
- **Degraded model quality**: Random IDs introduce noise, reducing detection accuracy

**Auto-discovery automatically excludes** columns matching these patterns (case-insensitive):
- Ending in `id`, `key`, `uuid`, `guid`
- Examples: `user_id`, `customer_id`, `session_key`, `request_uuid`, `document_guid`

### Using exclude_columns

When explicitly selecting columns, use `exclude_columns` to filter out unwanted columns:

```python
from databricks.labs.dqx.anomaly import AnomalyEngine
from databricks.sdk import WorkspaceClient

anomaly_engine = AnomalyEngine(WorkspaceClient())

# Recommended: Exclude IDs and other non-behavioral columns
anomaly_engine.train(
    df=spark.table("catalog.schema.orders"),
    model_name="orders_monitor",
    columns=["amount", "quantity", "discount"],  # Behavioral columns only
    exclude_columns=["user_id", "order_id", "session_uuid"]  # Filter out IDs
)

# Or let auto-discovery handle it (simplest)
anomaly_engine.train(
    df=spark.table("orders"),
    model_name="orders_auto",
    exclude_columns=["user_id", "ground_truth_label"]  # Exclude specific columns from discovery
)
```

**Benefits of exclude_columns**:
- Works with both auto-discovery and explicit column selection
- Prevents accidental overlap (raises error if column in both lists)
- Clearer intent than manually listing all desired columns

### Validation Warnings

If you explicitly provide ID-like columns, DQX issues warnings to help catch potential issues:

```python
anomaly_engine.train(df, model_name="test", columns=["user_id", "amount"])  # âš ï¸ Warning issued

# UserWarning: Column 'user_id' appears to be an ID field (matches pattern 
# '(?i)(id|key|uuid|guid)$' and has very high cardinality (>80% unique values 
# or >1000 distinct values)). ID fields can lead to poor anomaly detection models 
# due to overfitting. Consider using exclude_columns=['user_id'] or removing 
# from columns list.
```

**Detection criteria** - DQX warns about columns that:
1. Match ID naming patterns: `(?i)(id|key|uuid|guid)$`
2. Have high cardinality: >1000 distinct values OR >80% unique values

**Good vs. Bad column choices:**
- âœ… **Good**: `transaction_amount`, `click_count`, `session_duration`, `order_quantity`, `discount_rate`
- âŒ **Bad**: `user_id`, `session_uuid`, `customer_key`, `transaction_id`, `record_guid`

### Example: Full Workflow

```python
from databricks.labs.dqx.anomaly import AnomalyEngine, has_no_anomalies
from databricks.sdk import WorkspaceClient

anomaly_engine = AnomalyEngine(WorkspaceClient())

# Train with proper column selection
model_name = anomaly_engine.train(
    df=spark.table("catalog.schema.transactions"),
    model_name="transaction_monitor",
    columns=["amount", "quantity", "discount", "shipping_cost"],  # Behavioral
    exclude_columns=["transaction_id", "user_id", "session_uuid"],  # IDs
    segment_by=["region"],  # Optional segmentation
)

# Use in checks
checks = [
    has_no_anomalies(
        merge_columns=["transaction_id"],  # Use ID for merging, not for modeling
        model=model_name,  # Use the model we just trained
        columns=["amount", "quantity", "discount", "shipping_cost"],
        score_threshold=0.5,
    )
]
```

## Segment-Based Monitoring

Detect anomalies specific to regions, categories, or other groupings:

**Automatic** (discovers segments from data):
```python
from databricks.labs.dqx.anomaly import AnomalyEngine
from databricks.sdk import WorkspaceClient

anomaly_engine = AnomalyEngine(WorkspaceClient())

# Auto-detects region, product_category as segments
anomaly_engine.train(df=spark.table("orders"), model_name="orders_monitor")
```

**Explicit** (specify segments):
```python
anomaly_engine.train(
    df=spark.table("orders"),
    model_name="regional_monitor",
    columns=["amount", "quantity"],
    segment_by=["region"]  # Train separate model per region
)

# Scoring automatically uses correct regional model
from databricks.labs.dqx.anomaly import has_no_anomalies

checks = [
    has_no_anomalies(
        merge_columns=["order_id"],
        model="regional_monitor",  # DQX infers segment_by and columns from model
    )
]
```

**Use cases**:
- Multi-regional data with different baselines
- Product categories with distinct patterns
- Multi-tenant monitoring without cross-contamination

## When to use DQX anomaly detection

- You need to catch rows that look plausible per-column but are unlikely together (e.g., high amount + large discount + low quantity).
- You want the model to adapt to recent "normal" patterns instead of fixed thresholds.
- You need a row-level anomaly score for quarantine/reporting/dashboards.
- Your validation logic requires multivariate patterns that simple rules can't capture.

When NOT to use
- If a simple rule covers it (e.g., `amount > 10000`), prefer rules for clarity.
- If data is extremely small or highly noisy, results may be unstable; consider simpler checks.
- For table-level monitoring (freshness/completeness), use Databricks native instead.

## How it works

1. Train on recent â€œgoodâ€ data to learn typical ranges and combinations.
2. Score new rows; higher scores = more anomalous.
3. `has_no_anomalies` flags rows whose score exceeds your threshold (default 0.5).

## Quick start (Python)

```python
from databricks.labs.dqx.anomaly import AnomalyEngine, has_no_anomalies
from databricks.sdk import WorkspaceClient

anomaly_engine = AnomalyEngine(WorkspaceClient())

# Train with explicit columns (exclude IDs)
anomaly_engine.train(
    df=spark.table("catalog.schema.orders"),
    model_name="orders_monitor",
    columns=["amount", "quantity", "shipping_cost"],
    exclude_columns=["order_id", "user_id"],  # Exclude ID fields
)

# Use in checks
checks = [
    has_no_anomalies(
        merge_columns=["order_id"],
        model="orders_monitor",  # Use the trained model
        columns=["amount", "quantity", "shipping_cost"],
        score_threshold=0.5,  # higher = more anomalous
    )
]
```

## YAML example

```yaml
# config.yml
run_configs:
  - name: orders
    input_config:
      location: catalog.schema.orders
    anomaly_config:
      columns: [amount, quantity, shipping_cost]
      # optional overrides:
      # params:
      #   sample_fraction: 0.3
      #   max_rows: 1000000
      #   train_ratio: 0.8
      #   algorithm_config:
      #     contamination: 0.1
      #     num_trees: 200

# checks.yml
- criticality: warn
  check:
    function: has_no_anomalies
    arguments:
      merge_columns: [order_id]
      model: orders_monitor
      columns: [amount, quantity, shipping_cost]
      score_threshold: 0.5
```

## Threshold guidance
| Threshold | Use case |
|-----------|----------|
| 0.3 | Aggressive: catch more, risk higher false positives |
| 0.5 | Balanced (default) |
| 0.7 | Conservative |
| 0.9 | Very conservative |

## Practical scenarios

- Transactions: unusual amount + discount + quantity combinations.
- Logistics: weight and dimensions that donâ€™t align with historical norms.
- KPIs: correlated metrics diverging (e.g., requests vs. errors) even if each is in-range.
- Sensor data: multivariate drift (e.g., temp + pressure + flow) even if each is within a threshold.

## Supported Column Types

DQX anomaly detection now supports **multiple data types** with automatic feature engineering:

| **Type** | **Examples** | **Feature Engineering** | **Features per Column** |
|----------|--------------|-------------------------|-------------------------|
| **Numeric** | int, long, float, double, decimal | None (use as-is) | 1 |
| **Categorical** | string | OneHot (low card) or Frequency (high card) | 1-20 |
| **Datetime** | date, timestamp | Cyclical encoding (hour, day-of-week, weekend) | 5 |
| **Boolean** | boolean | Map to 0/1 | 1 |
| **Unsupported** | array, map, struct, binary | Skip with warning | - |

**Limits** (ensures optimal Isolation Forest performance):
- **Max 10 input columns** (before feature engineering)
- **Max 50 engineered features** (after encoding)
- Violation raises error with actionable suggestions

### Categorical Columns

**Low cardinality** (â‰¤20 distinct values): OneHot encoding
```python
# Example: product_type with ["Electronics", "Clothing", "Food"]
anomaly_engine.train(df=df, model_name="products", columns=["amount", "product_type"])
# Creates: amount, product_type_Electronics, product_type_Clothing, product_type_Food
```

**High cardinality** (>20 distinct values): Frequency encoding
```python
# Example: category with 500 distinct values
anomaly_engine.train(df=df, model_name="categories", columns=["amount", "category"])
# Creates: amount, category_freq (frequency ratio: count/total)
```

**Unknown categories** during scoring: Handled gracefully
- OneHot: All indicator columns set to 0
- Frequency: Uses global frequency or 0
- Warning if >10% of rows have unknowns

### Datetime Columns

**Cyclical encoding** preserves temporal relationships:
```python
# Example: order_timestamp
anomaly_engine.train(df=df, model_name="orders", columns=["amount", "order_timestamp"])
# Creates 5 features:
#   - order_timestamp_hour_sin, order_timestamp_hour_cos (daily cycle)
#   - order_timestamp_dow_sin, order_timestamp_dow_cos (weekly cycle)
#   - order_timestamp_is_weekend (0/1)
```

**Why cyclical?** Hour 23 is close to hour 0 in real-world patterns. Sin/cos encoding preserves this distance.

**Use cases**:
- Detect unusual transaction times (e.g., large transfers at 3am)
- Capture weekday vs. weekend behavior differences
- Identify time-based fraud patterns

### Boolean Columns

**Simple 0/1 mapping**:
```python
# Example: is_premium, is_discounted
anomaly_engine.train(df=df, model_name="flags", columns=["amount", "is_premium", "is_discounted"])
# Creates: amount, is_premium (0/1), is_discounted (0/1)
```

### Null Handling

**Strategy**: Null indicators + constant imputation

For every column with nulls, DQX creates:
1. **Original column** (imputed with type-specific constant)
2. **Null indicator**: `{column}_is_null` (binary 0/1)

**Imputation by type**:
- **Numeric**: Replace with 0
- **Categorical**: Replace with "MISSING" string
- **Datetime**: Replace with dataset minimum date
- **Boolean**: Replace with 0 (False)

**Example**:
```python
# Input: amount column with nulls
# Output: amount (imputed with 0), amount_is_null (0 or 1)
```

**High null rate** (>50%): Warning issued but column is included
```
UserWarning: Column 'optional_field' has 73% nulls. Null indicator feature added.
```

**Rationale**: Null patterns themselves can be anomalous (e.g., missing sensor readings indicate hardware failure).

### Mixed Type Example

```python
from databricks.labs.dqx.anomaly import AnomalyEngine
from databricks.sdk import WorkspaceClient

anomaly_engine = AnomalyEngine(WorkspaceClient())

# Train with all supported types
anomaly_engine.train(
    df=spark.table("catalog.schema.transactions"),
    model_name="transactions_monitor",
    columns=[
        "amount",           # numeric (1 feature)
        "quantity",         # numeric (1 feature)
        "product_type",     # categorical, 5 values (5 features via one-hot)
        "is_premium",       # boolean (1 feature)
        "order_time",       # datetime (5 cyclical features)
    ]
    # Total: ~13 features (well within 50 limit)
)
```

### Feature Count Planning

**Estimate before training**:
```python
# Feature count = 
#   num_numeric * 1 
#   + num_boolean * 1
#   + num_datetime * 5
#   + sum(cardinalities for low-card categoricals)
#   + num_high_card_categorical * 1
#   + num_columns_with_nulls * 1

# Example:
# 3 numeric â†’ 3
# 1 boolean â†’ 1
# 2 datetime â†’ 10
# 1 categorical (8 values) â†’ 8
# All have nulls â†’ 7
# Total: 29 features âœ“ (under 50 limit)
```

**Error if exceeded**:
```
InvalidParameterError: Feature engineering would create 58 features (limit: 50).
Feature breakdown:
  - 3 datetime â†’ 15 features
  - 2 categorical â†’ 30 features
  - 3 numeric â†’ 3 features
  ...
Suggestions:
  1. Reduce number of categorical columns (highest impact)
  2. Use columns with lower cardinality
  3. Prioritize numeric/boolean columns (1 feature each)
```

## Data prep and performance

- Supports numeric, categorical (string), datetime, boolean columns with automatic feature engineering.
- Sampling defaults: 30% up to 1M rows; auto warnings if capped. Override via `params` only if needed.
- Train/validation split: default 80/20; basic score distribution metrics are logged to the registry.
- Large datasets: training on driver (efficient for sampled data â‰¤1M rows); scoring fully distributed via pandas UDFs.

## Model lifecycle

- **Model storage**: Models are stored in **MLflow Model Registry** with versioning support. Each training run creates a new version.
- **Registry table**: Auto-created Delta table stores metadata (metrics, hyperparameters, feature engineering info, baseline statistics). Auto-derived from catalog.schema if not provided (default: `dqx_anomaly_models`).
- **Model naming**: `model_name` is **required** (e.g., `'field_force_anomaly'`). Can be simple name or full path (`'catalog.schema.my_model'`).
- **Retraining**: Retrain periodically (workflow provided) to adapt to drift; warning issued if model is >30 days old.

## Model performance and metrics

After training, comprehensive validation metrics are stored in the registry:

**View metrics**:
```sql
SELECT 
    model_name, 
    metrics['recommended_threshold'] as recommended_threshold,
    metrics['threshold_50_precision'] as precision_at_50,
    metrics['threshold_50_recall'] as recall_at_50,
    metrics['threshold_50_f1'] as f1_at_50,
    metrics['estimated_contamination'] as est_contamination,
    feature_importance,
    training_time
FROM catalog.schema.dqx_anomaly_models
WHERE status = 'active'
ORDER BY training_time DESC
```

**Metrics included**:
- Precision, recall, F1 at thresholds 0.3, 0.5, 0.7, 0.9
- **Recommended threshold**: F1-optimal balance point
- Distribution statistics: mean, std, skewness, percentiles
- Estimated contamination rate in validation data

**Feature importance**: Global importance scores show which columns matter most for anomaly detection across all rows.

## Threshold selection

Use the `recommended_threshold` from registry metrics as a starting point:

```python
from databricks.labs.dqx.anomaly import has_no_anomalies

# Query recommended threshold from registry
recommended = spark.sql("""
    SELECT metrics['recommended_threshold'] as threshold
    FROM catalog.schema.dqx_anomaly_models
    WHERE model_name = 'orders__abc123__anomaly' AND status = 'active'
""").first()['threshold']

# Use in checks
checks = [
    has_no_anomalies(
        merge_columns=["order_id"],
        model="orders__abc123__anomaly",
        columns=["amount", "quantity", "shipping_cost"],
        score_threshold=recommended,
    )
]
```

The recommended threshold balances precision and recall based on validation data. Adjust up (more conservative) or down (more aggressive) based on business impact of false positives vs. false negatives.

## Explainability (feature contributions)

Understand which columns drove each anomaly score using `include_contributions`:

```python
from databricks.labs.dqx.anomaly import has_no_anomalies

checks = [
    has_no_anomalies(
        merge_columns=["order_id"],
        model="my_model",
        columns=["amount", "quantity", "discount"],
        score_threshold=0.5,
        include_contributions=True,  # adds contributions to _info.anomaly.contributions
    )
]
```

**Output example**:
```
| amount | quantity | discount | _info                                                |
|--------|----------|----------|------------------------------------------------------|
| 9999   | 1        | 0.95     | {anomaly: {score: 0.87, is_anomaly: true, ...}}     |
```

**Access anomaly metadata**:
```python
from pyspark.sql import functions as F

# Access score and contributions via _info column
df_scored.select(
    "amount", 
    "quantity",
    "_info.anomaly.score",
    "_info.anomaly.is_anomaly",
    "_info.anomaly.contributions"
)

# Filter anomalies
df_scored.filter(F.col("_info.anomaly.is_anomaly"))
```

Top contributors are shown as percentages summing to 1.0. Use this to:
- Investigate why specific rows were flagged
- Validate the model learned correct patterns
- Enrich quarantine data for downstream review

### Understanding Feature Names in Contributions

Contributions use **engineered feature names** rather than original column names. This provides more granular insight into *what aspect* of a column drove the anomaly.

**Feature naming patterns:**

| Pattern | Original Type | Meaning |
|---------|---------------|---------|
| `{col}` | numeric | Original numeric value |
| `{col}_hour_sin`, `{col}_hour_cos` | datetime | Time-of-day pattern (cyclical: hour 23 â‰ˆ hour 0) |
| `{col}_dow_sin`, `{col}_dow_cos` | datetime | Day-of-week pattern (cyclical: Sunday â‰ˆ Monday) |
| `{col}_month_sin`, `{col}_month_cos` | datetime | Seasonal/monthly pattern |
| `{col}_is_weekend` | datetime | Saturday/Sunday vs weekday (0 or 1) |
| `{col}_{value}` | categorical | OneHot indicator for specific category value |
| `{col}_freq` | categorical | Frequency encoding (how common this value is) |
| `{col}_bool` | boolean | Boolean mapped to 0/1 |
| `{col}_is_null` | any | Missing value indicator (1 if null, 0 otherwise) |

**Example interpretation:**

```
contributions: {
  "amount": 0.02,
  "date_is_weekend": 0.38,
  "date_hour_cos": 0.28,
  "date_dow_cos": 0.16,
  ...
}
```

Reading this: "38% of this anomaly is explained by **weekend timing**, 28% by **hour of day**, and only 2% by the **amount** itself." This suggests the transaction occurred at an unusual time (e.g., large transfer on Saturday at 3am) rather than having an unusual amount.

**Aggregating to original columns:**

To see total contribution per original column, sum related features:
```python
# Total date contribution = is_weekend + hour_sin + hour_cos + dow_sin + dow_cos + month_sin + month_cos
date_total = 0.38 + 0.28 + 0.07 + 0.16 + 0.02 + 0.04 + 0.02  # = 0.97 (97%)
```

**Why show engineered features?**

Engineered feature names reveal *which aspect* of a column matters:
- High `date_is_weekend` â†’ weekend timing is unusual
- High `date_hour_sin/cos` â†’ hour of day is unusual
- High `category_Electronics` â†’ being in Electronics category is unusual for this combination

This granularity helps identify root causes more precisely than aggregated column-level contributions.

## Performance optimization

When using `row_filter` to score only a subset of rows, DQX joins the scored results back to preserve all original rows. For better performance with wide DataFrames, specify `merge_columns`:

```python
from databricks.labs.dqx.anomaly import has_no_anomalies

checks = [
    has_no_anomalies(
        merge_columns=["order_id"],  # Use PK for fast join instead of all 50 columns
        model="my_model",
        columns=["amount", "quantity", "discount"],
        score_threshold=0.5,
        row_filter="is_high_value = true",  # Score only high-value transactions
    )
]
```

**Performance tips**:
- Provide primary key columns in `merge_columns` when using `row_filter`
- Without `merge_columns`: joins on all DataFrame columns (safe but slower for wide tables)
- With `merge_columns`: faster join, but columns must form a unique key to avoid duplicates
- Similar to `sql_query` function's `merge_columns` pattern

## Drift detection and retraining

Drift detection warns when current data distribution significantly differs from training data:

```python
from databricks.labs.dqx.anomaly import has_no_anomalies

checks = [
    has_no_anomalies(
        merge_columns=["order_id"],
        model="my_model",
        columns=["amount", "quantity"],
        drift_threshold=3.0,  # Z-score threshold (default)
    )
]
```

**When drift detected**, you'll see a warning like:
```
UserWarning: Data drift detected in columns: amount, quantity (drift score: 4.2).
Model may be stale. Consider retraining with AnomalyEngine.train().
```

**Workflow for handling drift**:
1. Enable drift detection in production checks
2. Monitor warnings in check results or logs
3. When drift exceeds threshold, retrain the model using the provided command
4. New model automatically replaces old one in registry (old archived)

**Drift metrics**:
- Z-score for mean shift
- Relative change in standard deviation
- Overall drift score = max across all columns

Set `drift_threshold=None` to disable drift detection.

## Temporal data (seasonal patterns)

For time-series data with seasonal patterns, extract temporal features before training:

```python
from databricks.labs.dqx.anomaly import AnomalyEngine
from databricks.labs.dqx.anomaly.temporal import extract_temporal_features
from databricks.sdk import WorkspaceClient

anomaly_engine = AnomalyEngine(WorkspaceClient())

# Extract time-based features
df_with_temporal = extract_temporal_features(
    df,
    timestamp_column="event_time",
    features=["hour", "day_of_week", "month"]
)

# Train on original + temporal features
anomaly_engine.train(
    df=df_with_temporal,
    model_name="temporal_monitor",
    columns=[
        "amount", "quantity", "shipping_cost",
        "temporal_hour", "temporal_day_of_week", "temporal_month"
    ],
)
```

**Use cases**:
- Detect anomalies that depend on time-of-day (e.g., unusual traffic at 3am)
- Capture day-of-week patterns (weekday vs. weekend behavior)
- Account for seasonal variations (monthly/quarterly cycles)

**Available temporal features**:
- `hour`: Hour of day (0-23)
- `day_of_week`: Day of week (1=Monday, 7=Sunday)
- `day_of_month`: Day of month (1-31)
- `month`: Month (1-12)
- `quarter`: Quarter (1-4)
- `week_of_year`: Week of year (1-53)
- `is_weekend`: Boolean for Saturday/Sunday (0.0 or 1.0)

## Quarantine workflow

Anomaly detection integrates seamlessly with DQX's quarantine workflow:

```python
from databricks.labs.dqx.engine import DQEngine
from databricks.labs.dqx.config import OutputConfig
from databricks.labs.dqx.anomaly import has_no_anomalies
from databricks.sdk import WorkspaceClient

dq_engine = DQEngine(WorkspaceClient())

checks = [
    has_no_anomalies(
        merge_columns=["order_id"],
        model="orders_monitor",
        columns=["amount", "quantity", "discount"],
        score_threshold=0.5,
        include_contributions=True,
    )
]

# Split valid and invalid rows
valid_df, invalid_df = dq_engine.apply_checks_by_metadata_and_split(
    spark.table("catalog.schema.orders"), checks
)

# Save to separate tables
dq_engine.save_results_in_table(
    output_df=valid_df,
    quarantine_df=invalid_df,
    output_config=OutputConfig(location="catalog.schema.valid_orders"),
    quarantine_config=OutputConfig(location="catalog.schema.quarantine_orders"),
)
```

**Quarantine DataFrame includes**:
- Original columns
- `_info` column with anomaly metadata:
  - `_info.anomaly.score`: Anomaly score (0-1)
  - `_info.anomaly.is_anomaly`: Boolean flag
  - `_info.anomaly.threshold`: Threshold used
  - `_info.anomaly.contributions`: SHAP values (if `include_contributions=True`)
  - `_info.anomaly.model`: Model name used
- DQX metadata columns (`_errors`, `_warnings`, etc.)

**ðŸ’¡ Customizing column names:** You can customize the `_info` column name (along with `_errors` and `_warnings`) via `ExtraParams`. 
See [Customizing result columns](/docs/guide/additional_configuration#customizing-result-columns) for details.

**Using with Delta Live Tables**:
```python
import dlt
from databricks.labs.dqx.engine import DQEngine
from databricks.labs.dqx.anomaly import has_no_anomalies
from databricks.sdk import WorkspaceClient

dq_engine = DQEngine(WorkspaceClient())
checks = [
    has_no_anomalies(
        merge_columns=["order_id"],
        model="orders_monitor",
        columns=["amount", "quantity", "discount"],
        score_threshold=0.5,
    )
]

@dlt.view
def bronze_with_anomaly_checks():
    df = dlt.read_stream("bronze")
    return dq_engine.apply_checks_by_metadata(df, checks)

@dlt.table
def silver():
    df = dlt.read_stream("bronze_with_anomaly_checks")
    return dq_engine.get_valid(df)

@dlt.table
def quarantine():
    df = dlt.read_stream("bronze_with_anomaly_checks")
    return dq_engine.get_invalid(df)
```

## Feature Engineering Configuration

Customize feature engineering behavior via `FeatureEngineeringConfig`:

```python
from databricks.labs.dqx.anomaly import AnomalyEngine, AnomalyParams, FeatureEngineeringConfig
from databricks.sdk import WorkspaceClient

anomaly_engine = AnomalyEngine(WorkspaceClient())

anomaly_engine.train(
    df=df,
    model_name="custom_config_model",
    columns=["amount", "category", "timestamp"],
    params=AnomalyParams(
        feature_engineering=FeatureEngineeringConfig(
            max_input_columns=10,                      # Hard limit on input columns
            max_engineered_features=50,                # Hard limit on total features
            categorical_cardinality_threshold=20,      # OneHot if â‰¤20, Frequency if >20
            enable_categorical=True,                   # Toggle categorical encoding
            enable_datetime=True,                      # Toggle datetime feature extraction
            enable_boolean=True,                       # Toggle boolean encoding
        )
    )
)
```

**When to customize**:
- **Increase cardinality threshold** (e.g., 30): More one-hot encoded features, better for interpretability
- **Decrease cardinality threshold** (e.g., 10): Fewer features, faster training
- **Disable types**: Force numeric-only if needed (`enable_categorical=False`)

**YAML configuration**:
```yaml
run_configs:
  - name: transactions
    input_config:
      location: catalog.schema.transactions
    anomaly_config:
      columns: [amount, category, timestamp]
      params:
        feature_engineering:
          categorical_cardinality_threshold: 15
          max_input_columns: 8
```

## Requirements and notes

### Databricks Runtime Compatibility

| Compute Type | Recommended Versions | Notes |
|-------------|---------------------|-------|
| **Classic Compute (ML Runtime)** | DBR 14.3 ML+, 15.x ML, 16.x ML, 17.x ML | âœ… sklearn, scipy, numpy pre-installed |
| **Classic Compute (Standard)** | DBR 16.x+, 17.x+ | âœ… Recent versions include sklearn |
| **Serverless** | All versions | âœ… Fully supported |

**ðŸ’¡ Recommended Setup:**
- **Serverless compute** (simplest - no cluster management)
- **ML Runtime** on classic compute (all dependencies pre-installed)
- **Standard Runtime 16.x+** if ML Runtime is not needed for other workloads

### Installation

```bash
pip install 'databricks-labs-dqx[anomaly]'
```

This installs: MLflow, scikit-learn, SHAP, and cloudpickle. On ML Runtimes and Serverless, most dependencies are already available.

### General Requirements

- Spark â‰¥ 3.4 for pandas UDF support (distributed scoring).
- Works on Databricks, local Spark clusters, and any Spark 3.4+ environment.
- **Training** runs on driver node (efficient for sampled data â‰¤1M rows).
- **Scoring** is fully distributed across Spark cluster via pandas UDFs.
- **Null handling**: Automatic via null indicators + imputation (no rows skipped).

## Looking ahead

The API is designed to support additional algorithms in the future (e.g., alternative anomaly detectors, ensemble methods) without changing how you define checks.

