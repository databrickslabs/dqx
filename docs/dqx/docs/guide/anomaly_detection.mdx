---
sidebar_position: 15
---

import Admonition from '@theme/Admonition';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Anomaly Detection (ML-based)

Detect unusual rows by learning what "normal" looks like across multiple columns, then flag rows that deviate.

## What it is (and why it helps)

Anomaly detection learns typical patterns from recent "good" data, then highlights rows that look unusual when you consider multiple columns together.
This complements rule-based checks:
- Rules catch the issues you can anticipate and describe.
- Anomaly detection surfaces unusual patterns you did not think to write rules for.

Example: amount, quantity, and discount are all valid individually, but the combination is rare for your data. Anomaly detection can flag that row.

## Complements Databricks data quality monitoring

Databricks data quality monitoring focuses on table-level signals like freshness and completeness. DQX anomaly detection focuses on unusual rows and cross-column patterns.
Use both together to cover:
- **Data availability**: Is new data arriving as expected?
- **Data content quality**: Are the rows themselves unusual or inconsistent?

Example: monitoring tells you new data arrived as expected; DQX flags a spike in unusual orders inside it.

<Admonition type="tip" title="Use both together">
Use Databricks data quality monitoring for table health (freshness/completeness) and DQX anomaly detection for row-level content quality.
</Admonition>

## Prerequisites

Install DQX with anomaly support:

```bash
pip install 'databricks-labs-dqx[anomaly]'
```

## Quick start

Train a model on recent "good" data, then use it in checks:

<Tabs>
  <TabItem value="Python" label="Python" default>
    ```python
    from databricks.labs.dqx.anomaly import AnomalyEngine, has_no_anomalies
    from databricks.labs.dqx.engine import DQEngine
    from databricks.sdk import WorkspaceClient

    ws = WorkspaceClient()
    anomaly_engine = AnomalyEngine(ws)

    model_name = anomaly_engine.train(
        df=spark.table("catalog.schema.orders"),
        model_name="orders_monitor",
        columns=["amount", "quantity", "discount"],
    )

    checks = [
        has_no_anomalies(
            merge_columns=["order_id"],
            model=model_name,
            columns=["amount", "quantity", "discount"],
        )
    ]

    dq_engine = DQEngine(ws, spark)
    scored_df = dq_engine.apply_checks(spark.table("catalog.schema.orders"), checks)
    ```
  </TabItem>
  <TabItem value="Workflows" label="Workflows">
    Configure anomaly training per run config:

    ```yaml
    run_configs:
      - name: orders
        input_config:
          location: catalog.schema.orders
        anomaly_config:
          columns: [amount, quantity, discount]
    ```

    Define checks separately (YAML or Python). See the reference section for examples.
  </TabItem>
</Tabs>

## Investigate anomalies

Anomaly checks add `_dq_info` with structured metadata. Feature contributions are enabled by default; disable them for faster scoring:

```python
has_no_anomalies(
    merge_columns=["order_id"],
    model=model_name,
    columns=["amount", "quantity", "discount"],
    include_contributions=False,
)
```

To review the most unusual rows:

```python
import pyspark.sql.functions as F

top = scored_df.orderBy(F.col("_dq_info.anomaly.score").desc()).limit(20)
top.select("order_id", "_dq_info.anomaly.score", "_dq_info.anomaly.contributions").show(truncate=False)
```

If you want to quarantine anomalies for investigation, use DQX output/quarantine patterns (see [Applying Checks](/docs/guide/quality_checks_apply)).

<Admonition type="tip" title="When to use anomaly detection">
Use anomaly detection when you want to catch unusual combinations across columns that are hard to capture with static rules.
</Admonition>

## Next steps

For training options, parameters, and YAML examples, see [Anomaly Detection in Quality Checks](/docs/reference/quality_checks#anomaly-detection).
