---
sidebar_position: 5
---

import Admonition from '@theme/Admonition';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# AI-Assisted Quality Checks Generation

DQX provides the capability to generate data quality rules using AI/LLM assistance based on natural language descriptions of your data quality requirements. This feature leverages Large Language Models (LLMs) to automatically create appropriate data quality checks from business descriptions, significantly reducing the time and effort required to define quality rules.

## Overview

The AI-assisted quality checks generation feature uses the `DQGenerator` class with LLM integration to:
- Analyze natural language descriptions of data quality requirements
- Optionally inspect table schemas to understand data structure
- Generate appropriate data quality rules that match the requirements
- Validate the generated rules to ensure they are syntactically correct

This approach is particularly useful when you have clear business requirements but need help translating them into technical data quality rules.

<Admonition type="tip" title="When to Use AI-Assisted Generation">
AI-assisted generation is ideal for:
- Translating business requirements into technical quality rules
- Quickly prototyping quality checks for new datasets
- Generating comprehensive checks based on compliance requirements
- Supplementing profiler-generated rules with business-specific logic

For automated discovery of data patterns and statistics-based rules, consider using the [Data Profiling](/docs/guide/data_profiling) approach instead.
</Admonition>

## Prerequisites

### Installing LLM Dependencies

To use the AI-assisted quality checks generation feature, you need to install DQX with the LLM extra dependencies:

```bash
pip install 'databricks-labs-dqx[llm]'
```

This will install the required packages including DSPy and other LLM-related dependencies.

### Model Access

The feature requires access to an LLM model. DQX supports:
- **Databricks Foundation Model APIs** (recommended): Use Databricks-hosted models like `databricks/databricks-claude-sonnet-4-5`
- **Custom API endpoints**: Any OpenAI-compatible API endpoint
- **Local models**: Any model supported by DSPy

You'll need appropriate API credentials to access the model endpoint.

## Using AI-Assisted Generation

### Basic Usage

Here's a simple example of generating quality rules from a natural language description:

<Tabs>
  <TabItem value="Python" label="Python" default>
    ```python
    from databricks.labs.dqx.profiler.generator import DQGenerator
    from databricks.sdk import WorkspaceClient

    # Initialize the generator with LLM support
    ws = WorkspaceClient()
    generator = DQGenerator(
        workspace_client=ws,
        model="databricks/databricks-claude-sonnet-4-5"
    )

    # Generate rules from natural language description
    user_input = """
    Username should not start with 's' if age is less than 18.
    All users must have a valid email address.
    Age should be between 0 and 120.
    """

    checks = generator.generate_dq_rules_ai_assisted(user_input=user_input)
    
    print(checks)
    ```
  </TabItem>
</Tabs>

### Using with Table Schema

For better results, you can provide a fully qualified table name. The LLM will analyze the table schema to generate more accurate rules:

<Tabs>
  <TabItem value="Python" label="Python" default>
    ```python
    from databricks.labs.dqx.profiler.generator import DQGenerator
    from databricks.sdk import WorkspaceClient

    ws = WorkspaceClient()
    generator = DQGenerator(
        workspace_client=ws,
        model="databricks/databricks-claude-sonnet-4-5"
    )

    # Generate rules with table schema awareness
    user_input = """
    All customer records must have complete contact information.
    Email addresses must be valid.
    Phone numbers should follow standard format.
    Registration date should not be in the future.
    """

    checks = generator.generate_dq_rules_ai_assisted(
        user_input=user_input,
        table_name="catalog1.schema1.customers"
    )
    
    print(checks)
    ```
  </TabItem>
</Tabs>

### Using Custom Model Configuration

You can configure the generator to use custom models or API endpoints:

<Tabs>
  <TabItem value="Python" label="Python" default>
    ```python
    from databricks.labs.dqx.profiler.generator import DQGenerator
    from databricks.sdk import WorkspaceClient

    ws = WorkspaceClient()

    # Option 1: Using Databricks Foundation Model API with explicit credentials
    generator = DQGenerator(
        workspace_client=ws,
        model="databricks/databricks-claude-sonnet-4-5",
        api_key="your-api-key",
        api_base="https://your-workspace.azuredatabricks.net/serving-endpoints"
    )

    # Option 2: Using a different model
    generator = DQGenerator(
        workspace_client=ws,
        model="databricks/meta-llama-3-1-70b-instruct",
        api_key="your-api-key",
        api_base="https://your-workspace.azuredatabricks.net/serving-endpoints"
    )

    user_input = "All fields should contain data and have no empty values"
    checks = generator.generate_dq_rules_ai_assisted(user_input=user_input)
    ```
  </TabItem>
</Tabs>

## Complete Example with Storage

Here's a complete workflow showing how to generate, review, and save AI-assisted quality rules:

<Tabs>
  <TabItem value="Python" label="Python" default>
    ```python
    from databricks.labs.dqx.profiler.generator import DQGenerator
    from databricks.labs.dqx.engine import DQEngine
    from databricks.labs.dqx.config import WorkspaceFileChecksStorageConfig
    from databricks.sdk import WorkspaceClient
    import yaml

    # Initialize workspace client and generator
    ws = WorkspaceClient()
    generator = DQGenerator(
        workspace_client=ws,
        model="databricks/databricks-claude-sonnet-4-5"
    )

    # Define business requirements in natural language
    business_requirements = """
    Data Quality Requirements for User Registration Table:
    
    1. All user IDs must be unique and not null
    2. Usernames should not contain special characters
    3. Email addresses must be valid and unique
    4. Age must be between 18 and 100 for adult users
    5. Country must be from a list of supported countries
    6. Join date should not be in the future
    7. Verified users must have a non-zero followers count
    """

    # Generate quality rules using AI
    checks = generator.generate_dq_rules_ai_assisted(
        user_input=business_requirements,
        table_name="production.users.registrations"
    )

    # Review the generated checks
    print("Generated Quality Checks:")
    print(yaml.safe_dump(checks, default_flow_style=False))

    # Initialize DQ Engine for validation and storage
    dq_engine = DQEngine(ws)

    # Validate the generated checks
    validation_status = dq_engine.validate_checks(checks)
    if validation_status.has_errors:
        print(f"Validation errors: {validation_status.errors}")
    else:
        print("All checks validated successfully!")

    # Save the generated checks to workspace file
    dq_engine.save_checks(
        checks=checks,
        config=WorkspaceFileChecksStorageConfig(
            location="/Shared/DataQuality/user_registration_checks.yml"
        )
    )

    print("Quality checks saved successfully!")
    ```
  </TabItem>
</Tabs>

## Combining AI-Assisted with Profiler-Based Rules

You can combine AI-assisted rules with profiler-generated rules for comprehensive coverage:

<Tabs>
  <TabItem value="Python" label="Python" default>
    ```python
    from databricks.labs.dqx.profiler.profiler import DQProfiler
    from databricks.labs.dqx.profiler.generator import DQGenerator
    from databricks.labs.dqx.engine import DQEngine
    from databricks.sdk import WorkspaceClient

    ws = WorkspaceClient()

    # Step 1: Profile the data to get statistics-based rules
    profiler = DQProfiler(ws)
    input_df = spark.read.table("catalog1.schema1.sales_data")
    summary_stats, profiles = profiler.profile(input_df)

    generator = DQGenerator(ws, model="databricks/databricks-claude-sonnet-4-5")
    
    # Generate profiler-based rules
    profiler_checks = generator.generate_dq_rules(profiles)

    # Step 2: Generate business logic rules using AI
    business_requirements = """
    Sales data must follow these business rules:
    - Transaction amount must be positive
    - Discount cannot exceed 50% of original price
    - Customer ID must exist in the customer master table
    - Sale date should be within the last 2 years
    """
    
    ai_checks = generator.generate_dq_rules_ai_assisted(
        user_input=business_requirements,
        table_name="catalog1.schema1.sales_data"
    )

    # Step 3: Combine both sets of rules
    all_checks = profiler_checks + ai_checks

    # Step 4: Save combined checks
    dq_engine = DQEngine(ws)
    dq_engine.save_checks(
        checks=all_checks,
        config=WorkspaceFileChecksStorageConfig(
            location="/Shared/DataQuality/combined_sales_checks.yml"
        )
    )

    print(f"Combined {len(profiler_checks)} profiler rules with {len(ai_checks)} AI rules")
    print(f"Total: {len(all_checks)} quality checks")
    ```
  </TabItem>
</Tabs>

## Example Use Cases

### Use Case 1: Compliance Requirements

Generate quality rules from compliance documentation:

```python
compliance_requirements = """
GDPR Compliance Requirements:
- User email addresses must be validated and properly formatted
- User consent date must not be null for any user
- Data retention: user records older than 7 years should be flagged
- Personal data fields must not contain placeholder values like 'N/A' or 'Unknown'
"""

checks = generator.generate_dq_rules_ai_assisted(
    user_input=compliance_requirements,
    table_name="gdpr.users.personal_data"
)
```

### Use Case 2: Financial Data Validation

Generate rules for financial data:

```python
financial_requirements = """
Financial Transaction Rules:
- Transaction amount must be non-negative
- Account balance should not go below minimum threshold
- Currency code must be valid ISO 4217 code
- Transaction timestamp should be within business hours (9 AM - 5 PM)
- Suspicious transactions above $10,000 should be flagged
"""

checks = generator.generate_dq_rules_ai_assisted(
    user_input=financial_requirements,
    table_name="finance.transactions.daily"
)
```

### Use Case 3: IoT Sensor Data

Generate rules for IoT sensor readings:

```python
iot_requirements = """
IoT Sensor Data Quality Rules:
- Temperature readings should be between -40°C and 125°C
- Humidity levels must be between 0% and 100%
- Sensor reading timestamps should not have gaps longer than 5 minutes
- Battery level should not drop below 10% without alert
- All sensor IDs must be registered in the device registry
"""

checks = generator.generate_dq_rules_ai_assisted(
    user_input=iot_requirements,
    table_name="iot.sensors.readings"
)
```

## Configuration Options

### DQGenerator Initialization Parameters

| Parameter | Description | Default | Example |
|-----------|-------------|---------|---------|
| `workspace_client` | Databricks workspace client instance | Required | `WorkspaceClient()` |
| `spark` | SparkSession instance | Auto-created if not provided | `spark` |
| `model` | LLM model to use for generation | `databricks/databricks-claude-sonnet-4-5` | `databricks/meta-llama-3-1-70b-instruct` |
| `api_key` | API key for the model endpoint | Empty string (uses workspace auth) | `"dapi..."` |
| `api_base` | Base URL for the model API | Empty string (uses default) | `"https://workspace.cloud.databricks.net/serving-endpoints"` |

### generate_dq_rules_ai_assisted Parameters

| Parameter | Type | Description | Required |
|-----------|------|-------------|----------|
| `user_input` | str | Natural language description of data quality requirements | Yes |
| `table_name` | str | Fully qualified table name (catalog.schema.table) for schema analysis | No |

## Best Practices

1. **Be Specific**: Provide clear and specific business requirements in your input. The more detailed your description, the better the generated rules.

2. **Include Table Name**: When possible, provide the fully qualified table name to help the LLM understand the actual data structure.

3. **Review Generated Rules**: Always review the generated rules before applying them to production data. The AI may not perfectly understand all nuances of your requirements.

4. **Combine Approaches**: Use AI-assisted generation for business logic rules and profiler-based generation for statistical rules.

5. **Iterate**: If the generated rules don't match your expectations, refine your input description and regenerate.

6. **Validate**: Always validate the generated checks using `DQEngine.validate_checks()` before applying them.

7. **Start Simple**: Begin with simple requirements and gradually add complexity as you become familiar with the feature.

## Troubleshooting

### Error: DSPy compiler not available

**Problem**: You receive an error saying "DSPy compiler not available"

**Solution**: Install the LLM dependencies:
```bash
pip install 'databricks-labs-dqx[llm]'
```

### Generated Rules Don't Match Requirements

**Problem**: The AI generates rules that don't align with your business requirements

**Solution**:
- Make your input description more specific and detailed
- Provide the table name so the LLM can analyze the actual schema
- Break complex requirements into simpler, more focused descriptions
- Include examples in your description

### Model Access Issues

**Problem**: Unable to connect to the LLM model endpoint

**Solution**:
- Verify your API credentials are correct
- Check that the API base URL is accessible from your workspace
- Ensure you have permissions to access the model endpoint
- Try using the default Databricks Foundation Model API

### Validation Errors in Generated Rules

**Problem**: The generated rules fail validation

**Solution**:
- Check the validation error messages for specific issues
- Verify that the column names mentioned in your requirements exist in the table
- Ensure the rule types requested are supported by DQX (see [Quality Checks Definition](/docs/guide/quality_checks_definition))
- Simplify your requirements and regenerate

## Limitations

- The AI-assisted generation requires network access to the LLM model endpoint
- Generated rules quality depends on the clarity of the input description
- Complex business logic may require manual refinement of generated rules
- The feature requires additional dependencies (`dqx[llm]`) which increases package size
- LLM inference may take a few seconds to complete

## Related Topics

- [Data Profiling](/docs/guide/data_profiling): Automatic rule generation based on data statistics
- [Quality Checks Definition](/docs/guide/quality_checks_definition): Manual definition of quality rules
- [Quality Checks Storage](/docs/guide/quality_checks_storage): How to store and manage quality rules
- [Applying Quality Checks](/docs/guide/quality_checks_apply): How to apply quality checks to your data

## API Reference

For detailed API documentation, see:
- [DQGenerator API Reference](/docs/reference/profiler#dqgenerator-methods)
