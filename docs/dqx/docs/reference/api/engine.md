---
sidebar_label: engine
title: databricks.labs.dqx.engine
---

## DQEngineCore Objects

```python
class DQEngineCore(DQEngineCoreBase)
```

Data Quality Engine Core class to apply data quality checks to a given dataframe.

**Arguments**:

- `workspace_client` _WorkspaceClient_ - WorkspaceClient instance to use for accessing the workspace.
- `extra_params` _ExtraParams_ - Extra parameters for the DQEngine.

## DQEngine Objects

```python
class DQEngine(DQEngineBase)
```

Data Quality Engine class to apply data quality checks to a given dataframe.

#### apply\_checks

```python
def apply_checks(df: DataFrame,
                 checks: list[DQRule],
                 ref_dfs: dict[str, DataFrame] | None = None) -> DataFrame
```

Applies data quality checks to a given dataframe.

**Arguments**:

- `df`: dataframe to check
- `checks`: list of checks to apply to the dataframe. Each check is an instance of DQRule class.
- `ref_dfs`: reference dataframes to use in the checks, if applicable

**Returns**:

dataframe with errors and warning result columns

#### apply\_checks\_and\_split

```python
def apply_checks_and_split(
    df: DataFrame,
    checks: list[DQRule],
    ref_dfs: dict[str, DataFrame] | None = None
) -> tuple[DataFrame, DataFrame]
```

Applies data quality checks to a given dataframe and split it into two (&quot;good&quot; and &quot;bad&quot;),

according to the data quality checks.

**Arguments**:

- `df`: dataframe to check
- `checks`: list of checks to apply to the dataframe. Each check is an instance of DQRule class.
- `ref_dfs`: reference dataframes to use in the checks, if applicable

**Returns**:

two dataframes - &quot;good&quot; which includes warning rows but no result columns, and &quot;data&quot; having
error and warning rows and corresponding result columns

#### apply\_checks\_by\_metadata\_and\_split

```python
def apply_checks_by_metadata_and_split(
    df: DataFrame,
    checks: list[dict],
    custom_check_functions: dict[str, Any] | None = None,
    ref_dfs: dict[str, DataFrame] | None = None
) -> tuple[DataFrame, DataFrame]
```

Wrapper around `apply_checks_and_split` for use in the metadata-driven pipelines. The main difference

is how the checks are specified - instead of using functions directly, they are described as function name plus
arguments.

**Arguments**:

- `df`: dataframe to check
- `checks`: list of dictionaries describing checks. Each check is a dictionary consisting of following fields:
* `check` - Column expression to evaluate. This expression should return string value if it&#x27;s evaluated to true -
it will be used as an error/warning message, or `null` if it&#x27;s evaluated to `false`
* `name` - name that will be given to a resulting column. Autogenerated if not provided
* `criticality` (optional) - possible values are `error` (data going only into &quot;bad&quot; dataframe),
and `warn` (data is going into both dataframes)
* `df`0 (optional) - Expression for filtering data quality checks
* `df`1 (optional) - User-defined key-value pairs added to metadata generated by the check.
- `df`2: dictionary with custom check functions (eg. `df`3globals()`df`3 of the calling module).
If not specified, then only built-in functions are used for the checks.
- `df`5: reference dataframes to use in the checks, if applicable

**Returns**:

two dataframes - &quot;good&quot; which includes warning rows but no result columns, and &quot;bad&quot; having
error and warning rows and corresponding result columns

#### apply\_checks\_by\_metadata

```python
def apply_checks_by_metadata(
        df: DataFrame,
        checks: list[dict],
        custom_check_functions: dict[str, Any] | None = None,
        ref_dfs: dict[str, DataFrame] | None = None) -> DataFrame
```

Wrapper around `apply_checks` for use in the metadata-driven pipelines. The main difference

is how the checks are specified - instead of using functions directly, they are described as function name plus
arguments.

**Arguments**:

- `df`: dataframe to check
- `checks`: list of dictionaries describing checks. Each check is a dictionary consisting of following fields:
* `check` - Column expression to evaluate. This expression should return string value if it&#x27;s evaluated to true -
it will be used as an error/warning message, or `null` if it&#x27;s evaluated to `false`
* `name` - name that will be given to a resulting column. Autogenerated if not provided
* `criticality` (optional) - possible values are `error` (data going only into &quot;bad&quot; dataframe),
and `warn` (data is going into both dataframes)
* `df`0 (optional) - Expression for filtering data quality checks
* `df`1 (optional) - User-defined key-value pairs added to metadata generated by the check.
- `df`2: dictionary with custom check functions (eg. `df`3globals()`df`3 of calling module).
- `df`5: reference dataframes to use in the checks, if applicable
If not specified, then only built-in functions are used for the checks.

**Returns**:

dataframe with errors and warning result columns

#### apply\_checks\_and\_save\_in\_table

```python
def apply_checks_and_save_in_table(
        checks: list[DQRule],
        input_config: InputConfig,
        output_config: OutputConfig,
        quarantine_config: OutputConfig | None = None,
        ref_dfs: dict[str, DataFrame] | None = None) -> None
```

Apply data quality checks to a table or view and write the result to table(s).

If quarantine_config is provided, the data will be split into good and bad records,
with good records written to the output table and bad records to the quarantine table.
If quarantine_config is not provided, all records (with error/warning columns)
will be written to the output table.

**Arguments**:

- `checks`: list of checks to apply to the dataframe. Each check is an instance of DQRule class.
- `input_config`: Input data configuration (e.g. table name or file location, read options)
- `output_config`: Output data configuration (e.g. table name, output mode, write options)
- `quarantine_config`: Optional quarantine data configuration (e.g. table name, output mode, write options)
- `ref_dfs`: Reference dataframes to use in the checks, if applicable

#### apply\_checks\_by\_metadata\_and\_save\_in\_table

```python
def apply_checks_by_metadata_and_save_in_table(
        checks: list[dict],
        input_config: InputConfig,
        output_config: OutputConfig,
        quarantine_config: OutputConfig | None = None,
        custom_check_functions: dict[str, Any] | None = None,
        ref_dfs: dict[str, DataFrame] | None = None) -> None
```

Apply data quality checks to a table or view and write the result to table(s).

If quarantine_config is provided, the data will be split into good and bad records,
with good records written to the output table and bad records to the quarantine table.
If quarantine_config is not provided, all records (with error/warning columns)
will be written to the output table.

**Arguments**:

- `checks`: List of dictionaries describing checks. Each check is a dictionary consisting of following fields:
* `check` - Column expression to evaluate. This expression should return string value if it&#x27;s evaluated to true -
it will be used as an error/warning message, or `null` if it&#x27;s evaluated to `false`
* `name` - Name that will be given to a resulting column. Autogenerated if not provided
* `criticality` (optional) -Possible values are `error` (data going only into &quot;bad&quot; dataframe),
and `warn` (data is going into both dataframes)
- `input_config`: Input data configuration (e.g. table name or file location, read options)
- `output_config`: Output data configuration (e.g. table name, output mode, write options)
- `check`0: Optional quarantine data configuration (e.g. table name, output mode, write options)
- `check`1: Dictionary with custom check functions (eg. `check`2globals()`check`2 of calling module).
- `check`4: Reference dataframes to use in the checks, if applicable

#### validate\_checks

```python
@staticmethod
def validate_checks(
        checks: list[dict],
        custom_check_functions: dict[str, Any] | None = None,
        validate_custom_check_functions: bool = True
) -> ChecksValidationStatus
```

Validate the input dict to ensure they conform to expected structure and types.

Each check can be a dictionary. The function validates
the presence of required keys, the existence and callability of functions, and the types
of arguments passed to these functions.

**Arguments**:

- `checks`: List of checks to apply to the dataframe. Each check should be a dictionary.
- `custom_check_functions`: Optional dictionary with custom check functions.
- `validate_custom_check_functions`: If True, validate custom check functions.

**Returns**:

`ValidationStatus`: The validation status.

#### get\_invalid

```python
def get_invalid(df: DataFrame) -> DataFrame
```

Get records that violate data quality checks (records with warnings and errors).
@param df: input DataFrame.
@return: dataframe with error and warning rows and corresponding result columns.

#### get\_valid

```python
def get_valid(df: DataFrame) -> DataFrame
```

Get records that don&#x27;t violate data quality checks (records with warnings but no errors).
@param df: input DataFrame.
@return: dataframe with warning rows but no result columns.

#### save\_results\_in\_table

```python
def save_results_in_table(output_df: DataFrame | None = None,
                          quarantine_df: DataFrame | None = None,
                          output_config: OutputConfig | None = None,
                          quarantine_config: OutputConfig | None = None,
                          run_config_name: str | None = "default",
                          product_name: str = "dqx",
                          assume_user: bool = True)
```

Save quarantine and output data to the `quarantine_table` and `output_table`.

**Arguments**:

- `quarantine_df`: Optional Dataframe containing the quarantine data
- `output_df`: Optional Dataframe containing the output data. If not provided, use run config
- `output_config`: Optional configuration for saving the output data. If not provided, use run config
- `quarantine_config`: Optional configuration for saving the quarantine data. If not provided, use run config
- `run_config_name`: Optional name of the run (config) to use
- `product_name`: name of the product/installation directory
- `assume_user`: if True, assume user installation

#### load\_checks

```python
def load_checks(config: BaseChecksStorageConfig) -> list[dict]
```

Load checks (dq rules) from the specified source type (file or table).

**Arguments**:

- `config`: storage configuration
Allowed configs are:
- `FileChecksStorageConfig`: for loading checks from a file in the local filesystem
- `WorkspaceFileChecksStorageConfig`: for loading checks from a workspace file
- `TableChecksStorageConfig`: for loading checks from a table
- `InstallationChecksStorageConfig`: for loading checks from the installation directory
- `VolumeFileChecksStorageConfig`: for loading checks from a Unity Catalog volume file
- ...

**Raises**:

- `ValueError`: if the source type is unknown

#### save\_checks

```python
def save_checks(checks: list[dict], config: BaseChecksStorageConfig) -> None
```

Save checks (dq rules) to the specified storage type (file or table).

**Arguments**:

- `checks`: list of dq rules to save
- `config`: storage configuration
Allowed configs are:
- `FileChecksStorageConfig`: for saving checks in a file in the local filesystem
- `WorkspaceFileChecksStorageConfig`: for saving checks in a workspace file
- `TableChecksStorageConfig`: for saving checks in a table
- `InstallationChecksStorageConfig`: for saving checks in the installation directory
- `VolumeFileChecksStorageConfig`: for saving checks in a Unity Catalog volume file
- ...

**Raises**:

- `ValueError`: if the storage type is unknown

