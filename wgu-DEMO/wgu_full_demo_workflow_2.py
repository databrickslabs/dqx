# Databricks notebook source
# MAGIC %md
# MAGIC # üß† Hybrid DQX Flow ‚Äî Best Practice Implementation (Final)
# MAGIC
# MAGIC Applies **auto**, **manual**, and **custom** DQX rules to:
# MAGIC - Source: `wgu_poc.wgu_bronze.students_data_workflow`
# MAGIC - Outputs:
# MAGIC   - ‚úÖ `wgu_poc.dqx_output.students_valid_workflow`
# MAGIC   - ‚ö†Ô∏è `wgu_poc.dqx_output.students_quarantined_workflow`
# MAGIC   - üìä `wgu_poc.dqx_output.dq_run_log`
# MAGIC
# MAGIC ## Key Features
# MAGIC ‚úÖ Incremental runs ‚Äî only validate new data  
# MAGIC ‚úÖ Deduped quarantines ‚Äî one record per student  
# MAGIC ‚úÖ Success rate logging for trend analysis  
# MAGIC ‚úÖ Upserts instead of overwrites  
# MAGIC ‚úÖ Balanced criticality (warns vs. errors)

# COMMAND ----------

# MAGIC %md
# MAGIC ### 0Ô∏è‚É£ Ensure Libraries Installed and Restart Python

# COMMAND ----------

dbutils.widgets.text("test_library_ref", "", "Test Library Ref")

if dbutils.widgets.get("test_library_ref") != "":
    %pip install '{dbutils.widgets.get("test_library_ref")}'
else:
    %pip install databricks-labs-dqx pyyaml

%restart_python

# COMMAND ----------

# MAGIC %md
# MAGIC ### 1Ô∏è‚É£ Setup & Configuration

# COMMAND ----------

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, year, current_timestamp, max as spark_max
from delta.tables import DeltaTable
from datetime import datetime
import pytz, yaml

spark = SparkSession.builder.getOrCreate()

source_table  = "wgu_poc.wgu_bronze.students_data_workflow"
valid_table   = "wgu_poc.dqx_output.students_valid_workflow"
invalid_table = "wgu_poc.dqx_output.students_quarantined_workflow"
run_log_table = "wgu_poc.dqx_output.dq_run_log"

print(f"‚úÖ Source Table: {source_table}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### 2Ô∏è‚É£ Load Only New / Unprocessed Rows

# COMMAND ----------

students_df = spark.table(source_table)

if spark.catalog.tableExists(valid_table):
    last_processed_ts = spark.table(valid_table).agg(spark_max("load_timestamp")).collect()[0][0]
    print(f"üïì Last processed timestamp: {last_processed_ts}")
    if last_processed_ts:
        students_df = students_df.filter(col("load_timestamp") > last_processed_ts)
else:
    last_processed_ts = None
    print("üÜï No prior runs found ‚Äî processing full dataset.")

if students_df.count() == 0:
    print("‚úÖ No new rows to process. Exiting early.")
    dbutils.notebook.exit("No new rows detected.")

display(students_df.limit(10))

# COMMAND ----------

# MAGIC %md
# MAGIC ### 3Ô∏è‚É£ Filter Out Corrupt Timestamps

# COMMAND ----------

students_df_filtered = students_df.filter(
    (year(col("load_timestamp")).between(1900, 9999)) & 
    (year(col("month_end_date")).between(1900, 9999))
)
print(f"Filtered record count: {students_df_filtered.count()} (of {students_df.count()})")

# COMMAND ----------

# MAGIC %md
# MAGIC ### 4Ô∏è‚É£ Profile & Auto-Generate Rules

# COMMAND ----------

from databricks.sdk import WorkspaceClient
from databricks.labs.dqx.profiler.profiler import DQProfiler
from databricks.labs.dqx.profiler.generator import DQGenerator
from databricks.labs.dqx.engine import DQEngine

ws = WorkspaceClient()
profiler = DQProfiler(ws)
summary_stats, profiles = profiler.profile(students_df_filtered, options={"sample_fraction": 0.7})

generator = DQGenerator(ws)
auto_rules = generator.generate_dq_rules(profiles)

print("‚úÖ Auto-generated DQ Rules:")
print(yaml.safe_dump(auto_rules, sort_keys=False))

# COMMAND ----------

# MAGIC %md
# MAGIC ### 5Ô∏è‚É£ Custom Rule ‚Äî Paid vs Student Status Consistency

# COMMAND ----------

from databricks.labs.dqx.rule import register_rule
from databricks.labs.dqx.check_funcs import make_condition
from pyspark.sql import functions as F

@register_rule("row")
def paid_status_consistency(paid_col: str, status_col: str):
    """
    Fail when paid = TRUE AND status NOT IN ('graduated', 'dropped').
    Pass otherwise.
    """
    invalid_condition = (
        (F.col(paid_col) == True) &
        (~F.col(status_col).isin("graduated", "dropped"))
    )
    # IMPORTANT: pass the *failure* condition to make_condition
    return make_condition(
        invalid_condition,
        "Paid is TRUE but student_status is not graduated or dropped",
        "paid_status_consistency"
    )

# COMMAND ----------

# MAGIC %md
# MAGIC ### 6Ô∏è‚É£ Manual Business Rules (Balanced Criticality)

# COMMAND ----------

manual_rules = yaml.safe_load("""
# ==========================================================
# DQX Manual Business Rules (Balanced)
# ==========================================================

# 1. Core fields must not be null (CRITICAL)
- check:
    function: is_not_null_and_not_empty
    for_each_column:
      - student_id
      - name
      - email
      - student_status
      - load_timestamp
      - month_end_date
      - paid
      - stays_on_campus
  criticality: error
  name: not_null_core_fields

# 2. Student ID must be unique (CRITICAL)
- check:
    function: is_unique
    arguments:
      columns:
        - student_id
  criticality: error
  name: unique_student_id

# 3. Student ID format (warn)
- check:
    function: regex_match
    for_each_column:
      - student_id
    arguments:
      regex: '^[A-Za-z]{3}\\d{8}$'
      negate: false
  criticality: warn
  name: valid_student_id_pattern

# 4. Email format (warn)
- check:
    function: regex_match
    for_each_column:
      - email
    arguments:
      regex: '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'
      negate: false
  criticality: warn
  name: valid_email_format

# 5. student_status list
- check:
    function: is_in_list
    for_each_column:
      - student_status
    arguments:
      allowed:
        - graduated
        - current
        - break
        - dropped
  criticality: error
  name: valid_student_status

# 6. Paid status consistency (custom)
- check:
    function: paid_status_consistency
    arguments:
      paid_col: paid
      status_col: student_status
  criticality: warn
  name: paid_status_inconsistent_with_status
""")

print("‚úÖ Manual YAML rules loaded successfully.")

# COMMAND ----------

# MAGIC %md
# MAGIC ### 7Ô∏è‚É£ Merge + Validate + Apply Rules

# COMMAND ----------

dq_engine = DQEngine(ws)
merged_rules = manual_rules + auto_rules

validation_status = dq_engine.validate_checks(merged_rules, custom_check_functions=globals())
if validation_status.has_errors:
    print("‚ö†Ô∏è Validation errors found:")
    print(validation_status.errors)
else:
    print("‚úÖ All rules validated successfully!")

valid_df, invalid_df = dq_engine.apply_checks_by_metadata_and_split(
    students_df_filtered,
    merged_rules,
    custom_check_functions=globals()
)

invalid_df_unique = invalid_df.dropDuplicates(["student_id"])

print(f"‚úÖ Applied DQX rules. Valid: {valid_df.count()}, Quarantined: {invalid_df_unique.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### 8Ô∏è‚É£ Upsert Results into Delta Tables

# COMMAND ----------

valid_df = valid_df.withColumn("dq_run_timestamp", current_timestamp())
invalid_df_unique = invalid_df_unique.withColumn("dq_run_timestamp", current_timestamp())

def upsert_to_table(source_df, target_table, key_col="student_id"):
    spark = source_df.sparkSession
    if spark.catalog.tableExists(target_table):
        delta_table = DeltaTable.forName(spark, target_table)
        (
            delta_table.alias("tgt")
            .merge(source_df.alias("src"), f"tgt.{key_col} = src.{key_col}")
            .whenMatchedUpdateAll()
            .whenNotMatchedInsertAll()
            .execute()
        )
        print(f"‚úÖ Upserted {source_df.count()} records into {target_table}")
    else:
        source_df.write.format("delta").mode("overwrite").saveAsTable(target_table)
        print(f"‚úÖ Created new table and loaded {source_df.count()} records into {target_table}")

upsert_to_table(valid_df, valid_table)
upsert_to_table(invalid_df_unique, invalid_table)

# COMMAND ----------

# MAGIC %md
# MAGIC ### 9Ô∏è‚É£ Log DQ Run Metrics (Success Rate Summary)

# COMMAND ----------

# ============================================
# üîç Final DQX Record Accounting Validation (Audit-Safe, Rule-Aware)
# ============================================

raw_invalid_count   = invalid_df.count()
dedup_invalid_count = invalid_df_unique.count()
valid_count_final   = valid_df.count()
total_source_count  = students_df_filtered.count()

# ID-level validation
valid_ids   = valid_df.select("student_id").distinct()
invalid_ids = invalid_df_unique.select("student_id").distinct()
source_ids  = students_df_filtered.select("student_id").distinct()

missing_in_results = source_ids.subtract(valid_ids.union(invalid_ids))
extras_in_results  = valid_ids.union(invalid_ids).subtract(source_ids)

missing_count = missing_in_results.count()
extra_count   = extras_in_results.count()

# Human-readable audit summary
audit_summary = spark.createDataFrame(
    [
        ("Raw Invalid Rows (rule-level)", raw_invalid_count),
        ("Unique Invalid Rows (record-level)", dedup_invalid_count),
        ("Final Valid Records", valid_count_final),
        ("Total Input Records", total_source_count),
        ("Sum (Valid + Unique Invalid)", valid_count_final + dedup_invalid_count),
        ("Missing from Both Outputs", missing_count),
        ("Extra (Not in Source)", extra_count),
    ],
    ["Metric", "Count"]
)

# ‚úÖ Logic: Only flag if IDs are missing or extra
if missing_count == 0 and extra_count == 0:
    print("\n‚úÖ All records accounted for ‚Äî record-level 1:1 partition verified.")
    print("‚ÑπÔ∏è Note: Multiple rule violations per record cause higher raw invalid counts (expected).")
else:
    print("\n‚ö†Ô∏è Record mismatch detected ‚Äî investigate filtering, duplication, or rule overlap.")

print(f"\n{'='*90}")
print("üìä FINAL DQX RECORD ACCOUNTING SUMMARY")
print(f"üì¶ Total Input Records: {total_source_count}")
print(f"‚úÖ Valid Records: {valid_count_final}")
print(f"‚ö†Ô∏è Unique Quarantined Warns/Errors: {dedup_invalid_count}")
print(f"‚ùó Raw Rule Violations Logged: {raw_invalid_count}")
print(f"üö´ Missing Records: {missing_count}")
print(f"üö´ Extra Records: {extra_count}")
print(f"{'='*90}")

display(audit_summary)
