{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè¶ Investment Banking Market Data Quality - Anomaly Detection Demo\n",
    "\n",
    "## Business Context\n",
    "\n",
    "**Scenario**: Monitor real-time market data feeds to detect:\n",
    "- Pricing anomalies and stale quotes\n",
    "- Volume spikes indicating market manipulation\n",
    "- Feed latency issues\n",
    "- Off-hours trading anomalies\n",
    "\n",
    "**Data**: Market data feed with prices, volumes, spreads, and timestamps across multiple exchanges\n",
    "\n",
    "## What You'll Learn (30-45 min)\n",
    "\n",
    "1. **Temporal Patterns**: Datetime encoding for trading hours\n",
    "2. **Multi-Asset Monitoring**: Exchange-specific baselines\n",
    "3. **Performance Optimization**: row_filter and merge_columns for high-frequency data\n",
    "4. **Feature Contributions**: Triaging different anomaly types\n",
    "5. **Ensemble Models**: Confidence intervals for financial data\n",
    "6. **Production Integration**: Streaming batch patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Setup & Data Generation (5 min)\n",
    "\n",
    "First, install DQX with anomaly support if not already installed:\n",
    "```bash\n",
    "%pip install databricks-labs-dqx[anomaly]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from databricks.labs.dqx.anomaly import AnomalyEngine, has_no_anomalies, AnomalyParams, IsolationForestConfig\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Initialize\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "ws = WorkspaceClient()\n",
    "anomaly_engine = AnomalyEngine(ws)\n",
    "dq_engine = DQEngine(ws)\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"   Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Realistic Market Data\n",
    "\n",
    "We'll create high-frequency market data with:\n",
    "- **Mixed data types**: Numeric (prices, volume), categorical (exchange, symbol), datetime (timestamp), boolean (is_official_hours)\n",
    "- **Exchange-specific patterns**: Different baselines for NASDAQ, NYSE, LSE\n",
    "- **Injected anomalies**: Pricing errors, stale quotes, volume spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate market data feed\n",
    "def generate_market_data(num_rows=2000, anomaly_rate=0.05):\n",
    "    data = []\n",
    "    exchanges = [\"NASDAQ\", \"NYSE\", \"LSE\"]\n",
    "    symbols = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"META\", \"NVDA\", \"JPM\", \"BAC\", \"GS\"]\n",
    "    \n",
    "    # Exchange-specific baseline patterns\n",
    "    exchange_patterns = {\n",
    "        \"NASDAQ\": {\"base_price\": 150, \"spread_bp\": 5, \"volume\": 50000, \"lag_ms\": 15},\n",
    "        \"NYSE\": {\"base_price\": 100, \"spread_bp\": 8, \"volume\": 30000, \"lag_ms\": 20},\n",
    "        \"LSE\": {\"base_price\": 80, \"spread_bp\": 12, \"volume\": 20000, \"lag_ms\": 35},\n",
    "    }\n",
    "    \n",
    "    start_time = datetime(2024, 12, 1, 9, 30)  # Market open\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        exchange = random.choice(exchanges)\n",
    "        symbol = random.choice(symbols)\n",
    "        pattern = exchange_patterns[exchange]\n",
    "        \n",
    "        # Generate timestamp (mostly during trading hours)\n",
    "        minutes_offset = random.randint(0, 390)  # 6.5 hours = 390 min\n",
    "        timestamp = start_time + timedelta(minutes=minutes_offset)\n",
    "        \n",
    "        # Trading hours: 9:30-16:00\n",
    "        is_official_hours = 9.5 <= timestamp.hour + timestamp.minute/60 <= 16\n",
    "        \n",
    "        # Normal patterns\n",
    "        if random.random() > anomaly_rate:\n",
    "            base = pattern[\"base_price\"] * random.uniform(0.95, 1.05)\n",
    "            spread_bp = max(1, pattern[\"spread_bp\"] + np.random.normal(0, 2))\n",
    "            spread = base * (spread_bp / 10000)\n",
    "            \n",
    "            bid_price = round(base, 2)\n",
    "            ask_price = round(base + spread, 2)\n",
    "            volume = int(np.random.normal(pattern[\"volume\"], pattern[\"volume\"] * 0.3))\n",
    "            lag_ms = int(np.random.normal(pattern[\"lag_ms\"], 5))\n",
    "        else:\n",
    "            # Inject anomalies\n",
    "            anomaly_type = random.choice([\"pricing_error\", \"stale_quote\", \"volume_spike\", \"off_hours\"])\n",
    "            \n",
    "            if anomaly_type == \"pricing_error\":\n",
    "                base = pattern[\"base_price\"] * random.uniform(1.5, 3.0)  # 50-200% price jump\n",
    "                spread = base * random.uniform(0.05, 0.15)  # Wide spread\n",
    "                bid_price = round(base, 2)\n",
    "                ask_price = round(base + spread, 2)\n",
    "                volume = int(pattern[\"volume\"] * random.uniform(0.5, 1.0))\n",
    "                lag_ms = int(pattern[\"lag_ms\"])\n",
    "            \n",
    "            elif anomaly_type == \"stale_quote\":\n",
    "                base = pattern[\"base_price\"] * random.uniform(0.95, 1.05)\n",
    "                spread = base * (pattern[\"spread_bp\"] / 10000)\n",
    "                bid_price = round(base, 2)\n",
    "                ask_price = round(base + spread, 2)\n",
    "                volume = int(pattern[\"volume\"] * random.uniform(0.8, 1.2))\n",
    "                lag_ms = int(pattern[\"lag_ms\"] * random.uniform(10, 30))  # 10-30x normal lag\n",
    "            \n",
    "            elif anomaly_type == \"volume_spike\":\n",
    "                base = pattern[\"base_price\"] * random.uniform(0.98, 1.02)\n",
    "                spread = base * (pattern[\"spread_bp\"] / 10000)\n",
    "                bid_price = round(base, 2)\n",
    "                ask_price = round(base + spread, 2)\n",
    "                volume = int(pattern[\"volume\"] * random.uniform(5, 15))  # 5-15x normal volume\n",
    "                lag_ms = int(pattern[\"lag_ms\"])\n",
    "            \n",
    "            else:  # off_hours\n",
    "                base = pattern[\"base_price\"] * random.uniform(0.95, 1.05)\n",
    "                spread = base * (pattern[\"spread_bp\"] / 10000)\n",
    "                bid_price = round(base, 2)\n",
    "                ask_price = round(base + spread, 2)\n",
    "                volume = int(pattern[\"volume\"] * random.uniform(0.1, 0.3))\n",
    "                lag_ms = int(pattern[\"lag_ms\"])\n",
    "                # Force off-hours timestamp\n",
    "                timestamp = timestamp.replace(hour=random.choice([2, 3, 4, 22, 23]))\n",
    "                is_official_hours = False\n",
    "        \n",
    "        data.append((\n",
    "            symbol,\n",
    "            exchange,\n",
    "            timestamp,\n",
    "            bid_price,\n",
    "            ask_price,\n",
    "            int(max(100, volume)),\n",
    "            round(ask_price - bid_price, 3),\n",
    "            max(1, lag_ms),\n",
    "            is_official_hours\n",
    "        ))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate data\n",
    "market_data = generate_market_data(num_rows=2000, anomaly_rate=0.05)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"symbol\", StringType(), False),\n",
    "    StructField(\"exchange\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"bid_price\", DoubleType(), False),\n",
    "    StructField(\"ask_price\", DoubleType(), False),\n",
    "    StructField(\"volume\", IntegerType(), False),\n",
    "    StructField(\"spread\", DoubleType(), False),\n",
    "    StructField(\"last_update_lag_ms\", IntegerType(), False),\n",
    "    StructField(\"is_official_hours\", BooleanType(), False),\n",
    "])\n",
    "\n",
    "df_market = spark.createDataFrame(market_data, schema)\n",
    "\n",
    "print(\"\\nüìä Sample of market data feed:\")\n",
    "df_market.orderBy(\"timestamp\").show(10, truncate=False)\n",
    "print(f\"\\n‚úÖ Generated {df_market.count()} rows with ~5% injected anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Auto-Discovery with Temporal Patterns (10 min)\n",
    "\n",
    "Train model with auto-discovery, which will automatically extract temporal features from the timestamp column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to table\n",
    "catalog = spark.sql(\"SELECT current_catalog()\").first()[0]\n",
    "schema_name = \"dqx_demo\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema_name}\")\n",
    "\n",
    "table_name = f\"{catalog}.{schema_name}.market_data_feed\"\n",
    "df_market.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"‚úÖ Data saved to: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with ZERO configuration (auto-discovery)\n",
    "print(\"üéØ Training with AUTO-DISCOVERY (zero config)...\\n\")\n",
    "\n",
    "model_uri_auto = anomaly_engine.train(\n",
    "    df=spark.table(table_name),\n",
    "    # NO columns specified - auto-discovered!\n",
    "    # Model name auto-generated!\n",
    "    registry_table=f\"{catalog}.{schema_name}.anomaly_model_registry\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Auto-discovery model trained!\")\n",
    "print(f\"   Model URI: {model_uri_auto}\")\n",
    "\n",
    "# Check what was auto-discovered\n",
    "registry_df = spark.table(f\"{catalog}.{schema_name}.anomaly_model_registry\")\n",
    "auto_model = registry_df.filter(F.col(\"model_uri\") == model_uri_auto).first()\n",
    "\n",
    "print(f\"\\nüìã Auto-Discovered Configuration:\")\n",
    "print(f\"   Columns: {auto_model['columns']}\")\n",
    "print(f\"   Column types: {auto_model['column_types']}\")\n",
    "print(f\"\\nüí° Datetime columns automatically encoded as 5 cyclical features:\")\n",
    "print(f\"   - hour_sin, hour_cos (daily cycle)\")\n",
    "print(f\"   - day_of_week_sin, day_of_week_cos (weekly cycle)\")\n",
    "print(f\"   - is_weekend (binary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score with auto-discovered model\n",
    "checks_auto = [\n",
    "    has_no_anomalies(\n",
    "        merge_columns=[\"symbol\", \"exchange\", \"timestamp\"],  # Primary key (now first param)\n",
    "        score_threshold=0.5,\n",
    "        registry_table=f\"{catalog}.{schema_name}.anomaly_model_registry\"\n",
    "    )\n",
    "]\n",
    "\n",
    "df_scored_auto = dq_engine.apply_checks_by_metadata(df_market, checks_auto)\n",
    "anomalies_auto = df_scored_auto.filter(F.col(\"anomaly_score\") >= 0.5)\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Auto-discovery found {anomalies_auto.count()} anomalies:\\n\")\n",
    "display(anomalies_auto.orderBy(F.col(\"anomaly_score\").desc()).select(\n",
    "    \"symbol\", \"exchange\", \"timestamp\", \"price\", \"volume\",\n",
    "    F.round(\"anomaly_score\", 3).alias(\"score\")\n",
    ").limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Manual Column Selection & Parameter Tuning\n",
    "\n",
    "Now let's manually select columns and tune hyperparameters for financial data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with MANUAL configuration and tuned parameters\n",
    "print(\"üéØ Training with MANUAL tuning for financial data...\\n\")\n",
    "\n",
    "model_uri_manual = anomaly_engine.train(\n",
    "    df=spark.table(table_name),\n",
    "    columns=[\"bid_price\", \"ask_price\", \"spread\", \"volume\", \"last_update_lag_ms\", \"timestamp\"],  # Manual selection\n",
    "    model_name=\"market_data_tuned\",\n",
    "    params=AnomalyParams(\n",
    "        isolation_forest=IsolationForestConfig(\n",
    "            contamination=0.05,  # Expected 5% anomaly rate\n",
    "            n_estimators=200,    # More trees for financial data stability\n",
    "            max_samples=1024,    # Larger subsample for accuracy\n",
    "            random_state=42\n",
    "        ),\n",
    "        sample_fraction=1.0,\n",
    "        max_rows=None\n",
    "    ),\n",
    "    registry_table=f\"{catalog}.{schema_name}.anomaly_model_registry\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Manual tuned model trained!\")\n",
    "print(f\"   Model URI: {model_uri_manual}\")\n",
    "print(f\"\\nüí° Financial Data Tuning:\")\n",
    "print(f\"   ‚Ä¢ Higher n_estimators (200) for stability\")\n",
    "print(f\"   ‚Ä¢ Larger max_samples (1024) for accuracy\")\n",
    "print(f\"   ‚Ä¢ Datetime features automatically extracted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Multi-Asset Monitoring (8 min)\n",
    "\n",
    "Different exchanges have different characteristics. Train per-exchange models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with exchange segmentation\n",
    "print(\"üåç Training exchange-specific models...\\n\")\n",
    "\n",
    "model_uri_segmented = anomaly_engine.train(\n",
    "    df=spark.table(table_name),\n",
    "    columns=[\"bid_price\", \"ask_price\", \"spread\", \"volume\", \"last_update_lag_ms\"],\n",
    "    segment_by=[\"exchange\"],\n",
    "    model_name=\"market_data_by_exchange\",\n",
    "    registry_table=f\"{catalog}.{schema_name}.anomaly_model_registry\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Exchange-specific models trained!\")\n",
    "\n",
    "# Show exchange baselines\n",
    "exchange_models = spark.table(f\"{catalog}.{schema_name}.anomaly_model_registry\").filter(\n",
    "    F.col(\"model_name\") == \"market_data_by_exchange\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Exchange-Specific Baselines:\\n\")\n",
    "exchange_models.select(\"segment_values.exchange\", \"training_rows\", \"baseline_stats\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: High-Frequency Scoring with Performance (8 min)\n",
    "\n",
    "Optimize scoring for high-frequency data using row_filter and merge_columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score only official trading hours data with optimized joins\n",
    "checks_optimized = [\n",
    "    has_no_anomalies(\n",
    "        merge_columns=[\"symbol\", \"exchange\", \"timestamp\"],  # Primary key (now first param)\n",
    "        model=\"market_data_by_exchange\",\n",
    "        score_threshold=0.5,\n",
    "        row_filter=\"is_official_hours = true\",  # Filter before scoring\n",
    "        registry_table=f\"{catalog}.{schema_name}.anomaly_model_registry\"\n",
    "    )\n",
    "]\n",
    "\n",
    "df_scored_optimized = dq_engine.apply_checks_by_metadata(df_market, checks_optimized)\n",
    "print(f\"‚úÖ Scored {df_scored_optimized.count()} rows with optimized join\")\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Feature Contributions for Investigation (5 min)\n",
    "\n",
    "Use SHAP to understand which features drove anomaly scores and triage by type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score with feature contributions\n",
    "checks_contrib = [\n",
    "    has_no_anomalies(\n",
    "        model=\"market_data_by_exchange\",\n",
    "        score_threshold=0.5,\n",
    "        include_contributions=True,\n",
    "        registry_table=f\"{catalog}.{schema_name}.anomaly_model_registry\"\n",
    "    )\n",
    "]\n",
    "\n",
    "df_with_contrib = dq_engine.apply_checks_by_metadata(df_market, checks_contrib)\n",
    "\n",
    "print(\"üîç Top Anomalies with Feature Contributions:\\n\")\n",
    "anomalies_contrib = df_with_contrib.filter(F.col(\"anomaly_score\") >= 0.5).orderBy(\n",
    "    F.col(\"anomaly_score\").desc()\n",
    ").limit(10)\n",
    "\n",
    "anomalies_contrib.select(\n",
    "    \"symbol\", \"exchange\", \"bid_price\", \"volume\", \"last_update_lag_ms\",\n",
    "    F.round(\"anomaly_score\", 3).alias(\"score\"),\n",
    "    \"anomaly_contributions\"\n",
    ").show(truncate=False)\n",
    "\n",
    "print(\"\\nüí° Triage by contribution pattern:\")\n",
    "print(\"   - High 'last_update_lag_ms' ‚Üí Stale quote, route to data feed team\")\n",
    "print(\"   - High 'bid_price' + 'spread' ‚Üí Pricing error, route to pricing team\")\n",
    "print(\"   - High 'volume' ‚Üí Potential manipulation, route to compliance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Ensemble Models & Confidence (4 min)\n",
    "\n",
    "For financial data, use ensemble models to get confidence intervals on anomaly scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble model (3 models)\n",
    "print(\"üé≤ Training ensemble model (3 members)...\\n\")\n",
    "\n",
    "model_uri_ensemble = anomaly_engine.train(\n",
    "    df=spark.table(table_name),\n",
    "    columns=[\"bid_price\", \"ask_price\", \"spread\", \"volume\", \"last_update_lag_ms\"],\n",
    "    segment_by=[\"exchange\"],\n",
    "    model_name=\"market_data_ensemble\",\n",
    "    params=AnomalyParams(\n",
    "        isolation_forest=IsolationForestConfig(ensemble_size=3)  # 3-model ensemble\n",
    "    ),\n",
    "    registry_table=f\"{catalog}.{schema_name}.anomaly_model_registry\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Ensemble trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Drift Detection & Retraining (6 min)\n",
    "\n",
    "Market conditions change. Detect when models become stale and need retraining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate market regime change (increased volatility, wider spreads)\n",
    "def generate_volatile_market_data(num_rows=300):\n",
    "    \"\"\"Generate data with shifted volatility (market stress period).\"\"\"\n",
    "    data = []\n",
    "    exchanges = [\"NASDAQ\", \"NYSE\", \"LSE\"]\n",
    "    symbols = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"META\", \"NVDA\", \"JPM\", \"BAC\", \"GS\"]\n",
    "    \n",
    "    # NEW PATTERNS: Higher volatility, wider spreads, increased volume\n",
    "    volatile_patterns = {\n",
    "        \"NASDAQ\": {\"base_price\": 150, \"spread_bp\": 15, \"volume\": 75000, \"lag_ms\": 25},  # +200% spread\n",
    "        \"NYSE\": {\"base_price\": 100, \"spread_bp\": 20, \"volume\": 45000, \"lag_ms\": 35},    # +150% spread\n",
    "        \"LSE\": {\"base_price\": 80, \"spread_bp\": 25, \"volume\": 30000, \"lag_ms\": 50},      # +108% spread\n",
    "    }\n",
    "    \n",
    "    start_time = datetime(2024, 12, 15, 9, 30)  # New period\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        exchange = random.choice(exchanges)\n",
    "        symbol = random.choice(symbols)\n",
    "        pattern = volatile_patterns[exchange]\n",
    "        \n",
    "        minutes_offset = random.randint(0, 390)\n",
    "        timestamp = start_time + timedelta(minutes=minutes_offset)\n",
    "        is_official_hours = 9.5 <= timestamp.hour + timestamp.minute/60 <= 16\n",
    "        \n",
    "        base = pattern[\"base_price\"] * random.uniform(0.90, 1.10)  # +100% price volatility\n",
    "        spread_bp = max(1, pattern[\"spread_bp\"] + np.random.normal(0, 5))\n",
    "        spread = base * (spread_bp / 10000)\n",
    "        \n",
    "        bid_price = round(base, 2)\n",
    "        ask_price = round(base + spread, 2)\n",
    "        volume = int(np.random.normal(pattern[\"volume\"], pattern[\"volume\"] * 0.4))  # +33% vol volatility\n",
    "        lag_ms = int(np.random.normal(pattern[\"lag_ms\"], 10))  # +100% lag variability\n",
    "        \n",
    "        data.append((symbol, exchange, timestamp, bid_price, ask_price, int(max(100, volume)), \n",
    "                    round(ask_price - bid_price, 3), max(1, lag_ms), is_official_hours))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate volatile market data\n",
    "volatile_data = generate_volatile_market_data(num_rows=300)\n",
    "df_volatile = spark.createDataFrame(volatile_data, schema)\n",
    "\n",
    "print(\"üìä Normal vs Volatile Market Comparison:\\n\")\n",
    "print(\"Normal Market (original):\")\n",
    "df_market.agg(\n",
    "    F.avg(\"spread\").alias(\"avg_spread\"),\n",
    "    F.avg(\"volume\").alias(\"avg_volume\"),\n",
    "    F.avg(\"last_update_lag_ms\").alias(\"avg_lag\")\n",
    ").show()\n",
    "\n",
    "print(\"Volatile Market (stress period):\")\n",
    "df_volatile.agg(\n",
    "    F.avg(\"spread\").alias(\"avg_spread\"),\n",
    "    F.avg(\"volume\").alias(\"avg_volume\"),\n",
    "    F.avg(\"last_update_lag_ms\").alias(\"avg_lag\")\n",
    ").show()\n",
    "\n",
    "print(\"‚úÖ Market regime changed:\")\n",
    "print(\"   ‚Ä¢ Spreads: +150-200% (market stress)\")\n",
    "print(\"   ‚Ä¢ Volume: +50% (panic trading)\")\n",
    "print(\"   ‚Ä¢ Latency: +67% (system overload)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score volatile data with drift detection\n",
    "checks_with_drift = [\n",
    "    has_no_anomalies(\n",
    "        model=\"market_data_by_exchange\",\n",
    "        drift_threshold=3.0,  # Z-score threshold\n",
    "        registry_table=f\"{catalog}.{schema_name}.anomaly_model_registry\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"üîç Scoring volatile market data (watch for drift warnings)...\\n\")\n",
    "df_drift_scored = dq_engine.apply_checks_by_metadata(df_volatile, checks_with_drift)\n",
    "\n",
    "print(\"\\n‚ÑπÔ∏è  Drift warnings indicate distribution shift!\")\n",
    "print(\"   Example: 'Data drift detected in columns: spread, last_update_lag_ms (drift score: 5.3)'\")\n",
    "print(\"   Action: Retrain model to adapt to new market regime\")\n",
    "print(\"\\nüí° In production: Set up alerts when drift_score > 3.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain with combined data (normal + volatile periods)\n",
    "df_combined = df_market.union(df_volatile)\n",
    "\n",
    "print(\"üîÑ Retraining model with combined market conditions...\\n\")\n",
    "\n",
    "model_uri_retrained = anomaly_engine.train(\n",
    "    df=df_combined,\n",
    "    columns=[\"bid_price\", \"ask_price\", \"spread\", \"volume\", \"last_update_lag_ms\"],\n",
    "    segment_by=[\"exchange\"],\n",
    "    model_name=\"market_data_by_exchange\",  # Same name = new version\n",
    "    params=AnomalyParams(\n",
    "        isolation_forest=IsolationForestConfig(\n",
    "            contamination=0.05,\n",
    "            n_estimators=200,\n",
    "            max_samples=1024,\n",
    "            random_state=42\n",
    "        )\n",
    "    ),\n",
    "    registry_table=f\"{catalog}.{schema_name}.anomaly_model_registry\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model retrained!\")\n",
    "print(\"   ‚Ä¢ Old model automatically archived\")\n",
    "print(\"   ‚Ä¢ New model adapts to both normal and volatile market conditions\")\n",
    "print(\"   ‚Ä¢ Baseline now includes wider spreads and higher volatility\")\n",
    "print(\"\\nüí° Best Practice:\")\n",
    "print(\"   ‚Ä¢ Monitor drift_score in production dashboards\")\n",
    "print(\"   ‚Ä¢ Set up automated retraining when drift > threshold\")\n",
    "print(\"   ‚Ä¢ Retrain quarterly or when market regimes change\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Production Integration & Quarantine (6 min)\n",
    "\n",
    "Integrate anomaly detection into production workflows with automated quarantine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine anomaly detection with traditional DQ checks\n",
    "from databricks.labs.dqx.check_funcs import is_not_null, is_in_range\n",
    "\n",
    "checks_combined = [\n",
    "    # Traditional data quality checks\n",
    "    is_not_null(columns=[\"symbol\", \"exchange\", \"timestamp\"]),\n",
    "    is_in_range(column=\"bid_price\", min_value=0, max_value=10000),\n",
    "    is_in_range(column=\"spread\", min_value=0, max_value=100),\n",
    "    is_in_range(column=\"last_update_lag_ms\", min_value=0, max_value=5000),\n",
    "    \n",
    "    # ML-based anomaly detection with explanations\n",
    "    has_no_anomalies(\n",
    "        model=\"market_data_by_exchange\",\n",
    "        score_threshold=0.5,\n",
    "        include_contributions=True,  # For root cause\n",
    "        drift_threshold=3.0,\n",
    "        merge_columns=[\"symbol\", \"exchange\", \"timestamp\"],  # Performance optimization\n",
    "        registry_table=f\"{catalog}.{schema_name}.anomaly_model_registry\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Apply all checks in single pass\n",
    "df_full_dq = dq_engine.apply_checks_by_metadata(df_market, checks_combined)\n",
    "\n",
    "print(\"üìä Full Data Quality Summary:\\n\")\n",
    "total_rows = df_full_dq.count()\n",
    "anomalies_found = df_full_dq.filter(F.col(\"anomaly_score\") >= 0.5).count()\n",
    "\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Anomalies Detected: {anomalies_found}\")\n",
    "print(f\"Clean Records: {total_rows - anomalies_found}\")\n",
    "print(f\"Anomaly Rate: {(anomalies_found/total_rows)*100:.2f}%\")\n",
    "print(f\"\\n‚úÖ All checks (traditional + ML) applied in single pass!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quarantine anomalies for investigation\n",
    "quarantine_table = f\"{catalog}.{schema_name}.market_data_quarantine\"\n",
    "\n",
    "quarantine_df = df_full_dq.filter(\n",
    "    F.col(\"anomaly_score\") >= 0.5\n",
    ").select(\n",
    "    \"*\",\n",
    "    F.current_timestamp().alias(\"quarantine_timestamp\"),\n",
    "    F.lit(\"anomaly_detected\").alias(\"quarantine_reason\"),\n",
    "    # Extract top contributor for triage\n",
    "    F.expr(\"transform_keys(anomaly_contributions, (k,v) -> k)[0]\").alias(\"top_contributor\")\n",
    ")\n",
    "\n",
    "quarantine_df.write.mode(\"overwrite\").saveAsTable(quarantine_table)\n",
    "\n",
    "print(f\"‚úÖ Quarantined {quarantine_df.count()} anomalies to: {quarantine_table}\")\n",
    "print(\"\\nüìã Quarantine Summary by Exchange and Issue Type:\")\n",
    "spark.table(quarantine_table).groupBy(\"exchange\", \"top_contributor\").agg(\n",
    "    F.count(\"*\").alias(\"count\"),\n",
    "    F.avg(\"anomaly_score\").alias(\"avg_score\")\n",
    ").orderBy(\"exchange\", F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "print(\"\\nüí° Automated Triage Workflow:\")\n",
    "print(\"   1. Anomalies automatically quarantined with contributions\")\n",
    "print(\"   2. Route by top_contributor:\")\n",
    "print(\"      ‚Ä¢ 'last_update_lag_ms' ‚Üí Data feed team\")\n",
    "print(\"      ‚Ä¢ 'bid_price' or 'spread' ‚Üí Pricing team\")\n",
    "print(\"      ‚Ä¢ 'volume' ‚Üí Compliance/surveillance team\")\n",
    "print(\"   3. Investigate using anomaly_contributions map\")\n",
    "print(\"   4. False positives ‚Üí Adjust threshold or retrain\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAML Configuration for Production\n",
    "\n",
    "For automated workflows, define checks in YAML:\n",
    "\n",
    "```yaml\n",
    "run_configs:\n",
    "  - name: market_data_quality_monitoring\n",
    "    input_config:\n",
    "      location: catalog.schema.market_data_feed\n",
    "    \n",
    "    # Traditional checks\n",
    "    quality_checks:\n",
    "      - function: is_not_null\n",
    "        arguments:\n",
    "          columns: [symbol, exchange, timestamp]\n",
    "      - function: is_in_range\n",
    "        arguments:\n",
    "          column: bid_price\n",
    "          min_value: 0\n",
    "          max_value: 10000\n",
    "      - function: is_in_range\n",
    "        arguments:\n",
    "          column: spread\n",
    "          min_value: 0\n",
    "          max_value: 100\n",
    "    \n",
    "    # Anomaly detection\n",
    "    anomaly_config:\n",
    "      columns: [bid_price, ask_price, spread, volume, last_update_lag_ms, timestamp]\n",
    "      segment_by: [exchange]\n",
    "      model_name: market_data_by_exchange\n",
    "      registry_table: catalog.schema.anomaly_model_registry\n",
    "      params:\n",
    "        isolation_forest:\n",
    "          contamination: 0.05\n",
    "          n_estimators: 200\n",
    "          max_samples: 1024\n",
    "          random_state: 42\n",
    "        sample_fraction: 1.0\n",
    "      \n",
    "      # Scoring options\n",
    "      score_options:\n",
    "        score_threshold: 0.5\n",
    "        include_contributions: true\n",
    "        drift_threshold: 3.0\n",
    "        row_filter: \"is_official_hours = true\"\n",
    "        merge_columns: [symbol, exchange, timestamp]\n",
    "    \n",
    "    # Quarantine configuration\n",
    "    quarantine_config:\n",
    "      enabled: true\n",
    "      table: catalog.schema.market_data_quarantine\n",
    "      condition: \"anomaly_score >= 0.5\"\n",
    "      \n",
    "    # Output configuration\n",
    "    output_config:\n",
    "      location: catalog.schema.market_data_clean\n",
    "      save_mode: overwrite\n",
    "```\n",
    "\n",
    "**Run with Databricks Asset Bundles:**\n",
    "```bash\n",
    "# Initial model training (one-time or scheduled monthly)\n",
    "databricks bundle run market_data_anomaly_trainer\n",
    "\n",
    "# Daily quality checks and scoring (scheduled)\n",
    "databricks bundle run market_data_quality_checker\n",
    "\n",
    "# Drift monitoring (scheduled weekly)\n",
    "databricks bundle run market_data_drift_monitor\n",
    "```\n",
    "\n",
    "**Alternative: Databricks Workflows**\n",
    "```python\n",
    "# In Databricks workflow notebook\n",
    "from databricks.labs.dqx.anomaly import train, has_no_anomalies\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "\n",
    "# Task 1: Train (runs weekly)\n",
    "if dbutils.widgets.get(\"task\") == \"train\":\n",
    "    train(df=spark.table(\"market_data_feed\"), ...)\n",
    "\n",
    "# Task 2: Score (runs hourly for real-time feeds)\n",
    "elif dbutils.widgets.get(\"task\") == \"score\":\n",
    "    dq_engine = DQEngine(ws)\n",
    "    checks = [has_no_anomalies(...)]\n",
    "    df_scored = dq_engine.apply_checks_by_metadata(df, checks)\n",
    "    df_scored.write.mode(\"append\").saveAsTable(\"scored_data\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score with confidence intervals\n",
    "checks_ensemble = [\n",
    "    has_no_anomalies(\n",
    "        model=\"market_data_ensemble\",\n",
    "        score_threshold=0.5,\n",
    "        include_confidence=True,  # Add standard deviation of scores\n",
    "        registry_table=f\"{catalog}.{schema_name}.anomaly_model_registry\"\n",
    "    )\n",
    "]\n",
    "\n",
    "df_ensemble_scored = dq_engine.apply_checks_by_metadata(df_market, checks_ensemble)\n",
    "\n",
    "print(\"üìä Anomalies with Confidence Intervals:\\n\")\n",
    "df_ensemble_scored.filter(F.col(\"anomaly_score\") >= 0.5).select(\n",
    "    \"symbol\", \"exchange\", \"bid_price\", \"volume\",\n",
    "    F.round(\"anomaly_score\", 3).alias(\"score\"),\n",
    "    F.round(\"anomaly_score_std\", 3).alias(\"std_dev\")\n",
    ").orderBy(F.desc(\"anomaly_score\")).show(15, truncate=False)\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   - Low std_dev ‚Üí High confidence anomaly (all models agree)\")\n",
    "print(\"   - High std_dev ‚Üí Ambiguous case (models disagree, may need retraining)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary\n",
    "\n",
    "### What You Learned (Comprehensive 45-min Demo):\n",
    "\n",
    "1. ‚úÖ **Auto-Discovery vs Manual Tuning** - Zero-config start, then refine for financial data\n",
    "2. ‚úÖ **Parameter Tuning** - Higher n_estimators and max_samples for financial stability\n",
    "3. ‚úÖ **Temporal Patterns** - Datetime encoding captures trading hour patterns automatically\n",
    "4. ‚úÖ **Multi-Asset Monitoring** - Exchange-specific baselines (NASDAQ vs NYSE vs LSE)\n",
    "5. ‚úÖ **Performance Optimization** - row_filter and merge_columns for high-frequency data\n",
    "6. ‚úÖ **Feature Contributions** - SHAP-based triage (pricing vs lag vs volume)\n",
    "7. ‚úÖ **Ensemble Models** - Confidence intervals for ambiguous cases\n",
    "8. ‚úÖ **Drift Detection** - Detect market regime changes, automated retraining signals\n",
    "9. ‚úÖ **Production Integration** - DQEngine + YAML workflows + quarantine\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Start simple**: Auto-discovery first, then tune for financial data requirements\n",
    "- **Temporal encoding**: Hour-of-day patterns captured via cyclical sin/cos features\n",
    "- **Exchange segmentation**: Critical for multi-venue data with different characteristics\n",
    "- **Performance**: row_filter + merge_columns essential for high-frequency scoring (millions of ticks)\n",
    "- **Ensembles**: Use for high-stakes financial data to quantify model uncertainty\n",
    "- **Drift monitoring**: Market regimes change - set up automated retraining triggers\n",
    "- **Triage**: Feature contributions enable automated routing (feed team vs pricing team vs compliance)\n",
    "- **Quarantine workflow**: Automate investigation with root cause explanations\n",
    "\n",
    "### Model Comparison Results:\n",
    "\n",
    "| Approach | Use Case | Configuration |\n",
    "|----------|----------|---------------|\n",
    "| Auto-discovery | Quick start, exploration | Zero config, auto column selection |\n",
    "| Manual tuned | Production, optimal performance | n_estimators=200, max_samples=1024 |\n",
    "| Exchange-segmented | Multi-venue monitoring | Separate baselines per exchange |\n",
    "| Ensemble | High-stakes decisions | ensemble_size=3, includes confidence intervals |\n",
    "\n",
    "### Production Deployment Checklist:\n",
    "\n",
    "- ‚úÖ Train models per exchange with tuned hyperparameters\n",
    "- ‚úÖ Set up drift monitoring (drift_threshold=3.0, alert on warnings)\n",
    "- ‚úÖ Configure quarantine workflow with automated triage\n",
    "- ‚úÖ Use row_filter + merge_columns for high-frequency scoring\n",
    "- ‚úÖ Enable include_contributions for investigation\n",
    "- ‚úÖ Schedule retraining: weekly/monthly or on drift detection\n",
    "- ‚úÖ Integrate with alerting (PagerDuty, Slack) for critical anomalies\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Apply to your data**: `train(df=spark.table(\"your_market_data\"))`\n",
    "2. **Set up YAML workflows**: Copy configuration above, customize for your tables\n",
    "3. **Configure quarantine**: Route anomalies to appropriate teams by contribution\n",
    "4. **Monitor drift**: Set up dashboards for drift_score, automate retraining\n",
    "5. **Optimize performance**: Use merge_columns with your primary keys\n",
    "6. **Test ensemble**: Evaluate if confidence intervals help decision-making\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [DQX Anomaly Detection Documentation](https://databrickslabs.github.io/dqx/guide/anomaly_detection)\n",
    "- [Performance Optimization Guide](https://databrickslabs.github.io/dqx/guide/anomaly_detection#performance-optimization)\n",
    "- [API Reference](https://databrickslabs.github.io/dqx/reference/quality_checks#has_no_anomalies)\n",
    "- [GitHub Repository](https://github.com/databrickslabs/dqx)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions? Feedback?** Open an issue on GitHub or contact the DQX team!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
