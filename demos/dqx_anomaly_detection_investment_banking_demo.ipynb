{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üè¶ Investment Banking Market Data Quality - Anomaly Detection Demo\n",
        "\n",
        "## Business Context\n",
        "\n",
        "**Scenario**: Monitor real-time market data feeds to detect:\n",
        "- Pricing anomalies and stale quotes\n",
        "- Volume spikes indicating market manipulation\n",
        "- Feed latency issues\n",
        "- Off-hours trading anomalies\n",
        "\n",
        "**Data**: Market data feed with prices, volumes, spreads, and timestamps across multiple exchanges\n",
        "\n",
        "## What You'll Learn (30-45 min)\n",
        "\n",
        "1. **Temporal Patterns**: Datetime encoding for trading hours\n",
        "2. **Multi-Asset Monitoring**: Exchange-specific baselines\n",
        "3. **Performance Optimization**: row_filter and merge_columns for high-frequency data\n",
        "4. **Feature Contributions**: Triaging different anomaly types\n",
        "5. **Ensemble Models**: Confidence intervals for financial data\n",
        "6. **Production Integration**: Streaming batch patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1: Setup & Data Generation (5 min)\n",
        "\n",
        "First, install DQX with anomaly support if not already installed:\n",
        "```bash\n",
        "%pip install databricks-labs-dqx[anomaly]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from databricks.labs.dqx.anomaly import AnomalyEngine, has_no_anomalies, AnomalyParams, IsolationForestConfig\n",
        "from databricks.labs.dqx.engine import DQEngine\n",
        "from databricks.labs.dqx.rule import DQDatasetRule, DQRowRule\n",
        "from databricks.sdk import WorkspaceClient\n",
        "\n",
        "# Initialize\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "ws = WorkspaceClient()\n",
        "anomaly_engine = AnomalyEngine(ws)\n",
        "dq_engine = DQEngine(ws)\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "print(f\"   Spark version: {spark.version}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Realistic Market Data\n",
        "\n",
        "We'll create high-frequency market data with:\n",
        "- **Mixed data types**: Numeric (prices, volume), categorical (exchange, symbol), datetime (timestamp), boolean (is_official_hours)\n",
        "- **Exchange-specific patterns**: Different baselines for NASDAQ, NYSE, LSE\n",
        "- **Injected anomalies**: Pricing errors, stale quotes, volume spikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate market data feed\n",
        "def generate_market_data(num_rows=2000, anomaly_rate=0.05):\n",
        "    data = []\n",
        "    exchanges = [\"NASDAQ\", \"NYSE\", \"LSE\"]\n",
        "    symbols = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"META\", \"NVDA\", \"JPM\", \"BAC\", \"GS\"]\n",
        "    \n",
        "    # Exchange-specific baseline patterns\n",
        "    exchange_patterns = {\n",
        "        \"NASDAQ\": {\"base_price\": 150, \"spread_bp\": 5, \"volume\": 50000, \"lag_ms\": 15},\n",
        "        \"NYSE\": {\"base_price\": 100, \"spread_bp\": 8, \"volume\": 30000, \"lag_ms\": 20},\n",
        "        \"LSE\": {\"base_price\": 80, \"spread_bp\": 12, \"volume\": 20000, \"lag_ms\": 35},\n",
        "    }\n",
        "    \n",
        "    start_time = datetime(2024, 12, 1, 9, 30)  # Market open\n",
        "    \n",
        "    for i in range(num_rows):\n",
        "        exchange = random.choice(exchanges)\n",
        "        symbol = random.choice(symbols)\n",
        "        pattern = exchange_patterns[exchange]\n",
        "        \n",
        "        # Generate timestamp (mostly during trading hours)\n",
        "        minutes_offset = random.randint(0, 390)  # 6.5 hours = 390 min\n",
        "        timestamp = start_time + timedelta(minutes=minutes_offset)\n",
        "        \n",
        "        # Trading hours: 9:30-16:00\n",
        "        is_official_hours = 9.5 <= timestamp.hour + timestamp.minute/60 <= 16\n",
        "        \n",
        "        # Normal patterns\n",
        "        if random.random() > anomaly_rate:\n",
        "            base = pattern[\"base_price\"] * random.uniform(0.95, 1.05)\n",
        "            spread_bp = max(1, pattern[\"spread_bp\"] + np.random.normal(0, 2))\n",
        "            spread = base * (spread_bp / 10000)\n",
        "            \n",
        "            bid_price = round(base, 2)\n",
        "            ask_price = round(base + spread, 2)\n",
        "            volume = int(np.random.normal(pattern[\"volume\"], pattern[\"volume\"] * 0.3))\n",
        "            lag_ms = int(np.random.normal(pattern[\"lag_ms\"], 5))\n",
        "        else:\n",
        "            # Inject anomalies\n",
        "            anomaly_type = random.choice([\"pricing_error\", \"stale_quote\", \"volume_spike\", \"off_hours\"])\n",
        "            \n",
        "            if anomaly_type == \"pricing_error\":\n",
        "                base = pattern[\"base_price\"] * random.uniform(1.5, 3.0)  # 50-200% price jump\n",
        "                spread = base * random.uniform(0.05, 0.15)  # Wide spread\n",
        "                bid_price = round(base, 2)\n",
        "                ask_price = round(base + spread, 2)\n",
        "                volume = int(pattern[\"volume\"] * random.uniform(0.5, 1.0))\n",
        "                lag_ms = int(pattern[\"lag_ms\"])\n",
        "            \n",
        "            elif anomaly_type == \"stale_quote\":\n",
        "                base = pattern[\"base_price\"] * random.uniform(0.95, 1.05)\n",
        "                spread = base * (pattern[\"spread_bp\"] / 10000)\n",
        "                bid_price = round(base, 2)\n",
        "                ask_price = round(base + spread, 2)\n",
        "                volume = int(pattern[\"volume\"] * random.uniform(0.8, 1.2))\n",
        "                lag_ms = int(pattern[\"lag_ms\"] * random.uniform(10, 30))  # 10-30x normal lag\n",
        "            \n",
        "            elif anomaly_type == \"volume_spike\":\n",
        "                base = pattern[\"base_price\"] * random.uniform(0.98, 1.02)\n",
        "                spread = base * (pattern[\"spread_bp\"] / 10000)\n",
        "                bid_price = round(base, 2)\n",
        "                ask_price = round(base + spread, 2)\n",
        "                volume = int(pattern[\"volume\"] * random.uniform(5, 15))  # 5-15x normal volume\n",
        "                lag_ms = int(pattern[\"lag_ms\"])\n",
        "            \n",
        "            else:  # off_hours\n",
        "                base = pattern[\"base_price\"] * random.uniform(0.95, 1.05)\n",
        "                spread = base * (pattern[\"spread_bp\"] / 10000)\n",
        "                bid_price = round(base, 2)\n",
        "                ask_price = round(base + spread, 2)\n",
        "                volume = int(pattern[\"volume\"] * random.uniform(0.1, 0.3))\n",
        "                lag_ms = int(pattern[\"lag_ms\"])\n",
        "                # Force off-hours timestamp\n",
        "                timestamp = timestamp.replace(hour=random.choice([2, 3, 4, 22, 23]))\n",
        "                is_official_hours = False\n",
        "        \n",
        "        # Generate unique quote_id (primary key)\n",
        "        quote_id = f\"Q{i+1:06d}\"\n",
        "        \n",
        "        data.append((\n",
        "            quote_id,\n",
        "            symbol,\n",
        "            exchange,\n",
        "            timestamp,\n",
        "            bid_price,\n",
        "            ask_price,\n",
        "            int(max(100, volume)),\n",
        "            round(ask_price - bid_price, 3),\n",
        "            max(1, lag_ms),\n",
        "            is_official_hours\n",
        "        ))\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Generate data\n",
        "market_data = generate_market_data(num_rows=2000, anomaly_rate=0.05)\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"quote_id\", StringType(), False),\n",
        "    StructField(\"symbol\", StringType(), False),\n",
        "    StructField(\"exchange\", StringType(), False),\n",
        "    StructField(\"timestamp\", TimestampType(), False),\n",
        "    StructField(\"bid_price\", DoubleType(), False),\n",
        "    StructField(\"ask_price\", DoubleType(), False),\n",
        "    StructField(\"volume\", IntegerType(), False),\n",
        "    StructField(\"spread\", DoubleType(), False),\n",
        "    StructField(\"last_update_lag_ms\", IntegerType(), False),\n",
        "    StructField(\"is_official_hours\", BooleanType(), False),\n",
        "])\n",
        "\n",
        "df_market = spark.createDataFrame(market_data, schema)\n",
        "\n",
        "print(\"\\nüìä Sample of market data feed:\")\n",
        "display(df_market.orderBy(\"timestamp\"))\n",
        "print(f\"\\n‚úÖ Generated {df_market.count()} rows with ~5% injected anomalies\")\n",
        "print(f\"üí° Note: quote_id is the primary key for efficient merge operations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2: Auto-Discovery with Temporal Patterns (10 min)\n",
        "\n",
        "Train model with auto-discovery, which will automatically extract temporal features from the timestamp column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to table\n",
        "catalog = spark.sql(\"SELECT current_catalog()\").first()[0]\n",
        "schema_name = \"dqx_demo\"\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema_name}\")\n",
        "\n",
        "table_name = f\"{catalog}.{schema_name}.market_data_feed\"\n",
        "df_market.write.mode(\"overwrite\").saveAsTable(table_name)\n",
        "\n",
        "# Define unique registry table for this demo\n",
        "registry_table = f\"{catalog}.{schema_name}.anomaly_model_registry_investment_banking\"\n",
        "\n",
        "# Clean up old table if it exists (ensures new nested schema)\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {registry_table}\")\n",
        "print(f\"üóëÔ∏è  Cleaned up old registry table (if existed)\")\n",
        "\n",
        "print(f\"‚úÖ Data saved to: {table_name}\")\n",
        "print(f\"üìã Model registry: {registry_table}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with ZERO configuration (auto-discovery)\n",
        "print(\"üéØ Training with AUTO-DISCOVERY (zero config)...\\n\")\n",
        "\n",
        "model_uri_auto = anomaly_engine.train(\n",
        "    df=spark.table(table_name),\n",
        "    # NO columns specified - auto-discovered!\n",
        "    model_name=\"market_data_auto\",  # Specify name for later reference\n",
        "    registry_table=registry_table\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Auto-discovery model trained!\")\n",
        "print(f\"   Model URI: {model_uri_auto}\")\n",
        "\n",
        "# Check what was auto-discovered\n",
        "registry_df = spark.table(registry_table)\n",
        "auto_model = registry_df.filter(F.col(\"identity.model_uri\") == model_uri_auto).first()\n",
        "\n",
        "print(f\"\\nüìã Auto-Discovered Configuration:\")\n",
        "print(f\"   Columns: {auto_model['training']['columns']}\")\n",
        "print(f\"   Column types: {auto_model['features']['column_types']}\")\n",
        "print(f\"\\nüí° Datetime columns automatically encoded as 5 cyclical features:\")\n",
        "print(f\"   - hour_sin, hour_cos (daily cycle)\")\n",
        "print(f\"   - day_of_week_sin, day_of_week_cos (weekly cycle)\")\n",
        "print(f\"   - is_weekend (binary)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score with auto-discovered model\n",
        "checks_auto = [\n",
        "    DQDatasetRule(\n",
        "        check_func=has_no_anomalies,\n",
        "        check_func_kwargs={\n",
        "            \"merge_columns\": [\"quote_id\"],  # Simple primary key\n",
        "            \"model\": \"market_data_auto\",  # Reference the auto-discovered model\n",
        "            \"score_threshold\": 0.5,\n",
        "            \"registry_table\": registry_table\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "df_scored_auto = dq_engine.apply_checks(df_market, checks_auto)\n",
        "anomalies_auto = df_scored_auto.filter(F.col(\"_info.anomaly.score\") >= 0.5)\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  Auto-discovery found {anomalies_auto.count()} anomalies:\\n\")\n",
        "display(anomalies_auto.orderBy(F.col(\"_info.anomaly.score\").desc()).select(\n",
        "    \"symbol\", \"exchange\", \"timestamp\", \"bid_price\", \"volume\",\n",
        "    F.round(\"_info.anomaly.score\", 3).alias(\"score\")\n",
        ").limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Manual Column Selection & Parameter Tuning\n",
        "\n",
        "Now let's manually select columns and tune hyperparameters for financial data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with MANUAL configuration and tuned parameters\n",
        "print(\"üéØ Training with MANUAL tuning for financial data...\\n\")\n",
        "\n",
        "model_uri_manual = anomaly_engine.train(\n",
        "    df=spark.table(table_name),\n",
        "    columns=[\"bid_price\", \"ask_price\", \"spread\", \"volume\", \"last_update_lag_ms\", \"timestamp\"],  # Manual selection\n",
        "    model_name=\"market_data_tuned\",\n",
        "    params=AnomalyParams(\n",
        "        algorithm_config=IsolationForestConfig(\n",
        "            contamination=0.05,  # Expected 5% anomaly rate\n",
        "            num_trees=200,    # More trees for financial data stability\n",
        "            subsampling_rate=1024,    # Larger subsample for accuracy\n",
        "            random_seed=42\n",
        "        ),\n",
        "        sample_fraction=1.0,\n",
        "    ),\n",
        "    registry_table=registry_table\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Manual tuned model trained!\")\n",
        "print(f\"   Model URI: {model_uri_manual}\")\n",
        "print(f\"\\nüí° Financial Data Tuning:\")\n",
        "print(f\"   ‚Ä¢ Higher num_trees (200) for stability\")\n",
        "print(f\"   ‚Ä¢ Larger subsampling_rate (1024) for accuracy\")\n",
        "print(f\"   ‚Ä¢ Datetime features automatically extracted\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3: Multi-Asset Monitoring (8 min)\n",
        "\n",
        "Different exchanges have different characteristics. Train per-exchange models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with exchange segmentation\n",
        "print(\"üåç Training exchange-specific models...\\n\")\n",
        "\n",
        "model_uri_segmented = anomaly_engine.train(\n",
        "    df=spark.table(table_name),\n",
        "    columns=[\"bid_price\", \"ask_price\", \"spread\", \"volume\", \"last_update_lag_ms\"],\n",
        "    segment_by=[\"exchange\"],\n",
        "    model_name=\"market_data_by_exchange\",\n",
        "    registry_table=registry_table\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Exchange-specific models trained!\")\n",
        "\n",
        "# Show exchange baselines\n",
        "exchange_models = spark.table(registry_table).filter(\n",
        "    F.col(\"identity.model_name\") == \"market_data_by_exchange\"\n",
        ")\n",
        "\n",
        "print(\"\\nüìä Exchange-Specific Baselines:\\n\")\n",
        "display(exchange_models.select(\"segment_values.exchange\", \"training.training_rows\", \"training.baseline_stats\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4: High-Frequency Scoring with Performance (8 min)\n",
        "\n",
        "Optimize scoring for high-frequency data using row_filter and merge_columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score only official trading hours data with optimized joins\n",
        "checks_optimized = [\n",
        "    DQDatasetRule(\n",
        "        check_func=has_no_anomalies,\n",
        "        check_func_kwargs={\n",
        "            \"merge_columns\": [\"quote_id\"],  # Simple primary key\n",
        "            \"model\": \"market_data_by_exchange\",\n",
        "            \"score_threshold\": 0.5,\n",
        "            \"row_filter\": \"is_official_hours = true\",  # Filter before scoring\n",
        "            \"registry_table\": registry_table\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "df_scored_optimized = dq_engine.apply_checks(df_market, checks_optimized)\n",
        "print(f\"‚úÖ Scored {df_scored_optimized.count()} rows with optimized join\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 5: Feature Contributions for Investigation (5 min)\n",
        "\n",
        "Use SHAP to understand which features drove anomaly scores and triage by type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score with feature contributions\n",
        "checks_contrib = [\n",
        "    DQDatasetRule(\n",
        "        check_func=has_no_anomalies,\n",
        "        check_func_kwargs={\n",
        "            \"merge_columns\": [\"quote_id\"],  # Simple primary key\n",
        "            \"model\": \"market_data_by_exchange\",\n",
        "            \"score_threshold\": 0.5,\n",
        "            \"include_contributions\": True,\n",
        "            \"registry_table\": registry_table\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "df_with_contrib = dq_engine.apply_checks(df_market, checks_contrib)\n",
        "\n",
        "print(\"üîç Top Anomalies with Feature Contributions:\\n\")\n",
        "anomalies_contrib = df_with_contrib.filter(F.col(\"_info.anomaly.score\") >= 0.5).orderBy(\n",
        "    F.col(\"_info.anomaly.score\").desc()\n",
        ").limit(10)\n",
        "\n",
        "display(anomalies_contrib.select(\n",
        "    \"symbol\", \"exchange\", \"bid_price\", \"volume\", \"last_update_lag_ms\",\n",
        "    F.round(\"_info.anomaly.score\", 3).alias(\"score\"),\n",
        "    \"_info.anomaly.contributions\"))\n",
        "\n",
        "print(\"\\nüí° Triage by contribution pattern:\")\n",
        "print(\"   - High 'last_update_lag_ms' ‚Üí Stale quote, route to data feed team\")\n",
        "print(\"   - High 'bid_price' + 'spread' ‚Üí Pricing error, route to pricing team\")\n",
        "print(\"   - High 'volume' ‚Üí Potential manipulation, route to compliance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6: Ensemble Models & Confidence (4 min)\n",
        "\n",
        "For financial data, use ensemble models to get confidence intervals on anomaly scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train ensemble model (3 models)\n",
        "print(\"üé≤ Training ensemble model (3 members)...\\n\")\n",
        "\n",
        "model_uri_ensemble = anomaly_engine.train(\n",
        "    df=spark.table(table_name),\n",
        "    columns=[\"bid_price\", \"ask_price\", \"spread\", \"volume\", \"last_update_lag_ms\"],\n",
        "    segment_by=[\"exchange\"],\n",
        "    model_name=\"market_data_ensemble\",\n",
        "    params=AnomalyParams(\n",
        "        ensemble_size=3  # 3-model ensemble\n",
        "    ),\n",
        "    registry_table=registry_table\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Ensemble trained!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 7: Drift Detection & Retraining (6 min)\n",
        "\n",
        "Market conditions change. Detect when models become stale and need retraining.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate market regime change (increased volatility, wider spreads)\n",
        "def generate_volatile_market_data(num_rows=300):\n",
        "    \"\"\"Generate data with shifted volatility (market stress period).\"\"\"\n",
        "    data = []\n",
        "    exchanges = [\"NASDAQ\", \"NYSE\", \"LSE\"]\n",
        "    symbols = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"META\", \"NVDA\", \"JPM\", \"BAC\", \"GS\"]\n",
        "    \n",
        "    # NEW PATTERNS: Higher volatility, wider spreads, increased volume\n",
        "    volatile_patterns = {\n",
        "        \"NASDAQ\": {\"base_price\": 150, \"spread_bp\": 15, \"volume\": 75000, \"lag_ms\": 25},  # +200% spread\n",
        "        \"NYSE\": {\"base_price\": 100, \"spread_bp\": 20, \"volume\": 45000, \"lag_ms\": 35},    # +150% spread\n",
        "        \"LSE\": {\"base_price\": 80, \"spread_bp\": 25, \"volume\": 30000, \"lag_ms\": 50},      # +108% spread\n",
        "    }\n",
        "    \n",
        "    start_time = datetime(2024, 12, 15, 9, 30)  # New period\n",
        "    \n",
        "    for i in range(num_rows):\n",
        "        exchange = random.choice(exchanges)\n",
        "        symbol = random.choice(symbols)\n",
        "        pattern = volatile_patterns[exchange]\n",
        "        \n",
        "        minutes_offset = random.randint(0, 390)\n",
        "        timestamp = start_time + timedelta(minutes=minutes_offset)\n",
        "        is_official_hours = 9.5 <= timestamp.hour + timestamp.minute/60 <= 16\n",
        "        \n",
        "        base = pattern[\"base_price\"] * random.uniform(0.90, 1.10)  # +100% price volatility\n",
        "        spread_bp = max(1, pattern[\"spread_bp\"] + np.random.normal(0, 5))\n",
        "        spread = base * (spread_bp / 10000)\n",
        "        \n",
        "        bid_price = round(base, 2)\n",
        "        ask_price = round(base + spread, 2)\n",
        "        volume = int(np.random.normal(pattern[\"volume\"], pattern[\"volume\"] * 0.4))  # +33% vol volatility\n",
        "        lag_ms = int(np.random.normal(pattern[\"lag_ms\"], 10))  # +100% lag variability\n",
        "        \n",
        "        # Generate unique quote_id\n",
        "        quote_id = f\"Q{2000+i+1:06d}\"  # Continue from main dataset\n",
        "        \n",
        "        data.append((quote_id, symbol, exchange, timestamp, bid_price, ask_price, int(max(100, volume)), \n",
        "                    round(ask_price - bid_price, 3), max(1, lag_ms), is_official_hours))\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Generate volatile market data\n",
        "volatile_data = generate_volatile_market_data(num_rows=300)\n",
        "df_volatile = spark.createDataFrame(volatile_data, schema)\n",
        "\n",
        "print(\"üìä Normal vs Volatile Market Comparison:\\n\")\n",
        "print(\"Normal Market (original):\")\n",
        "display(df_market.agg(\n",
        "    F.avg(\"spread\").alias(\"avg_spread\"),\n",
        "    F.avg(\"volume\").alias(\"avg_volume\"),\n",
        "    F.avg(\"last_update_lag_ms\").alias(\"avg_lag\")))\n",
        "\n",
        "print(\"Volatile Market (stress period):\")\n",
        "display(df_volatile.agg(\n",
        "    F.avg(\"spread\").alias(\"avg_spread\"),\n",
        "    F.avg(\"volume\").alias(\"avg_volume\"),\n",
        "    F.avg(\"last_update_lag_ms\").alias(\"avg_lag\")))\n",
        "\n",
        "print(\"‚úÖ Market regime changed:\")\n",
        "print(\"   ‚Ä¢ Spreads: +150-200% (market stress)\")\n",
        "print(\"   ‚Ä¢ Volume: +50% (panic trading)\")\n",
        "print(\"   ‚Ä¢ Latency: +67% (system overload)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score volatile data with drift detection\n",
        "checks_with_drift = [\n",
        "    DQDatasetRule(\n",
        "        check_func=has_no_anomalies,\n",
        "        check_func_kwargs={\n",
        "            \"merge_columns\": [\"quote_id\"],  # Simple primary key\n",
        "            \"model\": \"market_data_by_exchange\",\n",
        "            \"drift_threshold\": 3.0,  # Z-score threshold\n",
        "            \"registry_table\": registry_table\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"üîç Scoring volatile market data (watch for drift warnings)...\\n\")\n",
        "df_drift_scored = dq_engine.apply_checks(df_volatile, checks_with_drift)\n",
        "\n",
        "print(\"\\n‚ÑπÔ∏è  Drift warnings indicate distribution shift!\")\n",
        "print(\"   Example: 'Data drift detected in columns: spread, last_update_lag_ms (drift score: 5.3)'\")\n",
        "print(\"   Action: Retrain model to adapt to new market regime\")\n",
        "print(\"\\nüí° In production: Set up alerts when drift_score > 3.0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrain with combined data (normal + volatile periods)\n",
        "df_combined = df_market.union(df_volatile)\n",
        "\n",
        "print(\"üîÑ Retraining model with combined market conditions...\\n\")\n",
        "\n",
        "model_uri_retrained = anomaly_engine.train(\n",
        "    df=df_combined,\n",
        "    columns=[\"bid_price\", \"ask_price\", \"spread\", \"volume\", \"last_update_lag_ms\"],\n",
        "    segment_by=[\"exchange\"],\n",
        "    model_name=\"market_data_by_exchange\",  # Same name = new version\n",
        "    params=AnomalyParams(\n",
        "        algorithm_config=IsolationForestConfig(\n",
        "            contamination=0.05,\n",
        "            num_trees=200,\n",
        "            subsampling_rate=1024,\n",
        "            random_seed=42\n",
        "        )\n",
        "    ),\n",
        "    registry_table=registry_table\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Model retrained!\")\n",
        "print(\"   ‚Ä¢ Old model automatically archived\")\n",
        "print(\"   ‚Ä¢ New model adapts to both normal and volatile market conditions\")\n",
        "print(\"   ‚Ä¢ Baseline now includes wider spreads and higher volatility\")\n",
        "print(\"\\nüí° Best Practice:\")\n",
        "print(\"   ‚Ä¢ Monitor drift_score in production dashboards\")\n",
        "print(\"   ‚Ä¢ Set up automated retraining when drift > threshold\")\n",
        "print(\"   ‚Ä¢ Retrain quarterly or when market regimes change\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 8: Production Integration & Quarantine (6 min)\n",
        "\n",
        "Integrate anomaly detection into production workflows with automated quarantine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine anomaly detection with traditional DQ checks\n",
        "from databricks.labs.dqx.check_funcs import is_not_null, is_in_range\n",
        "\n",
        "checks_combined = [\n",
        "    # Traditional data quality checks\n",
        "    DQRowRule(check_func=is_not_null, check_func_kwargs={\"column\": \"symbol\"}),\n",
        "    DQRowRule(check_func=is_not_null, check_func_kwargs={\"column\": \"exchange\"}),\n",
        "    DQRowRule(check_func=is_not_null, check_func_kwargs={\"column\": \"timestamp\"}),\n",
        "    DQRowRule(check_func=is_in_range, check_func_kwargs={\"column\": \"bid_price\", \"min_limit\": 0, \"max_limit\": 10000}),\n",
        "    DQRowRule(check_func=is_in_range, check_func_kwargs={\"column\": \"spread\", \"min_limit\": 0, \"max_limit\": 100}),\n",
        "    DQRowRule(check_func=is_in_range, check_func_kwargs={\"column\": \"last_update_lag_ms\", \"min_limit\": 0, \"max_limit\": 5000}),\n",
        "    \n",
        "    # ML-based anomaly detection with explanations\n",
        "    DQDatasetRule(\n",
        "        check_func=has_no_anomalies,\n",
        "        check_func_kwargs={\n",
        "            \"model\": \"market_data_by_exchange\",\n",
        "            \"score_threshold\": 0.5,\n",
        "            \"include_contributions\": True,  # For root cause\n",
        "            \"drift_threshold\": 3.0,\n",
        "            \"merge_columns\": [\"quote_id\"],  # Simple primary key for efficient merge\n",
        "            \"registry_table\": registry_table\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "# Apply all checks in single pass\n",
        "df_full_dq = dq_engine.apply_checks(df_market, checks_combined)\n",
        "\n",
        "print(\"üìä Full Data Quality Summary:\\n\")\n",
        "total_rows = df_full_dq.count()\n",
        "anomalies_found = df_full_dq.filter(F.col(\"_info.anomaly.score\") >= 0.5).count()\n",
        "\n",
        "print(f\"Total Rows: {total_rows}\")\n",
        "print(f\"Anomalies Detected: {anomalies_found}\")\n",
        "print(f\"Clean Records: {total_rows - anomalies_found}\")\n",
        "print(f\"Anomaly Rate: {(anomalies_found/total_rows)*100:.2f}%\")\n",
        "print(f\"\\n‚úÖ All checks (traditional + ML) applied in single pass!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quarantine anomalies for investigation\n",
        "quarantine_table = f\"{catalog}.{schema_name}.market_data_quarantine\"\n",
        "\n",
        "quarantine_df = df_full_dq.filter(\n",
        "    F.col(\"_info.anomaly.score\") >= 0.5\n",
        ").select(\n",
        "    \"*\",\n",
        "    F.current_timestamp().alias(\"quarantine_timestamp\"),\n",
        "    F.lit(\"anomaly_detected\").alias(\"quarantine_reason\"),\n",
        "    # Extract top contributor for triage (using element_at with 1-based indexing)\n",
        "    # Extract feature with highest contribution value (not just first key)\n",
        "    F.expr(\"\"\"\n",
        "        element_at(\n",
        "            array_sort(\n",
        "                transform(\n",
        "                    map_entries(`_info.anomaly.contributions`),\n",
        "                    x -> named_struct('feature', x.key, 'contrib', x.value)\n",
        "                ),\n",
        "                (left, right) -> case \n",
        "                    when left.contrib > right.contrib then -1 \n",
        "                    when left.contrib < right.contrib then 1 \n",
        "                    else 0 \n",
        "                end\n",
        "            ),\n",
        "            1\n",
        "        ).feature\n",
        "    \"\"\").alias(\"top_contributor\")\n",
        ")\n",
        "\n",
        "quarantine_df.write.mode(\"overwrite\").saveAsTable(quarantine_table)\n",
        "\n",
        "print(f\"‚úÖ Quarantined {quarantine_df.count()} anomalies to: {quarantine_table}\")\n",
        "print(\"\\nüìã Quarantine Summary by Exchange and Issue Type:\")\n",
        "display(spark.table(quarantine_table).groupBy(\"exchange\", \"top_contributor\").agg(\n",
        "    F.count(\"*\").alias(\"count\"),\n",
        "    F.avg(\"_info.anomaly.score\").alias(\"avg_score\")\n",
        ").orderBy(\"exchange\", F.desc(\"count\")))\n",
        "\n",
        "print(\"\\nüí° Automated Triage Workflow:\")\n",
        "print(\"   1. Anomalies automatically quarantined with contributions\")\n",
        "print(\"   2. Route by top_contributor:\")\n",
        "print(\"      ‚Ä¢ 'last_update_lag_ms' ‚Üí Data feed team\")\n",
        "print(\"      ‚Ä¢ 'bid_price' or 'spread' ‚Üí Pricing team\")\n",
        "print(\"      ‚Ä¢ 'volume' ‚Üí Compliance/surveillance team\")\n",
        "print(\"   3. Investigate using _info.anomaly.contributions map\")\n",
        "print(\"   4. False positives ‚Üí Adjust threshold or retrain\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### YAML Configuration for Production\n",
        "\n",
        "For automated workflows, define checks in YAML:\n",
        "\n",
        "```yaml\n",
        "run_configs:\n",
        "  - name: market_data_quality_monitoring\n",
        "    input_config:\n",
        "      location: catalog.schema.market_data_feed\n",
        "    \n",
        "    # Traditional checks\n",
        "    quality_checks:\n",
        "      - function: is_not_null\n",
        "        arguments:\n",
        "          columns: [symbol, exchange, timestamp]\n",
        "      - function: is_in_range\n",
        "        arguments:\n",
        "          column: bid_price\n",
        "          min_value: 0\n",
        "          max_value: 10000\n",
        "      - function: is_in_range\n",
        "        arguments:\n",
        "          column: spread\n",
        "          min_value: 0\n",
        "          max_value: 100\n",
        "    \n",
        "    # Anomaly detection\n",
        "    anomaly_config:\n",
        "      columns: [bid_price, ask_price, spread, volume, last_update_lag_ms, timestamp]\n",
        "      segment_by: [exchange]\n",
        "      model_name: market_data_by_exchange\n",
        "      registry_table: catalog.schema.anomaly_model_registry\n",
        "      params:\n",
        "        isolation_forest:\n",
        "          contamination: 0.05\n",
        "          num_trees: 200\n",
        "          subsampling_rate: 1024\n",
        "          random_seed: 42\n",
        "        sample_fraction: 1.0\n",
        "      \n",
        "      # Scoring options\n",
        "      score_options:\n",
        "        score_threshold: 0.5\n",
        "        include_contributions: true\n",
        "        drift_threshold: 3.0\n",
        "        row_filter: \"is_official_hours = true\"\n",
        "        merge_columns: [quote_id]\n",
        "    \n",
        "    # Quarantine configuration\n",
        "    quarantine_config:\n",
        "      enabled: true\n",
        "      table: catalog.schema.market_data_quarantine\n",
        "      condition: \"anomaly_score >= 0.5\"\n",
        "      \n",
        "    # Output configuration\n",
        "    output_config:\n",
        "      location: catalog.schema.market_data_clean\n",
        "      save_mode: overwrite\n",
        "```\n",
        "\n",
        "**Run with Databricks Asset Bundles:**\n",
        "```bash\n",
        "# Initial model training (one-time or scheduled monthly)\n",
        "databricks bundle run market_data_anomaly_trainer\n",
        "\n",
        "# Daily quality checks and scoring (scheduled)\n",
        "databricks bundle run market_data_quality_checker\n",
        "\n",
        "# Drift monitoring (scheduled weekly)\n",
        "databricks bundle run market_data_drift_monitor\n",
        "```\n",
        "\n",
        "**Alternative: Databricks Workflows**\n",
        "```python\n",
        "# In Databricks workflow notebook\n",
        "from databricks.labs.dqx.anomaly import train, has_no_anomalies\n",
        "from databricks.labs.dqx.engine import DQEngine\n",
        "\n",
        "# Task 1: Train (runs weekly)\n",
        "if dbutils.widgets.get(\"task\") == \"train\":\n",
        "    train(df=spark.table(\"market_data_feed\"), ...)\n",
        "\n",
        "# Task 2: Score (runs hourly for real-time feeds)\n",
        "elif dbutils.widgets.get(\"task\") == \"score\":\n",
        "    dq_engine = DQEngine(ws)\n",
        "    checks = [has_no_anomalies(...)]\n",
        "    df_scored = dq_engine.apply_checks(df, checks)\n",
        "    df_scored.write.mode(\"append\").saveAsTable(\"scored_data\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score with confidence intervals\n",
        "checks_ensemble = [\n",
        "    DQDatasetRule(\n",
        "        check_func=has_no_anomalies,\n",
        "        check_func_kwargs={\n",
        "            \"merge_columns\": [\"quote_id\"],  # Simple primary key\n",
        "            \"model\": \"market_data_ensemble\",\n",
        "            \"score_threshold\": 0.5,\n",
        "            \"include_confidence\": True,  # Add standard deviation of scores\n",
        "            \"registry_table\": registry_table\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "df_ensemble_scored = dq_engine.apply_checks(df_market, checks_ensemble)\n",
        "\n",
        "print(\"üìä Anomalies with Confidence Intervals:\\n\")\n",
        "display(df_ensemble_scored.filter(F.col(\"_info.anomaly.score\") >= 0.5).select(\n",
        "    \"symbol\", \"exchange\", \"bid_price\", \"volume\",\n",
        "    F.round(\"_info.anomaly.score\", 3).alias(\"score\"),\n",
        "    F.round(\"_info.anomaly.confidence_std\", 3).alias(\"std_dev\")\n",
        ").orderBy(F.desc(\"_info.anomaly.score\")))\n",
        "\n",
        "print(\"\\nüí° Interpretation:\")\n",
        "print(\"   - Low std_dev ‚Üí High confidence anomaly (all models agree)\")\n",
        "print(\"   - High std_dev ‚Üí Ambiguous case (models disagree, may need retraining)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Summary\n",
        "\n",
        "### What You Learned (Comprehensive 45-min Demo):\n",
        "\n",
        "1. ‚úÖ **Auto-Discovery vs Manual Tuning** - Zero-config start, then refine for financial data\n",
        "2. ‚úÖ **Parameter Tuning** - Higher num_trees and subsampling_rate for financial stability\n",
        "3. ‚úÖ **Temporal Patterns** - Datetime encoding captures trading hour patterns automatically\n",
        "4. ‚úÖ **Multi-Asset Monitoring** - Exchange-specific baselines (NASDAQ vs NYSE vs LSE)\n",
        "5. ‚úÖ **Performance Optimization** - row_filter and merge_columns for high-frequency data\n",
        "6. ‚úÖ **Feature Contributions** - SHAP-based triage (pricing vs lag vs volume)\n",
        "7. ‚úÖ **Ensemble Models** - Confidence intervals for ambiguous cases\n",
        "8. ‚úÖ **Drift Detection** - Detect market regime changes, automated retraining signals\n",
        "9. ‚úÖ **Production Integration** - DQEngine + YAML workflows + quarantine\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "- **Start simple**: Auto-discovery first, then tune for financial data requirements\n",
        "- **Temporal encoding**: Hour-of-day patterns captured via cyclical sin/cos features\n",
        "- **Exchange segmentation**: Critical for multi-venue data with different characteristics\n",
        "- **Performance**: row_filter + merge_columns essential for high-frequency scoring (millions of ticks)\n",
        "- **Ensembles**: Use for high-stakes financial data to quantify model uncertainty\n",
        "- **Drift monitoring**: Market regimes change - set up automated retraining triggers\n",
        "- **Triage**: Feature contributions enable automated routing (feed team vs pricing team vs compliance)\n",
        "- **Quarantine workflow**: Automate investigation with root cause explanations\n",
        "\n",
        "### Model Comparison Results:\n",
        "\n",
        "| Approach | Use Case | Configuration |\n",
        "|----------|----------|---------------|\n",
        "| Auto-discovery | Quick start, exploration | Zero config, auto column selection |\n",
        "| Manual tuned | Production, optimal performance | num_trees=200, subsampling_rate=1024 |\n",
        "| Exchange-segmented | Multi-venue monitoring | Separate baselines per exchange |\n",
        "| Ensemble | High-stakes decisions | ensemble_size=3, includes confidence intervals |\n",
        "\n",
        "### Production Deployment Checklist:\n",
        "\n",
        "- ‚úÖ Train models per exchange with tuned hyperparameters\n",
        "- ‚úÖ Set up drift monitoring (drift_threshold=3.0, alert on warnings)\n",
        "- ‚úÖ Configure quarantine workflow with automated triage\n",
        "- ‚úÖ Use row_filter + merge_columns for high-frequency scoring\n",
        "- ‚úÖ Enable include_contributions for investigation\n",
        "- ‚úÖ Schedule retraining: weekly/monthly or on drift detection\n",
        "- ‚úÖ Integrate with alerting (PagerDuty, Slack) for critical anomalies\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Apply to your data**: `train(df=spark.table(\"your_market_data\"))`\n",
        "2. **Set up YAML workflows**: Copy configuration above, customize for your tables\n",
        "3. **Configure quarantine**: Route anomalies to appropriate teams by contribution\n",
        "4. **Monitor drift**: Set up dashboards for drift_score, automate retraining\n",
        "5. **Optimize performance**: Use merge_columns with your primary keys\n",
        "6. **Test ensemble**: Evaluate if confidence intervals help decision-making\n",
        "\n",
        "### Resources:\n",
        "\n",
        "- [DQX Anomaly Detection Documentation](https://databrickslabs.github.io/dqx/guide/anomaly_detection)\n",
        "- [Performance Optimization Guide](https://databrickslabs.github.io/dqx/guide/anomaly_detection#performance-optimization)\n",
        "- [API Reference](https://databrickslabs.github.io/dqx/reference/quality_checks#has_no_anomalies)\n",
        "- [GitHub Repository](https://github.com/databrickslabs/dqx)\n",
        "\n",
        "---\n",
        "\n",
        "**Questions? Feedback?** Open an issue on GitHub or contact the DQX team!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
