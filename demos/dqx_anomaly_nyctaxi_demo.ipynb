{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöï DQX Anomaly Detection - NYC Taxi Data Quality\n",
        "\n",
        "## Find Real Data Quality Issues in Real Data\n",
        "\n",
        "This notebook demonstrates DQX anomaly detection on the **NYC Taxi dataset** - real trip data with **real data quality issues**.\n",
        "\n",
        "**What we'll do:**\n",
        "1. Load NYC Taxi data (available in Databricks samples)\n",
        "2. Train DQX anomaly detection model (unsupervised)\n",
        "3. Find anomalies and see they represent real DQ issues\n",
        "4. Understand WHY records are flagged\n",
        "\n",
        "**Time**: ~10 minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1: Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "from databricks.labs.dqx.anomaly import AnomalyEngine, has_no_anomalies\n",
        "from databricks.labs.dqx.engine import DQEngine\n",
        "from databricks.labs.dqx.rule import DQDatasetRule\n",
        "from databricks.sdk import WorkspaceClient\n",
        "\n",
        "# Initialize\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "ws = WorkspaceClient()\n",
        "anomaly_engine = AnomalyEngine(ws)\n",
        "dq_engine = DQEngine(ws)\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "print(f\"   Spark version: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2: Load NYC Taxi Data\n",
        "\n",
        "The NYC Taxi dataset is available in the Databricks `samples` catalog.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load NYC Taxi data from samples catalog\n",
        "# Try different locations based on workspace setup\n",
        "taxi_tables = [\n",
        "    \"samples.nyctaxi.trips\",\n",
        "]\n",
        "\n",
        "df_taxi = None\n",
        "for table in taxi_tables:\n",
        "    try:\n",
        "        df_taxi = spark.table(table)\n",
        "        print(f\"‚úÖ Loaded taxi data from: {table}\")\n",
        "        break\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "if df_taxi is None:\n",
        "    raise ValueError(\"Could not find NYC Taxi data. Check samples catalog.\")\n",
        "\n",
        "# Show schema\n",
        "print(f\"\\nüìã Schema:\")\n",
        "df_taxi.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample and prepare data for anomaly detection\n",
        "# Use 50K rows for quick demo (full dataset has millions)\n",
        "SAMPLE_SIZE = 50000\n",
        "\n",
        "print(f\"üîÑ Sampling {SAMPLE_SIZE:,} trips for demo...\\n\")\n",
        "\n",
        "# First, let's see what columns are available\n",
        "print(\"Available columns:\", df_taxi.columns)\n",
        "\n",
        "# Select relevant columns and add computed features\n",
        "# Note: Databricks samples.nyctaxi.trips schema varies - adjust as needed\n",
        "df_sample = (\n",
        "    df_taxi\n",
        "    .sample(fraction=0.1, seed=42)  # Random sample\n",
        "    .limit(SAMPLE_SIZE)\n",
        "    .withColumn(\"trip_id\", F.monotonically_increasing_id())  # Add ID for merging\n",
        "    .withColumn(\n",
        "        \"trip_duration_mins\",\n",
        "        (F.unix_timestamp(\"tpep_dropoff_datetime\") - F.unix_timestamp(\"tpep_pickup_datetime\")) / 60\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"speed_mph\",\n",
        "        F.when(F.col(\"trip_duration_mins\") > 0,\n",
        "               F.col(\"trip_distance\") / (F.col(\"trip_duration_mins\") / 60))\n",
        "        .otherwise(0)\n",
        "    )\n",
        "    .select(\n",
        "        \"trip_id\",\n",
        "        \"tpep_pickup_datetime\",\n",
        "        \"tpep_dropoff_datetime\",\n",
        "        \"trip_distance\",\n",
        "        \"trip_duration_mins\",\n",
        "        \"speed_mph\",\n",
        "        \"fare_amount\",\n",
        "        \"pickup_zip\",\n",
        "        \"dropoff_zip\"\n",
        "    )\n",
        "    .filter(F.col(\"trip_duration_mins\").isNotNull())  # Remove nulls\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Prepared {df_sample.count():,} trips\")\n",
        "print(f\"\\nüìä Sample data:\")\n",
        "display(df_sample.limit(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick stats to understand the data\n",
        "print(\"üìà Data Statistics:\\n\")\n",
        "\n",
        "stats = df_sample.select(\n",
        "    F.count(\"*\").alias(\"total_trips\"),\n",
        "    F.round(F.avg(\"trip_distance\"), 2).alias(\"avg_distance_mi\"),\n",
        "    F.round(F.avg(\"trip_duration_mins\"), 2).alias(\"avg_duration_mins\"),\n",
        "    F.round(F.avg(\"fare_amount\"), 2).alias(\"avg_fare\"),\n",
        "    F.round(F.avg(\"speed_mph\"), 2).alias(\"avg_speed_mph\"),\n",
        ").first()\n",
        "\n",
        "print(f\"Total trips: {stats['total_trips']:,}\")\n",
        "print(f\"Avg distance: {stats['avg_distance_mi']} miles\")\n",
        "print(f\"Avg duration: {stats['avg_duration_mins']} mins\")\n",
        "print(f\"Avg fare: ${stats['avg_fare']}\")\n",
        "print(f\"Avg speed: {stats['avg_speed_mph']} mph\")\n",
        "\n",
        "# Show potential issues already visible\n",
        "print(f\"\\nüîç Potential Data Quality Issues:\")\n",
        "\n",
        "issues = df_sample.select(\n",
        "    F.sum(F.when(F.col(\"fare_amount\") <= 0, 1).otherwise(0)).alias(\"zero_or_negative_fares\"),\n",
        "    F.sum(F.when(F.col(\"trip_distance\") <= 0, 1).otherwise(0)).alias(\"zero_distance_trips\"),\n",
        "    F.sum(F.when(F.col(\"speed_mph\") > 100, 1).otherwise(0)).alias(\"impossibly_fast_trips\"),\n",
        "    F.sum(F.when(F.col(\"trip_duration_mins\") > 180, 1).otherwise(0)).alias(\"very_long_trips_3hr_plus\"),\n",
        ").first()\n",
        "\n",
        "for col, count in issues.asDict().items():\n",
        "    if count > 0:\n",
        "        print(f\"   ‚ö†Ô∏è {col.replace('_', ' ').title()}: {count}\")\n",
        "\n",
        "print(f\"\\nüí° These are just obvious issues - DQX will find subtle patterns too!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to table for model training\n",
        "catalog = spark.sql(\"SELECT current_catalog()\").first()[0]\n",
        "schema_name = \"dqx_demo\"\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema_name}\")\n",
        "\n",
        "table_name = f\"{catalog}.{schema_name}.nyctaxi_sample\"\n",
        "df_sample.write.mode(\"overwrite\").saveAsTable(table_name)\n",
        "\n",
        "print(f\"‚úÖ Data saved to: {table_name}\")\n",
        "\n",
        "# Setup model registry\n",
        "registry_table = f\"{catalog}.{schema_name}.anomaly_model_registry_nyctaxi\"\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {registry_table}\")\n",
        "print(f\"üìã Model registry: {registry_table}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3: Train Anomaly Detection Model\n",
        "\n",
        "We'll train on numeric features that capture trip characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train anomaly detection model\n",
        "print(\"üéØ Training anomaly detection model...\\n\")\n",
        "\n",
        "# Select features that capture trip characteristics\n",
        "feature_columns = [\n",
        "    \"trip_distance\",\n",
        "    \"trip_duration_mins\",\n",
        "    \"speed_mph\",\n",
        "    \"fare_amount\",\n",
        "]\n",
        "\n",
        "model_uri = anomaly_engine.train(\n",
        "    df=spark.table(table_name),\n",
        "    columns=feature_columns,\n",
        "    model_name=\"nyctaxi_dq\",\n",
        "    registry_table=registry_table\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model trained!\")\n",
        "print(f\"   Model URI: {model_uri}\")\n",
        "\n",
        "# Show registry\n",
        "print(f\"\\nüìã Model Registry:\")\n",
        "display(\n",
        "    spark.table(registry_table)\n",
        "    .select(\n",
        "        \"identity.model_name\",\n",
        "        \"training.columns\",\n",
        "        \"training.training_rows\",\n",
        "        \"identity.status\"\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4: Find Anomalies\n",
        "\n",
        "Now let's score all trips and see what DQX finds!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply BOTH rule-based checks AND ML anomaly detection in one pass\n",
        "from databricks.labs.dqx.rule import DQRowRule\n",
        "from databricks.labs.dqx.check_funcs import is_in_range, is_not_less_than\n",
        "\n",
        "print(\"üîç Applying Rule-Based Checks + ML Anomaly Detection...\\n\")\n",
        "\n",
        "# Combine simple rules with ML anomaly detection\n",
        "# NOTE: Rules are intentionally STRICT (catch only obvious errors)\n",
        "# This lets ML demonstrate value by catching subtler issues\n",
        "all_checks = [\n",
        "    # Simple rule-based checks - only catch CLEARLY invalid data\n",
        "    DQRowRule(\n",
        "        name=\"impossible_speed\",\n",
        "        check_func=is_in_range,\n",
        "        check_func_kwargs={\"column\": \"speed_mph\", \"min_limit\": 0, \"max_limit\": 150}  # 150mph = clearly impossible\n",
        "    ),\n",
        "    DQRowRule(\n",
        "        name=\"negative_fare\",\n",
        "        check_func=is_not_less_than,\n",
        "        check_func_kwargs={\"column\": \"fare_amount\", \"limit\": 0}  # Only negative fares\n",
        "    ),\n",
        "    DQRowRule(\n",
        "        name=\"negative_distance\",\n",
        "        check_func=is_not_less_than,\n",
        "        check_func_kwargs={\"column\": \"trip_distance\", \"limit\": 0}  # Only negative distances\n",
        "    ),\n",
        "    \n",
        "    # ML anomaly detection (catches subtle issues rules miss)\n",
        "    DQDatasetRule(\n",
        "        check_func=has_no_anomalies,\n",
        "        check_func_kwargs={\n",
        "            \"merge_columns\": [\"trip_id\"],\n",
        "            \"model\": \"nyctaxi_dq\",\n",
        "            \"include_contributions\": True,\n",
        "            \"registry_table\": registry_table\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "# Apply all checks in ONE pass - this is the recommended pattern!\n",
        "df_scored = dq_engine.apply_checks(spark.table(table_name), all_checks)\n",
        "\n",
        "# Now df_scored has BOTH:\n",
        "# - _errors: contains BOTH rule violations AND anomaly failures\n",
        "# - _info.anomaly: ML anomaly scores (from has_no_anomalies)\n",
        "\n",
        "# Filter _errors to exclude anomaly check (we'll count that separately via _info.anomaly)\n",
        "row_rule_errors_col = F.filter(\n",
        "    F.col(\"_errors\"),\n",
        "    lambda x: x[\"function\"] != \"has_no_anomalies\"\n",
        ")\n",
        "\n",
        "total = df_scored.count()\n",
        "rule_violations = df_scored.filter(F.size(row_rule_errors_col) > 0).count()\n",
        "ml_anomalies = df_scored.filter(F.col(\"_info.anomaly.score\") >= 0.5).count()\n",
        "\n",
        "print(f\"‚úÖ Applied {len(all_checks)} checks in one pass!\")\n",
        "print(f\"\\nüìä Results:\")\n",
        "print(f\"   Total trips: {total:,}\")\n",
        "print(f\"   Rule violations: {rule_violations:,} ({rule_violations/total*100:.1f}%)\")\n",
        "print(f\"   ML anomalies (score ‚â• 0.5): {ml_anomalies:,} ({ml_anomalies/total*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show top anomalies\n",
        "print(\"üö® Top 15 Anomalous Trips:\\n\")\n",
        "\n",
        "top_anomalies = (\n",
        "    df_scored\n",
        "    .filter(F.col(\"_info.anomaly.score\") >= 0.5)\n",
        "    .orderBy(F.col(\"_info.anomaly.score\").desc())\n",
        "    .select(\n",
        "        \"trip_id\",\n",
        "        F.round(\"trip_distance\", 1).alias(\"distance_mi\"),\n",
        "        F.round(\"trip_duration_mins\", 1).alias(\"duration_mins\"),\n",
        "        F.round(\"speed_mph\", 1).alias(\"speed_mph\"),\n",
        "        F.round(\"fare_amount\", 2).alias(\"fare\"),\n",
        "        F.round(\"_info.anomaly.score\", 3).alias(\"anomaly_score\")\n",
        "    )\n",
        "    .limit(15)\n",
        ")\n",
        "\n",
        "display(top_anomalies)\n",
        "\n",
        "print(\"\\nüí° Look for patterns:\")\n",
        "print(\"   ‚Ä¢ Unusually high fares\")\n",
        "print(\"   ‚Ä¢ Impossible speeds (>100 mph in NYC traffic?)\")\n",
        "print(\"   ‚Ä¢ Very long durations with short distances\")\n",
        "print(\"   ‚Ä¢ Zero fares or distances\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 5: Understand WHY Trips Are Anomalous\n",
        "\n",
        "DQX provides **feature contributions** that explain which features made each trip unusual.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze top anomalies in detail\n",
        "print(\"üîé Deep Dive: Why Are These Trips Anomalous?\\n\")\n",
        "\n",
        "top_5 = (\n",
        "    df_scored\n",
        "    .filter(F.col(\"_info.anomaly.score\") >= 0.5)\n",
        "    .orderBy(F.col(\"_info.anomaly.score\").desc())\n",
        "    .select(\n",
        "        \"trip_id\",\n",
        "        \"trip_distance\",\n",
        "        \"trip_duration_mins\",\n",
        "        \"speed_mph\",\n",
        "        \"fare_amount\",\n",
        "        \"pickup_zip\",\n",
        "        F.col(\"_info.anomaly.score\").alias(\"score\"),\n",
        "        F.col(\"_info.anomaly.contributions\").alias(\"contributions\")\n",
        "    )\n",
        "    .limit(5)\n",
        "    .collect()\n",
        ")\n",
        "\n",
        "for i, row in enumerate(top_5, 1):\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Anomaly #{i} (Score: {row['score']:.3f})\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Trip Details:\")\n",
        "    print(f\"   Distance: {row['trip_distance']:.1f} miles\")\n",
        "    print(f\"   Duration: {row['trip_duration_mins']:.1f} mins\")\n",
        "    print(f\"   Speed: {row['speed_mph']:.1f} mph\")\n",
        "    print(f\"   Fare: ${row['fare_amount']:.2f}\")\n",
        "    print(f\"   Pickup ZIP: {row['pickup_zip']}\")\n",
        "    \n",
        "    print(f\"\\nTop Contributing Factors:\")\n",
        "    top_factors = []\n",
        "    if row['contributions']:\n",
        "        sorted_contrib = sorted(row['contributions'].items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "        for feature, value in sorted_contrib[:3]:\n",
        "            print(f\"   üìå {feature}: {abs(value)*100:.1f}% contribution\")\n",
        "            top_factors.append(feature)\n",
        "    \n",
        "    # Smarter interpretation based on contributing factors AND values\n",
        "    print(f\"\\nüéØ Likely Issue:\")\n",
        "    if row['speed_mph'] > 100:\n",
        "        print(f\"   ‚Üí Impossible speed ({row['speed_mph']:.0f} mph) - GPS or timestamp error\")\n",
        "    elif row['fare_amount'] <= 0:\n",
        "        print(f\"   ‚Üí Zero/negative fare - payment system error\")\n",
        "    elif row['trip_duration_mins'] > 180:\n",
        "        print(f\"   ‚Üí Very long trip ({row['trip_duration_mins']:.0f} mins) - meter issue?\")\n",
        "    elif row['trip_distance'] <= 0 and row['fare_amount'] > 0:\n",
        "        print(f\"   ‚Üí Zero distance but charged ${row['fare_amount']:.2f} - GPS error\")\n",
        "    else:\n",
        "        # Interpret based on top contributing factors\n",
        "        interpretations = []\n",
        "        if any('distance' in f for f in top_factors[:2]):\n",
        "            interpretations.append(f\"unusually long trip ({row['trip_distance']:.0f} mi)\")\n",
        "        if any('fare' in f for f in top_factors[:2]):\n",
        "            interpretations.append(f\"unusual fare (${row['fare_amount']:.0f})\")\n",
        "        if any('zip' in f for f in top_factors[:2]):\n",
        "            interpretations.append(f\"rare pickup location\")\n",
        "        if any('duration' in f for f in top_factors[:2]):\n",
        "            interpretations.append(f\"unusual duration ({row['trip_duration_mins']:.0f} mins)\")\n",
        "        if any('speed' in f for f in top_factors[:2]):\n",
        "            interpretations.append(f\"unusual speed ({row['speed_mph']:.0f} mph)\")\n",
        "        \n",
        "        if interpretations:\n",
        "            print(f\"   ‚Üí Multi-factor: {' + '.join(interpretations)}\")\n",
        "            print(f\"   ‚Üí This combination is rare in the training data\")\n",
        "        else:\n",
        "            print(f\"   ‚Üí Unusual combination of values\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the combined results - Rules vs ML\n",
        "print(\"üìä Rule-Based vs ML Detection Comparison\\n\")\n",
        "\n",
        "# Add derived columns for easier analysis\n",
        "# Filter _errors to exclude anomaly check (function=\"has_no_anomalies\")\n",
        "df_analysis = df_scored.withColumn(\n",
        "    \"row_rule_error_count\",\n",
        "    F.size(F.filter(F.col(\"_errors\"), lambda x: x[\"function\"] != \"has_no_anomalies\"))\n",
        ").withColumn(\n",
        "    \"is_ml_anomaly\",\n",
        "    F.col(\"_info.anomaly.score\") >= 0.5\n",
        ")\n",
        "\n",
        "# Count overlaps using the derived columns\n",
        "total_rows = df_analysis.count()\n",
        "rule_count = df_analysis.filter(F.col(\"row_rule_error_count\") > 0).count()\n",
        "ml_count = df_analysis.filter(F.col(\"is_ml_anomaly\")).count()\n",
        "\n",
        "both_flagged = df_analysis.filter((F.col(\"row_rule_error_count\") > 0) & F.col(\"is_ml_anomaly\")).count()\n",
        "rules_only = df_analysis.filter((F.col(\"row_rule_error_count\") > 0) & ~F.col(\"is_ml_anomaly\")).count()\n",
        "ml_only = df_analysis.filter((F.col(\"row_rule_error_count\") == 0) & F.col(\"is_ml_anomaly\")).count()\n",
        "\n",
        "print(\"üî¥ DQX Rule-Based Checks (from _errors):\")\n",
        "print(f\"   Rows with violations: {rule_count}\")\n",
        "\n",
        "print(f\"\\nüü° ML Anomaly Detection (from _info.anomaly):\")\n",
        "print(f\"   Anomalies detected: {ml_count}\")\n",
        "\n",
        "print(f\"\\nüìà Overlap Analysis:\")\n",
        "print(f\"   ‚Ä¢ Caught by BOTH rules AND ML: {both_flagged}\")\n",
        "print(f\"   ‚Ä¢ Caught by rules ONLY: {rules_only}\")\n",
        "print(f\"   ‚Ä¢ Caught by ML ONLY: {ml_only} ‚Üê This is the ML value-add!\")\n",
        "\n",
        "if ml_only > 0:\n",
        "    pct_ml_only = (ml_only / ml_count) * 100\n",
        "    print(f\"\\nüí° Key Insight:\")\n",
        "    print(f\"   ML found {ml_only} issues ({pct_ml_only:.0f}% of all anomalies) that simple rules missed!\")\n",
        "    print(f\"   These are subtle patterns like unusual trip+fare+location combinations.\")\n",
        "\n",
        "# Show a sample of ML-only anomalies\n",
        "print(f\"\\nüîé Sample ML-Only Anomalies (passed rules but flagged by ML):\")\n",
        "display(\n",
        "    df_analysis\n",
        "    .filter((F.col(\"row_rule_error_count\") == 0) & F.col(\"is_ml_anomaly\"))\n",
        "    .select(\n",
        "        \"trip_id\",\n",
        "        F.round(\"trip_distance\", 1).alias(\"distance\"),\n",
        "        F.round(\"trip_duration_mins\", 1).alias(\"duration\"),\n",
        "        F.round(\"fare_amount\", 2).alias(\"fare\"),\n",
        "        F.round(\"_info.anomaly.score\", 3).alias(\"anomaly_score\")\n",
        "    )\n",
        "    .orderBy(F.col(\"anomaly_score\").desc())\n",
        "    .limit(5)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6: Compare Normal vs Anomalous Trips\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical comparison\n",
        "print(\"üìà Normal vs Anomalous Trip Comparison:\\n\")\n",
        "\n",
        "normal = df_scored.filter(F.col(\"_info.anomaly.score\") < 0.5)\n",
        "anomalous = df_scored.filter(F.col(\"_info.anomaly.score\") >= 0.5)\n",
        "\n",
        "def get_stats(df, label):\n",
        "    return df.select(\n",
        "        F.lit(label).alias(\"type\"),\n",
        "        F.count(\"*\").alias(\"count\"),\n",
        "        F.round(F.avg(\"trip_distance\"), 2).alias(\"avg_distance\"),\n",
        "        F.round(F.avg(\"trip_duration_mins\"), 2).alias(\"avg_duration\"),\n",
        "        F.round(F.avg(\"speed_mph\"), 2).alias(\"avg_speed\"),\n",
        "        F.round(F.avg(\"fare_amount\"), 2).alias(\"avg_fare\"),\n",
        "    )\n",
        "\n",
        "comparison = get_stats(normal, \"Normal\").union(get_stats(anomalous, \"Anomalous\"))\n",
        "display(comparison)\n",
        "\n",
        "print(\"\\nüí° Anomalous trips have noticeably different patterns!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "### What We Found\n",
        "\n",
        "DQX successfully detected **real data quality issues** in NYC Taxi data:\n",
        "\n",
        "| Issue Type | Description | Likely Cause |\n",
        "|------------|-------------|-------------|\n",
        "| üöó Impossible speeds | >100 mph in NYC | GPS or timestamp error |\n",
        "| üí∞ Zero/negative fares | $0 or negative amounts | Payment system error |\n",
        "| üìç Zero distance trips | 0 miles but charged | GPS malfunction |\n",
        "| ‚è±Ô∏è Extreme durations | 3+ hour trips | Meter left running |\n",
        "| üíµ High fares | Unusually expensive trips | Long distance or data error |\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Unsupervised detection works** - No labels needed to find problems\n",
        "2. **Contributions explain WHY** - Not just \"anomaly\" but which features drove it\n",
        "3. **Catches subtle patterns** - Not just simple rule violations\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Quarantine** anomalous trips for investigation\n",
        "- **Combine** with rule-based checks for comprehensive DQ\n",
        "- **Monitor** ongoing data for new anomaly patterns\n",
        "- **Tune threshold** based on your tolerance (0.5 default, adjust up/down)\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [DQX Documentation](https://databrickslabs.github.io/dqx)\n",
        "- [Anomaly Detection Guide](https://databrickslabs.github.io/dqx/guide/anomaly_detection)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
