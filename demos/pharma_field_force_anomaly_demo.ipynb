{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfe5 Pharmaceutical Field Force Effectiveness - Anomaly Detection Demo\n",
    "\n",
    "## Business Context\n",
    "\n",
    "**Scenario**: Monitor sales rep performance across regions to detect unusual patterns:\n",
    "- \ud83d\udcb0 Expense fraud or policy violations\n",
    "- \ud83d\udcc9 Territory coverage issues and productivity gaps\n",
    "- \ud83d\udea8 Unrealistic prescription claims\n",
    "- \ud83d\udcda Training needs identification\n",
    "\n",
    "**Data**: Sales rep daily activity with calls, prescriptions, samples, expenses across US, EU, APAC\n",
    "\n",
    "## What You'll Learn (45 min comprehensive demo)\n",
    "\n",
    "1. **Auto-Discovery** - Zero-config vs manual tuning\n",
    "2. **Segment-Based Monitoring** - Regional baselines (US vs EU vs APAC)\n",
    "3. **Parameter Tuning** - Contamination, hyperparameters, model comparison\n",
    "4. **Feature Contributions** - SHAP-based root cause analysis\n",
    "5. **Drift Detection** - When to retrain models\n",
    "6. **Multi-Type Features** - Numeric, categorical, datetime, boolean\n",
    "7. **Production Integration** - DQEngine, YAML, quarantine workflows\n",
    "\n",
    "---\n",
    "\n",
    "**\ud83d\udccb Table of Contents:**\n",
    "- Section 1: Setup & Realistic Data (5 min)\n",
    "- Section 2: Auto-Discovery & Manual Tuning (12 min)\n",
    "- Section 3: Segment-Based Monitoring (8 min)\n",
    "- Section 4: Feature Contributions & Root Cause (8 min)\n",
    "- Section 5: Drift Detection & Retraining (6 min)\n",
    "- Section 6: Production Integration (6 min)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Setup & Data Generation (5 min)\n",
    "\n",
    "First, install DQX with anomaly support if not already installed:\n",
    "```bash\n",
    "%pip install databricks-labs-dqx[anomaly]\n",
    "```\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from databricks.labs.dqx.anomaly import AnomalyEngine, has_no_anomalies, AnomalyParams, IsolationForestConfig\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.labs.dqx.check_funcs import is_not_null, is_in_range\n",
    "from databricks.labs.dqx.config import OutputConfig\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Initialize\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "ws = WorkspaceClient()\n",
    "dq_engine = DQEngine(ws)\n",
    "anomaly_engine = AnomalyEngine(ws)\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\u2705 Setup complete!\")\n",
    "print(f\"   Spark version: {spark.version}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURABLE PARAMETERS ===\n",
    "# Adjust these to experiment with different thresholds\n",
    "\n",
    "# Anomaly score threshold (0-1 scale)\n",
    "# Lower = more sensitive (more anomalies detected, higher false positives)\n",
    "# Higher = more specific (fewer anomalies, lower false positives)\n",
    "ANOMALY_SCORE_THRESHOLD = 0.5\n",
    "\n",
    "# Drift detection threshold (z-score)\n",
    "# Typical range: 2.0 (sensitive) to 5.0 (conservative)\n",
    "DRIFT_THRESHOLD = 3.0\n",
    "\n",
    "print(f\"\ud83d\udccb Configuration:\")\n",
    "print(f\"   Anomaly Score Threshold: {ANOMALY_SCORE_THRESHOLD}\")\n",
    "print(f\"   Drift Detection Threshold: {DRIFT_THRESHOLD}\")\n",
    "print(f\"\\n\ud83d\udca1 You can change these values and re-run cells to see different results\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Realistic Sales Rep Activity Data\n",
    "\n",
    "We'll create 10,000 rows of daily sales rep activity with:\n",
    "- **Mixed data types**: Numeric, categorical, datetime, boolean\n",
    "- **Regional patterns**: Different baselines for US (high expenses), EU (moderate), APAC (high volume)\n",
    "- **Call type variations**: Promotional (higher cost), Educational (more samples), Follow-up (shorter)\n",
    "- **Temporal trends**: Seasonal variations (Q4 boost), weekday patterns\n",
    "- **Rep-specific behavior**: Consistent performers vs. inconsistent ones\n",
    "- **Realistic correlations**: More calls \u2192 more prescriptions, promotional \u2192 higher expenses\n",
    "- **Injected anomalies**: ~3% anomalous records (expense fraud, low productivity, data quality issues)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sales rep activity data with realistic patterns\n",
    "def generate_field_force_data(num_rows=10000, anomaly_rate=0.03):\n",
    "    \"\"\"Generate pharmaceutical field force activity data with realistic patterns.\"\"\"\n",
    "    data = []\n",
    "    regions = [\"US\", \"EU\", \"APAC\"]\n",
    "    call_types = [\"promotional\", \"educational\", \"follow_up\"]\n",
    "    num_reps = 100  # Increased from 50 to 100 reps\n",
    "    \n",
    "    # Regional baseline patterns (realistic differences)\n",
    "    regional_patterns = {\n",
    "        \"US\": {\"calls\": 8, \"prescriptions\": 12, \"samples\": 25, \"expenses\": 150, \"remote_rate\": 0.4},\n",
    "        \"EU\": {\"calls\": 6, \"prescriptions\": 9, \"samples\": 18, \"expenses\": 100, \"remote_rate\": 0.5},\n",
    "        \"APAC\": {\"calls\": 10, \"prescriptions\": 15, \"samples\": 30, \"expenses\": 120, \"remote_rate\": 0.3},\n",
    "    }\n",
    "    \n",
    "    # Call type modifiers (affect baseline metrics)\n",
    "    call_type_modifiers = {\n",
    "        \"promotional\": {\"calls\": 1.0, \"prescriptions\": 1.2, \"samples\": 1.4, \"expenses\": 1.3, \"remote\": 0.2},\n",
    "        \"educational\": {\"calls\": 0.9, \"prescriptions\": 0.8, \"samples\": 1.6, \"expenses\": 1.1, \"remote\": 0.6},\n",
    "        \"follow_up\": {\"calls\": 0.7, \"prescriptions\": 1.0, \"samples\": 0.6, \"expenses\": 0.8, \"remote\": 0.5},\n",
    "    }\n",
    "    \n",
    "    # Rep performance profiles (some reps are consistently better/worse)\n",
    "    rep_profiles = {}\n",
    "    for rep_id in range(num_reps):\n",
    "        # 70% average, 20% high performers, 10% low performers\n",
    "        perf_type = np.random.choice([\"average\", \"high\", \"low\"], p=[0.7, 0.2, 0.1])\n",
    "        if perf_type == \"high\":\n",
    "            multiplier = np.random.uniform(1.2, 1.5)\n",
    "        elif perf_type == \"low\":\n",
    "            multiplier = np.random.uniform(0.6, 0.8)\n",
    "        else:\n",
    "            multiplier = np.random.uniform(0.9, 1.1)\n",
    "        rep_profiles[f\"REP{rep_id:03d}\"] = multiplier\n",
    "    \n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    end_date = datetime(2024, 12, 31)\n",
    "    total_days = (end_date - start_date).days\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        rep_id = f\"REP{i % num_reps:03d}\"\n",
    "        region = random.choice(regions)\n",
    "        call_type = random.choice(call_types)\n",
    "        \n",
    "        # Get baseline patterns\n",
    "        pattern = regional_patterns[region]\n",
    "        call_modifier = call_type_modifiers[call_type]\n",
    "        rep_multiplier = rep_profiles[rep_id]\n",
    "        \n",
    "        # Generate date with temporal trends\n",
    "        days_offset = random.randint(0, total_days)\n",
    "        call_date = start_date + timedelta(days=days_offset)\n",
    "        \n",
    "        # Seasonal multiplier (Q4 boost for pharma year-end push)\n",
    "        month = call_date.month\n",
    "        if month in [10, 11, 12]:  # Q4\n",
    "            seasonal_multiplier = 1.15\n",
    "        elif month in [1, 2]:  # Post-holiday slump\n",
    "            seasonal_multiplier = 0.9\n",
    "        else:\n",
    "            seasonal_multiplier = 1.0\n",
    "        \n",
    "        # Weekday effect (lower activity on Fridays, higher Mon-Thu)\n",
    "        weekday = call_date.weekday()\n",
    "        if weekday == 4:  # Friday\n",
    "            weekday_multiplier = 0.85\n",
    "        elif weekday in [0, 1]:  # Monday, Tuesday\n",
    "            weekday_multiplier = 1.05\n",
    "        else:\n",
    "            weekday_multiplier = 1.0\n",
    "        \n",
    "        # Track ground truth for validation\n",
    "        anomaly_type_label = None\n",
    "        \n",
    "        # Normal patterns (97% of data)\n",
    "        if random.random() > anomaly_rate:\n",
    "            anomaly_type_label = \"normal\"\n",
    "            \n",
    "            # Base metrics with all multipliers\n",
    "            combined_multiplier = rep_multiplier * seasonal_multiplier * weekday_multiplier\n",
    "            \n",
    "            # Calls (with correlation to rep performance)\n",
    "            calls_base = pattern[\"calls\"] * call_modifier[\"calls\"] * combined_multiplier\n",
    "            calls = max(1, int(np.random.normal(calls_base, calls_base * 0.2)))\n",
    "            \n",
    "            # Prescriptions (correlated with calls - more calls \u2192 more prescriptions)\n",
    "            prescriptions_base = pattern[\"prescriptions\"] * call_modifier[\"prescriptions\"] * combined_multiplier\n",
    "            # Add correlation: prescription rate increases slightly with more calls\n",
    "            call_correlation = min(1.2, calls / calls_base)\n",
    "            prescriptions = max(0, int(np.random.normal(prescriptions_base * call_correlation, prescriptions_base * 0.25)))\n",
    "            \n",
    "            # Samples (correlated with call type)\n",
    "            samples_base = pattern[\"samples\"] * call_modifier[\"samples\"] * combined_multiplier\n",
    "            samples = max(0, int(np.random.normal(samples_base, samples_base * 0.2)))\n",
    "            \n",
    "            # Expenses (correlated with calls and call type)\n",
    "            expenses_base = pattern[\"expenses\"] * call_modifier[\"expenses\"] * combined_multiplier\n",
    "            # Add correlation: more calls = slightly higher expenses\n",
    "            expense_correlation = min(1.15, calls / calls_base)\n",
    "            expenses = max(10, round(np.random.normal(expenses_base * expense_correlation, expenses_base * 0.15), 2))\n",
    "            \n",
    "            # Remote flag (depends on call type and region)\n",
    "            is_remote = random.random() < (pattern[\"remote_rate\"] * call_modifier[\"remote\"])\n",
    "            \n",
    "        else:\n",
    "            # Inject realistic anomalies (3% of data)\n",
    "            anomaly_type = random.choice([\n",
    "                \"high_expense_fraud\",\n",
    "                \"low_productivity\",\n",
    "                \"unrealistic_prescriptions\",\n",
    "                \"data_quality_issue\"\n",
    "            ])\n",
    "            anomaly_type_label = anomaly_type\n",
    "            \n",
    "            if anomaly_type == \"high_expense_fraud\":\n",
    "                # Excessive expenses with low output (potential fraud)\n",
    "                calls = max(1, int(pattern[\"calls\"] * 0.4))\n",
    "                prescriptions = max(0, int(pattern[\"prescriptions\"] * 0.3))\n",
    "                samples = max(0, int(pattern[\"samples\"] * 0.5))\n",
    "                expenses = round(pattern[\"expenses\"] * random.uniform(3.0, 5.0), 2)\n",
    "                is_remote = False  # Fraudsters often claim in-person visits\n",
    "                \n",
    "            elif anomaly_type == \"low_productivity\":\n",
    "                # Many calls but few results (training need or territory issue)\n",
    "                calls = int(pattern[\"calls\"] * random.uniform(2.0, 3.0))\n",
    "                prescriptions = max(0, int(pattern[\"prescriptions\"] * random.uniform(0.15, 0.3)))\n",
    "                samples = int(pattern[\"samples\"] * random.uniform(0.4, 0.6))\n",
    "                expenses = round(pattern[\"expenses\"] * random.uniform(1.3, 1.6), 2)\n",
    "                is_remote = random.random() < 0.4\n",
    "                \n",
    "            elif anomaly_type == \"unrealistic_prescriptions\":\n",
    "                # Suspiciously high prescription rate (investigation needed)\n",
    "                calls = max(1, int(pattern[\"calls\"] * random.uniform(0.8, 1.2)))\n",
    "                prescriptions = int(pattern[\"prescriptions\"] * random.uniform(3.0, 5.0))\n",
    "                samples = int(pattern[\"samples\"] * random.uniform(2.0, 3.0))\n",
    "                expenses = round(pattern[\"expenses\"] * random.uniform(0.9, 1.3), 2)\n",
    "                is_remote = False\n",
    "                \n",
    "            else:  # data_quality_issue\n",
    "                # Outliers that don't follow normal patterns (data entry errors)\n",
    "                calls = random.choice([0, int(pattern[\"calls\"] * 10)])  # Either 0 or way too high\n",
    "                prescriptions = random.choice([int(pattern[\"prescriptions\"] * -1) if random.random() < 0.3 else 0, \n",
    "                                              int(pattern[\"prescriptions\"] * 8)])\n",
    "                samples = random.choice([0, int(pattern[\"samples\"] * 12)])\n",
    "                expenses = round(random.choice([1, pattern[\"expenses\"] * 20]), 2)\n",
    "                is_remote = random.random() < 0.5\n",
    "        \n",
    "        data.append((\n",
    "            f\"ACT{i:06d}\",  # Unique activity_id (primary key)\n",
    "            rep_id,\n",
    "            region,\n",
    "            call_date,\n",
    "            calls,\n",
    "            prescriptions,\n",
    "            samples,\n",
    "            expenses,\n",
    "            is_remote,\n",
    "            call_type,\n",
    "            anomaly_type_label  # Ground truth for validation\n",
    "        ))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate data\n",
    "print(\"\ud83d\udd04 Generating sales rep activity data with realistic patterns...\")\n",
    "field_force_data = generate_field_force_data(num_rows=10000, anomaly_rate=0.03)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"activity_id\", StringType(), False),  # Primary key\n",
    "    StructField(\"rep_id\", StringType(), False),\n",
    "    StructField(\"region\", StringType(), False),\n",
    "    StructField(\"call_date\", DateType(), False),\n",
    "    StructField(\"calls_made\", IntegerType(), False),\n",
    "    StructField(\"prescriptions_generated\", IntegerType(), False),\n",
    "    StructField(\"samples_distributed\", IntegerType(), False),\n",
    "    StructField(\"expenses\", DoubleType(), False),\n",
    "    StructField(\"is_remote\", BooleanType(), False),\n",
    "    StructField(\"call_type\", StringType(), False),\n",
    "    StructField(\"true_anomaly_type\", StringType(), False),  # Ground truth\n",
    "])\n",
    "\n",
    "df_sales = spark.createDataFrame(field_force_data, schema)\n",
    "\n",
    "print(\"\\n\ud83d\udcca Sample of field force activity data:\")\n",
    "display(df_sales.orderBy(\"call_date\").limit(10))\n",
    "\n",
    "print(f\"\\n\u2705 Generated {df_sales.count()} rows with ~3% injected anomalies\")\n",
    "print(f\"   Regions: {df_sales.select('region').distinct().count()}\")\n",
    "print(f\"   Call types: {df_sales.select('call_type').distinct().count()}\")\n",
    "print(f\"   Unique reps: {df_sales.select('rep_id').distinct().count()}\")\n",
    "print(f\"   Date range: {df_sales.agg(F.min('call_date'), F.max('call_date')).first()}\")\n",
    "print(f\"   Segments (region \u00d7 call_type): {df_sales.select('region', 'call_type').distinct().count()}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to table for training\n",
    "catalog = spark.sql(\"SELECT current_catalog()\").first()[0]\n",
    "schema_name = \"dqx_demo\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema_name}\")\n",
    "\n",
    "table_name = f\"{catalog}.{schema_name}.field_force_activity\"\n",
    "df_sales.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"\u2705 Data saved to: {table_name}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (80%) and test (20%) sets\n",
    "# Training: historical \"normal\" data to learn patterns\n",
    "# Test: new data to detect anomalies (simulates production)\n",
    "\n",
    "df_train, df_test = df_sales.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"\ud83d\udcca Data Split:\")\n",
    "print(f\"   Training set: {df_train.count()} rows\")\n",
    "print(f\"   Test set: {df_test.count()} rows\")\n",
    "print(f\"\\\\n\ud83d\udca1 We train on historical data and score on new data (like production)\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Auto-Discovery vs Manual Tuning (12 min)\n",
    "\n",
    "### 2.1 Auto-Discovery (Zero Configuration)\n",
    "\n",
    "Let's start with zero configuration - DQX will automatically select columns and detect segments.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let DQX automatically discover the best columns and segments\n",
    "# Use exclude_columns to skip ID and ground truth columns\n",
    "# This enables auto-discovery on remaining columns\n",
    "model_name_auto = anomaly_engine.train(\n",
    "    df=df_train,\n",
    "    model_name=\"field_force_auto\",\n",
    "    exclude_columns=['activity_id', 'true_anomaly_type'],  # Exclude ID and ground truth\n",
    "    registry_table=f\"{catalog}.{schema_name}.anomaly_model_registry\"\n",
    ")\n",
    "print(f\"\\\\n\ud83d\udcca Auto-discovery complete!\")\n",
    "print(f\"   Model: {model_name_auto}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what was auto-discovered\n",
    "registry_df = spark.table(f\"{catalog}.{schema_name}.anomaly_model_registry\")\n",
    "\n",
    "# For segmented models, we query by the base model name\n",
    "# The registry contains individual segment entries\n",
    "base_name_parts = model_name_auto.split(\".\")\n",
    "if len(base_name_parts) == 3:\n",
    "    base_model_name_only = base_name_parts[2]  # Get just the model name without catalog.schema\n",
    "else:\n",
    "    base_model_name_only = model_name_auto\n",
    "\n",
    "# Get a representative segment to show configuration\n",
    "sample_model = registry_df.filter(\n",
    "    F.col(\"model_name\").like(f\"%{base_model_name_only}%\")\n",
    ").orderBy(\"training_time\").first()\n",
    "\n",
    "print(f\"\\\\n\ud83d\udccb Auto-Discovered Configuration:\")\n",
    "if sample_model:\n",
    "    print(f\"   Model: {model_name_auto}\")\n",
    "    print(f\"   Columns: {sample_model['columns']}\")\n",
    "    print(f\"   Segments: {sample_model['segment_by']}\")\n",
    "    print(f\"   Column types: {sample_model['column_types']}\")\n",
    "    \n",
    "    # Count total segments if segmented model\n",
    "    if sample_model['segment_by']:\n",
    "        segment_count = registry_df.filter(\n",
    "            F.col(\"model_name\").like(f\"%{base_model_name_only}%\")\n",
    "        ).count()\n",
    "        print(f\"   Total segments trained: {segment_count}\")\n",
    "else:\n",
    "    print(\"   \u26a0\ufe0f Model not found in registry\")\n",
    "\n",
    "print(f\"\\\\n\ud83d\udca1 DQX prioritized: numeric > boolean > categorical > datetime\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.labs.dqx.rule import DQDatasetRule\n",
    "\n",
    "# Score with auto-discovered model (just pass the model name!)\n",
    "checks_auto = [\n",
    "    DQDatasetRule(\n",
    "        criticality=\"error\",\n",
    "        check_func=has_no_anomalies,\n",
    "        check_func_kwargs={\n",
    "            \"model\": model_name_auto,  # Just pass the model name - segments are handled automatically!\n",
    "            \"score_threshold\": ANOMALY_SCORE_THRESHOLD,  # Use configurable threshold\n",
    "            \"registry_table\": f\"{catalog}.{schema_name}.anomaly_model_registry\",\n",
    "            \"merge_columns\": [\"activity_id\"]\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "df_scored_auto = dq_engine.apply_checks(df_test, checks_auto)\n",
    "anomalies_auto = df_scored_auto.filter(F.col(\"anomaly_score\") >= ANOMALY_SCORE_THRESHOLD)\n",
    "\n",
    "print(f\"\\\\n\u26a0\ufe0f  Auto-discovery found {anomalies_auto.count()} anomalies (threshold: {ANOMALY_SCORE_THRESHOLD}):\\\\n\")\n",
    "display(anomalies_auto.orderBy(F.col(\"anomaly_score\").desc()).select(\n",
    "    \"rep_id\", \"region\", \"calls_made\", \"prescriptions_generated\", \"expenses\",\n",
    "    F.round(\"anomaly_score\", 3).alias(\"score\")\n",
    ").limit(10))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Validate Auto-Discovery Performance\n",
    "\n",
    "Let's validate how well auto-discovery worked by checking detection rate and false positives.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Explore the `_info` Column\n",
    "\n",
    "DQX provides a structured `_info` column that contains all anomaly metadata in one place.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the _info column structure\n",
    "print(\"\ud83d\udcca The _info Column Structure:\\n\")\n",
    "print(\"_info: struct {\")\n",
    "print(\"  anomaly: struct {\")\n",
    "print(\"    check_name: string        # Check function name\")\n",
    "print(\"    score: double              # Anomaly score (0-1)\")\n",
    "print(\"    is_anomaly: boolean        # True if score > threshold\")\n",
    "print(\"    threshold: double          # Detection threshold used\")\n",
    "print(\"    model: string              # Model name\")\n",
    "print(\"    segment: map<string,string> # Segment values (null for global)\")\n",
    "print(\"    contributions: map<string,double> # SHAP values (if requested)\")\n",
    "print(\"    confidence_std: double     # Ensemble std (if ensemble)\")\n",
    "print(\"  }\")\n",
    "print(\"}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\ud83d\udccb Sample _info values for detected anomalies:\\n\")\n",
    "\n",
    "# Show top 3 anomalies with their _info (using _info to filter)\n",
    "sample_anomalies = df_scored_auto.filter(\n",
    "    F.col(\"_info.anomaly\".\"is_anomaly\"]  # \u2705 Recommended way\n",
    ").orderBy(F.col(\"_info.anomaly\".\"score\"].desc()).limit(3)\n",
    "\n",
    "for row in sample_anomalies.collect():\n",
    "    print(f\"Activity {row['activity_id']}:\")\n",
    "    print(f\"  Region: {row['region']}, Calls: {row['calls_made']}, Expenses: ${row['expenses']:.2f}\")\n",
    "    \n",
    "    # Extract _info.anomaly array\n",
    "    anomaly_info = row['_info']['anomaly']  # Get first element\n",
    "    print(f\"  _info.anomaly:\")\n",
    "    print(f\"    check_name: {anomaly_info['check_name']}\")\n",
    "    print(f\"    score: {anomaly_info['score']:.3f}\")\n",
    "    print(f\"    is_anomaly: {anomaly_info['is_anomaly']}\")\n",
    "    print(f\"    threshold: {anomaly_info['threshold']}\")\n",
    "    print(f\"    model: {anomaly_info['model']}\")\n",
    "    print(f\"    segment: {anomaly_info['segment']}\")\n",
    "    print()\n",
    "\n",
    "print(\"\ud83d\udca1 Benefits of _info column:\")\n",
    "print(\"   \u2705 All metadata in one place\")\n",
    "print(\"   \u2705 Self-documenting schema\")\n",
    "print(\"   \u2705 Easy to query: df.filter(col('_info.anomaly.is_anomaly'))\")\n",
    "print(\"   \u2705 Extensible: Future checks (drift, profiling) can add their own keys\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPREHENSIVE VALIDATION WITH GROUND TRUTH ===\n",
    "\n",
    "print(f\"\ud83d\udd0d Validation using threshold: {ANOMALY_SCORE_THRESHOLD}\\n\")\n",
    "\n",
    "# Classify predictions (deduplication now handled in library)\n",
    "df_classified = df_scored_auto.withColumn(\n",
    "    \"predicted_anomaly\",\n",
    "    F.when(F.col(\"anomaly_score\") >= ANOMALY_SCORE_THRESHOLD, True).otherwise(False)\n",
    ").withColumn(\n",
    "    \"is_true_anomaly\",\n",
    "    F.when(F.col(\"true_anomaly_type\") != \"normal\", True).otherwise(False)\n",
    ")\n",
    "\n",
    "# === 1. Overall Confusion Matrix ===\n",
    "print(\"\ud83d\udcca Confusion Matrix:\")\n",
    "confusion = df_classified.groupBy(\"is_true_anomaly\", \"predicted_anomaly\").count()\n",
    "display(confusion.orderBy(\"is_true_anomaly\", \"predicted_anomaly\"))\n",
    "\n",
    "# Calculate metrics\n",
    "tp = df_classified.filter((F.col(\"is_true_anomaly\") == True) & (F.col(\"predicted_anomaly\") == True)).count()\n",
    "fp = df_classified.filter((F.col(\"is_true_anomaly\") == False) & (F.col(\"predicted_anomaly\") == True)).count()\n",
    "tn = df_classified.filter((F.col(\"is_true_anomaly\") == False) & (F.col(\"predicted_anomaly\") == False)).count()\n",
    "fn = df_classified.filter((F.col(\"is_true_anomaly\") == True) & (F.col(\"predicted_anomaly\") == False)).count()\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\n\ud83d\udcc8 Performance Metrics:\")\n",
    "metrics_df = spark.createDataFrame([\n",
    "    (\"Precision\", round(precision * 100, 2)),\n",
    "    (\"Recall\", round(recall * 100, 2)),\n",
    "    (\"F1-Score\", round(f1_score * 100, 2))\n",
    "], [\"Metric\", \"Value_%\"])\n",
    "display(metrics_df)\n",
    "\n",
    "# === 2. Detection Rate by Anomaly Type ===\n",
    "print(\"\\n\ud83c\udfaf Detection Performance by Anomaly Type:\")\n",
    "\n",
    "anomaly_type_stats = df_classified.filter(\n",
    "    F.col(\"true_anomaly_type\") != \"normal\"\n",
    ").groupBy(\"true_anomaly_type\").agg(\n",
    "    F.count(\"*\").alias(\"total_count\"),\n",
    "    F.sum(F.when(F.col(\"predicted_anomaly\"), 1).otherwise(0)).alias(\"detected_count\"),\n",
    "    F.avg(\"anomaly_score\").alias(\"avg_score\"),\n",
    "    F.max(\"anomaly_score\").alias(\"max_score\"),\n",
    "    F.min(\"anomaly_score\").alias(\"min_score\")\n",
    ").withColumn(\n",
    "    \"detection_rate\",\n",
    "    (F.col(\"detected_count\") / F.col(\"total_count\") * 100).cast(\"decimal(5,2)\")\n",
    ").orderBy(F.desc(\"detection_rate\"))\n",
    "\n",
    "display(anomaly_type_stats)\n",
    "\n",
    "# === 3. Score Distribution by Type (Visualization) ===\n",
    "print(\"\\n\ud83d\udcca Score Distribution by Anomaly Type:\")\n",
    "score_by_type = df_classified.select(\n",
    "    \"true_anomaly_type\",\n",
    "    F.round(\"anomaly_score\", 2).alias(\"anomaly_score\")\n",
    ")\n",
    "display(score_by_type)  # Databricks will auto-create histogram\n",
    "\n",
    "# === 4. False Positives Analysis ===\n",
    "false_positives = df_classified.filter(\n",
    "    (F.col(\"true_anomaly_type\") == \"normal\") & \n",
    "    (F.col(\"predicted_anomaly\") == True)\n",
    ")\n",
    "\n",
    "print(f\"\\n\u26a0\ufe0f False Positives: {false_positives.count()} records\")\n",
    "print(f\"Top false positive scores:\")\n",
    "display(\n",
    "    false_positives.select(\n",
    "        \"activity_id\", \"rep_id\", \"region\", \n",
    "        \"calls_made\", \"expenses\",\n",
    "        F.round(\"anomaly_score\", 3).alias(\"score\")\n",
    "    ).orderBy(F.desc(\"anomaly_score\")).limit(5)\n",
    ")\n",
    "\n",
    "# === 5. Threshold Sensitivity Analysis ===\n",
    "print(f\"\\n\ud83d\udd2c Impact of Different Thresholds:\")\n",
    "threshold_analysis = []\n",
    "for thresh in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    detected = df_classified.filter(\n",
    "        (F.col(\"is_true_anomaly\") == True) & \n",
    "        (F.col(\"anomaly_score\") >= thresh)\n",
    "    ).count()\n",
    "    false_pos = df_classified.filter(\n",
    "        (F.col(\"is_true_anomaly\") == False) & \n",
    "        (F.col(\"anomaly_score\") >= thresh)\n",
    "    ).count()\n",
    "    total_anomalies = df_classified.filter(F.col(\"is_true_anomaly\") == True).count()\n",
    "    \n",
    "    threshold_analysis.append((\n",
    "        thresh,\n",
    "        detected,\n",
    "        round(detected/total_anomalies*100, 1) if total_anomalies > 0 else 0,\n",
    "        false_pos\n",
    "    ))\n",
    "\n",
    "threshold_df = spark.createDataFrame(\n",
    "    threshold_analysis,\n",
    "    [\"Threshold\", \"Detected\", \"Detection_Rate_%\", \"False_Positives\"]\n",
    ")\n",
    "display(threshold_df)\n",
    "\n",
    "print(f\"\\n\ud83d\udca1 Current threshold ({ANOMALY_SCORE_THRESHOLD}) is highlighted above.\")\n",
    "print(f\"   Lower threshold = more sensitive (catches more, but more false alarms)\")\n",
    "print(f\"   Higher threshold = more specific (misses some, but fewer false alarms)\")\n",
    "\n",
    "# Store metrics for comparison later\n",
    "expected_anomalies = df_classified.filter(F.col(\"is_true_anomaly\") == True).count()\n",
    "normal_records_expected = df_classified.filter(F.col(\"is_true_anomaly\") == False).count()\n",
    "detected_anomalies = anomalies_auto.count()\n",
    "detection_rate = (recall * 100)\n",
    "fp_rate = (fp / normal_records_expected * 100) if normal_records_expected > 0 else 0\n",
    "\n",
    "auto_metrics = {\n",
    "    \"detected\": detected_anomalies,\n",
    "    \"detection_rate\": detection_rate,\n",
    "    \"fp_rate\": fp_rate\n",
    "}\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SHAP FEATURE CONTRIBUTIONS BY ANOMALY TYPE ===\n",
    "print(\"\ud83d\udd2c Computing SHAP contributions per anomaly type...\")\n",
    "print(\"   (This may take a moment...)\\n\")\n",
    "\n",
    "# Re-score with SHAP enabled\n",
    "checks_with_shap = [\n",
    "    DQDatasetRule(\n",
    "        criticality=\"error\",\n",
    "        check_func=has_no_anomalies,\n",
    "        check_func_kwargs={\n",
    "            \"model\": model_name_auto,\n",
    "            \"score_threshold\": ANOMALY_SCORE_THRESHOLD,\n",
    "            \"registry_table\": f\"{catalog}.{schema_name}.anomaly_model_registry\",\n",
    "            \"merge_columns\": [\"activity_id\"],\n",
    "            \"include_contributions\": True  # <- Enable SHAP\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "df_with_shap = dq_engine.apply_checks(df_test, checks_with_shap)\n",
    "\n",
    "# Use the scored DataFrame directly (deduplication now handled in library)\n",
    "df_shap_truth = df_with_shap\n",
    "\n",
    "# === Analyze Top Contributors Per Anomaly Type ===\n",
    "for anomaly_type in [\"high_expense_fraud\", \"low_productivity\", \"unrealistic_prescriptions\", \"data_quality_issue\"]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"\ud83d\udcca SHAP Analysis: {anomaly_type.upper().replace('_', ' ')}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get samples of this type that were detected\n",
    "    samples = df_shap_truth.filter(\n",
    "        (F.col(\"true_anomaly_type\") == anomaly_type) &\n",
    "        (F.col(\"anomaly_score\") >= ANOMALY_SCORE_THRESHOLD)\n",
    "    ).select(\n",
    "        \"activity_id\", \n",
    "        \"anomaly_score\",\n",
    "        \"anomaly_contributions\",\n",
    "        \"calls_made\", \"prescriptions_generated\", \"expenses\"\n",
    "    ).limit(3)\n",
    "    \n",
    "    # Check if we have samples\n",
    "    sample_count = samples.count()\n",
    "    if sample_count == 0:\n",
    "        print(f\"   \u26a0\ufe0f No detected anomalies of this type (try lowering threshold)\")\n",
    "        continue\n",
    "    \n",
    "    # Display the anomalies\n",
    "    print(f\"\\n\u2705 Sample detected anomalies ({sample_count}):\")\n",
    "    display(samples.select(\"activity_id\", \"calls_made\", \"prescriptions_generated\", \"expenses\", \n",
    "                           F.round(\"anomaly_score\", 3).alias(\"score\")))\n",
    "    \n",
    "    # Extract and display top SHAP contributors\n",
    "    # Note: anomaly_contributions is a Map<String, Double>\n",
    "    for row in samples.collect():\n",
    "        contributions = row[\"anomaly_contributions\"]\n",
    "        if contributions:\n",
    "            print(f\"\\n  \ud83d\udd0d Activity {row['activity_id']} - Top contributing features:\")\n",
    "            sorted_contrib = sorted(contributions.items(), key=lambda x: abs(x[1]), reverse=True)[:5]\n",
    "            for feature, value in sorted_contrib:\n",
    "                print(f\"      {feature:30s}: {value*100:6.1f}%\")\n",
    "\n",
    "print(f\"\\n\\n\ud83d\udca1 Key Insights:\")\n",
    "print(f\"   \u2022 high_expense_fraud: Usually driven by 'expenses' features\")\n",
    "print(f\"   \u2022 low_productivity: Driven by calls vs prescriptions ratio\")\n",
    "print(f\"   \u2022 unrealistic_prescriptions: Driven by prescription-related features\")\n",
    "print(f\"   \u2022 data_quality_issue: Mix of features with extreme values\")\n",
    "print(f\"\\n\u2705 SHAP helps explain WHY each anomaly was detected!\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Manual Column Selection & Parameter Tuning\n",
    "\n",
    "Now let's manually select specific columns and tune hyperparameters for better performance.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually specify columns (no segmentation)\n",
    "# Note: We explicitly select features, excluding activity_id and true_anomaly_type\n",
    "model_name_manual = anomaly_engine.train(\n",
    "    df=df_train,\n",
    "    columns=[\"calls_made\", \"prescriptions_generated\", \"expenses\", \"samples_distributed\"],\n",
    "    model_name=\"field_force_manual\",\n",
    "    registry_table=f\"{catalog}.{schema_name}.anomaly_model_registry\"\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score with manually configured model\n",
    "checks_manual = [\n",
    "    DQDatasetRule(\n",
    "        criticality=\"error\",\n",
    "        check_func=has_no_anomalies,\n",
    "        check_func_kwargs={\n",
    "            \"model\": model_name_manual,\n",
    "            \"score_threshold\": ANOMALY_SCORE_THRESHOLD,  # Use configurable threshold\n",
    "            \"registry_table\": f\"{catalog}.{schema_name}.anomaly_model_registry\",\n",
    "            \"merge_columns\": [\"activity_id\"]\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "df_scored_manual = dq_engine.apply_checks(df_test, checks_manual)\n",
    "anomalies_manual = df_scored_manual.filter(F.col(\"anomaly_score\") >= ANOMALY_SCORE_THRESHOLD)\n",
    "\n",
    "print(f\"\\\\n\u26a0\ufe0f  Manual config found {anomalies_manual.count()} anomalies (threshold: {ANOMALY_SCORE_THRESHOLD}):\\\\n\")\n",
    "display(anomalies_manual.orderBy(F.col(\"anomaly_score\").desc()).select(\n",
    "    \"rep_id\", \"region\", \"calls_made\", \"prescriptions_generated\", \"expenses\",\n",
    "    F.round(\"anomaly_score\", 3).alias(\"score\")\n",
    ").limit(10))\n",
    "\n",
    "\n",
    "# Store metrics for comparison\n",
    "manual_detected = anomalies_manual.count()\n",
    "manual_detection_rate = (manual_detected / expected_anomalies) * 100 if expected_anomalies > 0 else 0\n",
    "manual_flagged = df_scored_manual.filter(F.col(\"anomaly_score\") >= ANOMALY_SCORE_THRESHOLD).count()\n",
    "manual_fp = max(0, manual_flagged - expected_anomalies)\n",
    "manual_fp_rate = (manual_fp / normal_records_expected) * 100 if normal_records_expected > 0 else 0\n",
    "\n",
    "manual_metrics = {\n",
    "    \"detected\": manual_detected,\n",
    "    \"detection_rate\": manual_detection_rate,\n",
    "    \"fp_rate\": manual_fp_rate\n",
    "}\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Manual Tuning Performance: {manual_detected} detected ({manual_detection_rate:.1f}% rate), {manual_fp_rate:.2f}% FP\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model Comparison\n",
    "\n",
    "Let's compare the auto-discovered vs manually tuned models:\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "print(\"\ud83d\udcca Model Comparison:\\\\n\")\n",
    "comparison = registry_df.filter(\n",
    "    F.col(\"model_uri\").isin([model_name_auto, model_name_manual])\n",
    ").select(\n",
    "    \"model_name\",\n",
    "    \"columns\",\n",
    "    \"training_rows\",\n",
    "    \"metrics\"\n",
    ").collect()\n",
    "\n",
    "for model in comparison:\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Model: {model['model_name']}\")\n",
    "    print(f\"Columns: {model['columns']}\")\n",
    "    print(f\"Training rows: {model['training_rows']}\")\n",
    "    print(f\"Metrics: {model['metrics']}\")\n",
    "    print()\n",
    "\n",
    "print(\"\ud83d\udca1 Tuning Tips:\")\n",
    "print(\"   - contamination: Set to expected anomaly rate (0.01-0.1)\")\n",
    "print(\"   - num_trees: More trees = more stable (100-200)\")\n",
    "print(\"   - max_samples: Smaller = faster, larger = more accurate (256-1024)\")\n",
    "print(\"   - Start with auto-discovery, then refine based on domain knowledge\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Segment-Based Monitoring (8 min)\n",
    "\n",
    "Different regions have different patterns. Train per-region models for accurate baselines.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with regional segmentation\n",
    "print(\"\ud83c\udf0d Training region-specific anomaly models...\\\\n\")\n",
    "\n",
    "# Note: We explicitly select features, excluding activity_id and true_anomaly_type\n",
    "model_name_segmented = anomaly_engine.train(\n",
    "    df=df_train,\n",
    "    columns=[\"calls_made\", \"prescriptions_generated\", \"samples_distributed\", \"expenses\"],\n",
    "    segment_by=[\"region\"],  # Train separate model per region\n",
    "    model_name=\"field_force_regional\",\n",
    "    params=AnomalyParams(\n",
    "        algorithm_config=IsolationForestConfig(contamination=0.05, num_trees=150, random_seed=42)\n",
    "    ),\n",
    "    registry_table=f\"{catalog}.{schema_name}.anomaly_model_registry\"\n",
    ")\n",
    "\n",
    "print(f\"\\\\n\u2705 Regional models trained!\")\n",
    "print(\"   DQX automatically trained 3 models (US, EU, APAC)\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regional baselines\n",
    "regional_models = spark.table(f\"{catalog}.{schema_name}.anomaly_model_registry\").filter(\n",
    "    F.col(\"model_name\") == \"field_force_regional\"\n",
    ")\n",
    "\n",
    "print(\"\ud83d\udcca Regional Model Baselines:\\\\n\")\n",
    "for row in regional_models.select(\"segment_values\", \"training_rows\", \"baseline_stats\").collect():\n",
    "    region = row['segment_values']['region']\n",
    "    print(f\"Region: {region}\")\n",
    "    print(f\"  Training rows: {row['training_rows']}\")\n",
    "    print(f\"  Baseline stats: {row['baseline_stats']}\")\n",
    "    print()\n",
    "\n",
    "print(\"\ud83d\udd0d Notice: Each region has different baselines!\")\n",
    "print(\"   US: Higher expenses ($150 avg)\")\n",
    "print(\"   EU: Lower expenses ($100 avg)\")\n",
    "print(\"   APAC: Highest volume (10 calls, 15 prescriptions avg)\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.labs.dqx.rule import DQDatasetRule\n",
    "\n",
    "# Score with regional models (automatic routing)\n",
    "checks_regional = [\n",
    "    DQDatasetRule(\n",
    "        criticality=\"error\",\n",
    "        check_func=has_no_anomalies,\n",
    "        check_func_kwargs={\n",
    "            \"model\": \"field_force_regional\",\n",
    "            \"score_threshold\": ANOMALY_SCORE_THRESHOLD,  # Use configurable threshold\n",
    "            \"registry_table\": f\"{catalog}.{schema_name}.anomaly_model_registry\",\n",
    "            \"merge_columns\": [\"activity_id\"]\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "df_scored_regional = dq_engine.apply_checks(df_test, checks_regional)\n",
    "\n",
    "print(f\"\u26a0\ufe0f  Regional anomalies by region (threshold: {ANOMALY_SCORE_THRESHOLD}):\\\\n\")\n",
    "display(df_scored_regional.filter(F.col(\"anomaly_score\") >= ANOMALY_SCORE_THRESHOLD).groupBy(\"region\").agg(\n",
    "    F.count(\"*\").alias(\"anomaly_count\"),\n",
    "    F.avg(\"anomaly_score\").alias(\"avg_score\"),\n",
    "    F.max(\"anomaly_score\").alias(\"max_score\")\n",
    ").orderBy(\"region\"))\n",
    "\n",
    "print(\"\\\\n\ud83d\udccb Top regional anomalies:\")\n",
    "display(df_scored_regional.filter(F.col(\"anomaly_score\") >= ANOMALY_SCORE_THRESHOLD).orderBy(\n",
    "    F.col(\"anomaly_score\").desc()\n",
    ").select(\n",
    "    \"rep_id\", \"region\", \"calls_made\", \"prescriptions_generated\", \"expenses\",\n",
    "    F.round(\"anomaly_score\", 3).alias(\"score\")\n",
    ").limit(10))\n",
    "\n",
    "\n",
    "# Store metrics for comparison\n",
    "anomalies_regional = df_scored_regional.filter(F.col(\"anomaly_score\") >= ANOMALY_SCORE_THRESHOLD)\n",
    "segmented_detected = anomalies_regional.count()\n",
    "segmented_detection_rate = (segmented_detected / expected_anomalies) * 100 if expected_anomalies > 0 else 0\n",
    "segmented_flagged = df_scored_regional.filter(F.col(\"anomaly_score\") >= ANOMALY_SCORE_THRESHOLD).count()\n",
    "segmented_fp = max(0, segmented_flagged - expected_anomalies)\n",
    "segmented_fp_rate = (segmented_fp / normal_records_expected) * 100 if normal_records_expected > 0 else 0\n",
    "\n",
    "segmented_metrics = {\n",
    "    \"detected\": segmented_detected,\n",
    "    \"detection_rate\": segmented_detection_rate,\n",
    "    \"fp_rate\": segmented_fp_rate\n",
    "}\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Segmented Performance: {segmented_detected} detected ({segmented_detection_rate:.1f}% rate), {segmented_fp_rate:.2f}% FP\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Feature Contributions & Root Cause (8 min)\n",
    "\n",
    "**Why is a record anomalous?** Use SHAP to understand which columns drove the anomaly score.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.labs.dqx.rule import DQDatasetRule\n",
    "\n",
    "# Score with SHAP-based feature contributions\n",
    "checks_with_contrib = [\n",
    "    DQDatasetRule(\n",
    "        criticality=\"error\",\n",
    "        check_func=has_no_anomalies,\n",
    "        check_func_kwargs={\n",
    "            \"model\": \"field_force_regional\",\n",
    "            \"score_threshold\": ANOMALY_SCORE_THRESHOLD,  # Use configurable threshold\n",
    "            \"include_contributions\": True,  # Enable SHAP explanations\n",
    "            \"registry_table\": f\"{catalog}.{schema_name}.anomaly_model_registry\",\n",
    "            \"merge_columns\": [\"activity_id\"]\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "df_with_contrib = dq_engine.apply_checks(df_test, checks_with_contrib)\n",
    "\n",
    "print(f\"\ud83d\udd0d Top Anomalies with Feature Contributions (SHAP, threshold: {ANOMALY_SCORE_THRESHOLD}):\\\\n\")\n",
    "anomalies_contrib = df_with_contrib.filter(\n",
    "    F.col(\"anomaly_score\") >= ANOMALY_SCORE_THRESHOLD\n",
    ").orderBy(F.col(\"anomaly_score\").desc()).limit(10)\n",
    "\n",
    "display(anomalies_contrib.select(\n",
    "    \"rep_id\", \"region\",\n",
    "    \"calls_made\", \"prescriptions_generated\", \"samples_distributed\", \"expenses\",\n",
    "    F.round(\"anomaly_score\", 3).alias(\"score\"),\n",
    "    \"anomaly_contributions\"\n",
    "))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze contribution patterns for root cause\n",
    "print(\"\ud83d\udcca Root Cause Analysis:\\\\n\")\n",
    "\n",
    "top_anomaly = anomalies_contrib.first()\n",
    "print(f\"\ud83d\udd38 Top Anomaly: REP={top_anomaly['rep_id']}, Region={top_anomaly['region']}\")\n",
    "print(f\"   Score: {top_anomaly['anomaly_score']:.3f}\")\n",
    "print(f\"   Values:\")\n",
    "print(f\"     \u2022 calls_made: {top_anomaly['calls_made']}\")\n",
    "print(f\"     \u2022 prescriptions: {top_anomaly['prescriptions_generated']}\")\n",
    "print(f\"     \u2022 samples: {top_anomaly['samples_distributed']}\")\n",
    "print(f\"     \u2022 expenses: ${top_anomaly['expenses']:.2f}\")\n",
    "print(f\"\\\\n   \ud83d\udcc8 Feature Contributions (SHAP):\")\n",
    "\n",
    "if top_anomaly['anomaly_contributions']:\n",
    "    sorted_contribs = sorted(\n",
    "        top_anomaly['anomaly_contributions'].items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    for feature, contribution in sorted_contribs:\n",
    "        print(f\"      {feature:30s}: {contribution:.3f} ({contribution*100:.1f}%)\")\n",
    "\n",
    "print(\"\\\\n\ud83d\udca1 Business Interpretation Examples:\")\n",
    "print(\"   \u2022 High 'expenses' contribution \u2192 Potential fraud or policy violation\")\n",
    "print(\"   \u2022 High 'calls_made' + low 'prescriptions' \u2192 Training need or territory issue\")\n",
    "print(\"   \u2022 High 'prescriptions' contribution \u2192 Unrealistic claims to investigate\")\n",
    "print(\"   \u2022 Balanced contributions \u2192 Multivariate anomaly (multiple factors)\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### \ud83d\udcca Approach Comparison & Recommendations\n",
    "\n",
    "Let's compare all three approaches to see which performed best.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three approaches\n",
    "print(f\"\ud83c\udfc6 Performance Comparison (Threshold: {ANOMALY_SCORE_THRESHOLD})\\\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = [\n",
    "    (\"Auto-Discovery\", auto_metrics['detected'], auto_metrics['detection_rate'], auto_metrics['fp_rate']),\n",
    "    (\"Manual Tuned\", manual_metrics['detected'], manual_metrics['detection_rate'], manual_metrics['fp_rate']),\n",
    "    (\"Segmented (Regional)\", segmented_metrics['detected'], segmented_metrics['detection_rate'], segmented_metrics['fp_rate']),\n",
    "]\n",
    "\n",
    "# Create DataFrame for comparison\n",
    "comparison_df = spark.createDataFrame(comparison_data, [\"Approach\", \"Detected\", \"Detection_Rate_%\", \"FP_Rate_%\"])\n",
    "display(comparison_df)\n",
    "\n",
    "# Determine winner\n",
    "best_detection = max(auto_metrics['detection_rate'], manual_metrics['detection_rate'], segmented_metrics['detection_rate'])\n",
    "best_fp = min(auto_metrics['fp_rate'], manual_metrics['fp_rate'], segmented_metrics['fp_rate'])\n",
    "\n",
    "print(\"\\\\n\ud83c\udfaf Key Findings:\\\\n\")\n",
    "\n",
    "if segmented_metrics['detection_rate'] == best_detection:\n",
    "    print(\"\u2705 WINNER: Segmented approach has the BEST detection rate!\")\n",
    "    print(f\"   {segmented_metrics['detection_rate']:.1f}% detection with {segmented_metrics['fp_rate']:.2f}% false positives\")\n",
    "elif manual_metrics['detection_rate'] == best_detection:\n",
    "    print(\"\u2705 WINNER: Manual tuning has the BEST detection rate!\")\n",
    "    print(f\"   {manual_metrics['detection_rate']:.1f}% detection with {manual_metrics['fp_rate']:.2f}% false positives\")\n",
    "else:\n",
    "    print(\"\u2705 WINNER: Auto-discovery has the BEST detection rate!\")\n",
    "    print(f\"   {auto_metrics['detection_rate']:.1f}% detection with {auto_metrics['fp_rate']:.2f}% false positives\")\n",
    "\n",
    "print(\"\\\\n\ud83d\udca1 Recommendations:\\\\n\")\n",
    "print(\"| Approach | When to Use |\")\n",
    "print(\"|----------|-------------|\")\n",
    "print(\"| **Auto-Discovery** | Quick start, exploration, uniform data |\")\n",
    "print(\"| **Manual Tuned** | Production, known important features, single baseline |\")\n",
    "print(\"| **Segmented** | Multi-region/multi-product with different baselines |\")\n",
    "print(\"\\\\n\ud83d\udcc8 Best Practice: Start with auto-discovery, validate results, then refine with\")\n",
    "print(\"   manual tuning or segmentation based on your business context.\")\n",
    "\n",
    "# Show what segmentation helps with\n",
    "print(\"\\\\n\ud83c\udf0d Why Segmentation Works:\")\n",
    "print(f\"   \u2022 Different regions have different 'normal' patterns\")\n",
    "print(f\"   \u2022 US avg expenses: $150, APAC: $120, EU: $100\")\n",
    "print(f\"   \u2022 Segmented models catch region-specific anomalies better\")\n",
    "print(f\"   \u2022 Reduces false positives from natural regional differences\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcdd Note: All approaches use the same threshold ({ANOMALY_SCORE_THRESHOLD}).\")\n",
    "print(f\"   To experiment with different thresholds, change ANOMALY_SCORE_THRESHOLD at the top and re-run.\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Drift Detection & Retraining (6 min)\n",
    "\n",
    "Data distributions change over time. DQX can detect when your model becomes stale.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate drift: New patterns (more remote work, lower expenses post-policy change)\n",
    "def generate_drifted_data(num_rows=200):\n",
    "    \"\"\"Generate Q3 data with shifted distribution (post-policy change).\"\"\"\n",
    "    data = []\n",
    "    regions = [\"US\", \"EU\", \"APAC\"]\n",
    "    call_types = [\"promotional\", \"educational\", \"follow_up\"]\n",
    "    \n",
    "    # NEW PATTERNS: More remote work, lower expenses, similar productivity\n",
    "    new_patterns = {\n",
    "        \"US\": {\"calls\": (9, 2), \"prescriptions\": (12, 3), \"samples\": (20, 4), \"expenses\": (100, 20)},  # -33% expenses\n",
    "        \"EU\": {\"calls\": (7, 1.5), \"prescriptions\": (9, 2), \"samples\": (15, 3), \"expenses\": (70, 15)},   # -30% expenses\n",
    "        \"APAC\": {\"calls\": (11, 3), \"prescriptions\": (15, 4), \"samples\": (25, 6), \"expenses\": (85, 20)}, # -29% expenses\n",
    "    }\n",
    "    \n",
    "    start_date = datetime(2024, 7, 1)  # Q3 data\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        region = random.choice(regions)\n",
    "        pattern = new_patterns[region]\n",
    "        \n",
    "        calls = max(1, int(np.random.normal(pattern[\"calls\"][0], pattern[\"calls\"][1])))\n",
    "        prescriptions = max(0, int(np.random.normal(pattern[\"prescriptions\"][0], pattern[\"prescriptions\"][1])))\n",
    "        samples = max(0, int(np.random.normal(pattern[\"samples\"][0], pattern[\"samples\"][1])))\n",
    "        expenses = max(10, round(np.random.normal(pattern[\"expenses\"][0], pattern[\"expenses\"][1]), 2))\n",
    "        is_remote = random.random() < 0.7  # 70% remote now (was 30%)\n",
    "        call_type = random.choice(call_types)\n",
    "        \n",
    "        days_offset = random.randint(0, 90)\n",
    "        call_date = start_date + timedelta(days=days_offset)\n",
    "        \n",
    "        # Add activity_id and true_anomaly_type to match schema (11 columns)\n",
    "        data.append((\n",
    "            f\"ACT_DRIFT{i:06d}\",  # activity_id (primary key)\n",
    "            f\"REP{i % 50:03d}\",   # rep_id\n",
    "            region,\n",
    "            call_date,\n",
    "            calls,\n",
    "            prescriptions,\n",
    "            samples,\n",
    "            expenses,\n",
    "            is_remote,\n",
    "            call_type,\n",
    "            \"normal\"  # true_anomaly_type (drift data is normal, just shifted distribution)\n",
    "        ))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate and compare\n",
    "drifted_data = generate_drifted_data(num_rows=200)\n",
    "df_drifted = spark.createDataFrame(drifted_data, schema)\n",
    "\n",
    "print(\"\ud83d\udcca Original vs Drifted Data Comparison:\\\\n\")\n",
    "print(\"Original (Q1-Q2 2024):\") \n",
    "display(df_sales.agg(\n",
    "    F.avg(\"expenses\").alias(\"avg_expenses\"),\n",
    "    F.avg(F.col(\"is_remote\").cast(\"int\")).alias(\"remote_rate\")\n",
    "))\n",
    "\n",
    "print(\"Drifted (Q3 2024 - post policy change):\")\n",
    "display(df_drifted.agg(\n",
    "    F.avg(\"expenses\").alias(\"avg_expenses\"),\n",
    "    F.avg(F.col(\"is_remote\").cast(\"int\")).alias(\"remote_rate\")\n",
    "))\n",
    "\n",
    "print(\"\u2705 Distribution shifted:\")\n",
    "print(\"   \u2022 Expenses: -30% (policy change)\")\n",
    "print(\"   \u2022 Remote work: +133% (70% vs 30%)\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXPLICIT DRIFT STATISTICS ===\n",
    "# Python warnings can get lost in Databricks output, so let's explicitly compute drift\n",
    "\n",
    "from databricks.labs.dqx.anomaly.drift_detector import compute_drift_score\n",
    "\n",
    "print(\"\ud83d\udcca Explicit Drift Analysis:\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get the trained model's baseline statistics\n",
    "registry_df = spark.table(f\"{catalog}.{schema_name}.anomaly_model_registry\")\n",
    "regional_models = registry_df.filter(F.col(\"model_name\").like(\"%field_force_regional%\"))\n",
    "\n",
    "# Check drift for each region\n",
    "for row in regional_models.collect():\n",
    "    region = row[\"segment_values\"][\"region\"] if row[\"segment_values\"] else \"Global\"\n",
    "    baseline_stats = row[\"baseline_stats\"]\n",
    "    columns = row[\"columns\"]\n",
    "    \n",
    "    # Filter drifted data for this region/segment\n",
    "    if row[\"segment_values\"]:\n",
    "        segment_filter = \" AND \".join([f\"{k} = '{v}'\" for k, v in row[\"segment_values\"].items()])\n",
    "        df_segment = df_drifted.filter(segment_filter)\n",
    "    else:\n",
    "        df_segment = df_drifted\n",
    "    \n",
    "    # Compute drift\n",
    "    drift_result = compute_drift_score(\n",
    "        df_segment.select(columns),\n",
    "        columns,\n",
    "        baseline_stats,\n",
    "        DRIFT_THRESHOLD\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\ud83c\udf0d Region: {region}\")\n",
    "    print(f\"   Drift Score: {drift_result.drift_score:.2f}\")\n",
    "    print(f\"   Drift Detected: {'\ud83d\udea8 YES' if drift_result.drift_detected else '\u2705 NO'}\")\n",
    "    \n",
    "    if drift_result.drifted_columns:\n",
    "        print(f\"   Drifted Columns: {', '.join(drift_result.drifted_columns)}\")\n",
    "        \n",
    "        # Show baseline vs current stats for drifted columns\n",
    "        print(f\"\\n   \ud83d\udcc8 Baseline vs Current:\")\n",
    "        for col in drift_result.drifted_columns:\n",
    "            if col in baseline_stats:\n",
    "                baseline = baseline_stats[col]\n",
    "                current = df_segment.select(col).agg(\n",
    "                    F.avg(col).alias(\"mean\"),\n",
    "                    F.stddev(col).alias(\"stddev\")\n",
    "                ).first()\n",
    "                \n",
    "                print(f\"      {col}:\")\n",
    "                print(f\"         Baseline: mean={baseline['mean']:.2f}, std={baseline['std']:.2f}\")\n",
    "                print(f\"         Current:  mean={current['mean']:.2f}, std={current['stddev']:.2f}\")\n",
    "                print(f\"         Change:   {((current['mean'] - baseline['mean']) / baseline['mean'] * 100):.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"\\n\ud83d\udca1 Threshold: {DRIFT_THRESHOLD}\")\n",
    "print(\"   Drift score > threshold \u2192 Model needs retraining\")\n",
    "print(\"   High drift in 'expenses' is expected (policy change: -30% expenses)\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.labs.dqx.rule import DQDatasetRule\n",
    "\n",
    "# Score drifted data with drift detection enabled\n",
    "checks_with_drift = [\n",
    "    DQDatasetRule(\n",
    "        criticality=\"error\",\n",
    "        check_func=has_no_anomalies,\n",
    "        check_func_kwargs={\n",
    "            \"model\": \"field_force_regional\",\n",
    "            \"drift_threshold\": DRIFT_THRESHOLD,  # Use configurable threshold\n",
    "            \"registry_table\": f\"{catalog}.{schema_name}.anomaly_model_registry\",\n",
    "            \"merge_columns\": [\"activity_id\"]\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"\ud83d\udd0d Scoring drifted data with drift detection (threshold: {DRIFT_THRESHOLD})...\\n\")\n",
    "\n",
    "df_drift_scored = dq_engine.apply_checks(df_drifted, checks_with_drift)\n",
    "\n",
    "print(f\"\\n\ud83d\udca1 Drift score > {DRIFT_THRESHOLD} \u2192 Significant distribution shift, retrain recommended\")\n",
    "print(\"   DQX will show UserWarnings if drift is detected:\")\n",
    "print(\"   \ud83d\udea8 'DATA DRIFT DETECTED in columns: expenses (drift score: 4.2)...'\")\n",
    "print(\"\\n\u2705 Check cell output above for any drift UserWarnings.\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain with combined data\n",
    "df_combined = df_sales.union(df_drifted)\n",
    "\n",
    "print(\"\ud83d\udd04 Retraining model with combined data (old + new patterns)...\\\\n\")\n",
    "\n",
    "# Note: We explicitly select features, excluding activity_id and true_anomaly_type\n",
    "model_name_retrained = anomaly_engine.train(\n",
    "    df=df_combined,\n",
    "    columns=[\"calls_made\", \"prescriptions_generated\", \"samples_distributed\", \"expenses\"],\n",
    "    segment_by=[\"region\"],\n",
    "    model_name=\"field_force_regional\",  # Same name = new version\n",
    "    params=AnomalyParams(\n",
    "        algorithm_config=IsolationForestConfig(contamination=0.05, num_trees=150, random_seed=42)\n",
    "    ),\n",
    "    registry_table=f\"{catalog}.{schema_name}.anomaly_model_registry\"\n",
    ")\n",
    "\n",
    "print(\"\\\\n\u2705 Model retrained!\")\n",
    "print(\"   \u2022 Old model automatically archived\")\n",
    "print(\"   \u2022 New model active and includes both historical and recent patterns\")\n",
    "print(\"   \u2022 Baseline updated to reflect new expense policy and remote work rates\")\n",
    "print(\"\\\\n\ud83d\udca1 Best Practice: Set up drift monitoring in production, retrain monthly/quarterly\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Production Integration (6 min)\n",
    "\n",
    "Integrate anomaly detection into your DQX workflows for automated monitoring.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine anomaly detection with traditional DQ checks\n",
    "from databricks.labs.dqx.rule import DQRowRule, DQDatasetRule\n",
    "\n",
    "checks_combined = [\n",
    "    # Traditional data quality checks - one per column\n",
    "    DQRowRule(\n",
    "        criticality=\"error\",\n",
    "        check_func=is_not_null,\n",
    "        column=\"rep_id\",\n",
    "        name=\"rep_id_not_null\"\n",
    "    ),\n",
    "    DQRowRule(\n",
    "        criticality=\"error\",\n",
    "        check_func=is_not_null,\n",
    "        column=\"region\",\n",
    "        name=\"region_not_null\"\n",
    "    ),\n",
    "    DQRowRule(\n",
    "        criticality=\"error\",\n",
    "        check_func=is_not_null,\n",
    "        column=\"call_date\",\n",
    "        name=\"call_date_not_null\"\n",
    "    ),\n",
    "    DQRowRule(\n",
    "        criticality=\"error\",\n",
    "        check_func=is_in_range,\n",
    "        column=\"calls_made\",\n",
    "        name=\"calls_range\",\n",
    "        check_func_kwargs={\"min_limit\": 0, \"max_limit\": 50\n",
    "        }\n",
    "    ),\n",
    "    DQRowRule(\n",
    "        criticality=\"error\",\n",
    "        check_func=is_in_range,\n",
    "        column=\"expenses\",\n",
    "        name=\"expenses_range\",\n",
    "        check_func_kwargs={\"min_limit\": 0, \"max_limit\": 1000}\n",
    "    ),\n",
    "    \n",
    "    # ML-based anomaly detection with explanations\n",
    "    DQDatasetRule(\n",
    "        criticality=\"error\",\n",
    "        check_func=has_no_anomalies,\n",
    "        check_func_kwargs={\n",
    "            \"model\": \"field_force_regional\",\n",
    "            \"score_threshold\": ANOMALY_SCORE_THRESHOLD,  # Use configurable threshold\n",
    "            \"include_contributions\": True,\n",
    "            \"drift_threshold\": DRIFT_THRESHOLD,  # Use configurable threshold\n",
    "            \"registry_table\": f\"{catalog}.{schema_name}.anomaly_model_registry\",\n",
    "            \"merge_columns\": [\"activity_id\"]\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "# Apply all checks together\n",
    "df_full_dq = dq_engine.apply_checks(df_test, checks_combined)\n",
    "\n",
    "# Summary\n",
    "print(f\"\ud83d\udcca Full Data Quality Summary (threshold: {ANOMALY_SCORE_THRESHOLD}):\\\\n\")\n",
    "total_rows = df_full_dq.count()\n",
    "anomalies_found = df_full_dq.filter(F.col(\"anomaly_score\") >= ANOMALY_SCORE_THRESHOLD).count()\n",
    "\n",
    "# Note: Traditional check condition columns would have specific names based on implementation\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Anomalies Detected: {anomalies_found}\")\n",
    "print(f\"Clean Records: {total_rows - anomalies_found}\")\n",
    "print(f\"\\\\n\u2705 All checks applied in single pass!\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === QUARANTINE WORKFLOW (DQX Standard Pattern) ===\n",
    "# Use DQX's built-in split method to separate valid from quarantined records\n",
    "\n",
    "print(\"\ud83d\udd00 Applying quarantine workflow...\")\n",
    "print(f\"   Threshold: {ANOMALY_SCORE_THRESHOLD}\\n\")\n",
    "\n",
    "# Split valid and quarantined data using DQX standard method\n",
    "valid_df, quarantine_df = dq_engine.apply_checks_and_split(df_test, checks_combined)\n",
    "\n",
    "print(f\"\u2705 Valid records: {valid_df.count()}\")\n",
    "print(f\"\u26a0\ufe0f  Quarantined for review: {quarantine_df.count()}\\n\")\n",
    "\n",
    "# Save both valid and quarantine data using DQX standard method\n",
    "dq_engine.save_results_in_table(\n",
    "    output_df=valid_df,\n",
    "    quarantine_df=quarantine_df,\n",
    "    output_config=OutputConfig(\n",
    "        location=f\"{catalog}.{schema_name}.field_force_clean\",\n",
    "        mode=\"overwrite\"\n",
    "    ),\n",
    "    quarantine_config=OutputConfig(\n",
    "        location=f\"{catalog}.{schema_name}.field_force_quarantine\",\n",
    "        mode=\"overwrite\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"\ud83d\udcbe Saved valid data to: {catalog}.{schema_name}.field_force_clean\")\n",
    "print(f\"\ud83d\udcbe Saved quarantine to: {catalog}.{schema_name}.field_force_quarantine\")\n",
    "\n",
    "# Display quarantine summary\n",
    "print(\"\\n\ud83d\udcca Quarantine Summary by Region:\")\n",
    "quarantine_summary = spark.table(f\"{catalog}.{schema_name}.field_force_quarantine\").groupBy(\"region\").agg(\n",
    "    F.count(\"*\").alias(\"count\"),\n",
    "    F.avg(\"anomaly_score\").alias(\"avg_score\"),\n",
    "    F.max(\"anomaly_score\").alias(\"max_score\")\n",
    ").orderBy(\"region\")\n",
    "display(quarantine_summary)\n",
    "\n",
    "# Show top quarantined records with explanations\n",
    "print(\"\\n\ud83d\udccb Top Quarantined Records (for Manual Review):\")\n",
    "display(\n",
    "    spark.table(f\"{catalog}.{schema_name}.field_force_quarantine\")\n",
    "    .orderBy(F.desc(\"anomaly_score\"))\n",
    "    .select(\n",
    "        \"activity_id\", \"rep_id\", \"region\", \"calls_made\", \"prescriptions_generated\", \n",
    "        \"expenses\", F.round(\"anomaly_score\", 3).alias(\"score\"),\n",
    "        \"anomaly_contributions\", \"_errors\"\n",
    "    )\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Quarantine Workflow Best Practices:\")\n",
    "print(\"   1. Anomalies automatically sent to quarantine table\")\n",
    "print(\"   2. Review team investigates using anomaly_contributions\")\n",
    "print(\"   3. Check _errors column for all DQ violations\")\n",
    "print(\"   4. Confirmed issues \u2192 escalate to appropriate team\")\n",
    "print(\"   5. False positives \u2192 retune model or adjust threshold\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAML Configuration for Production\n",
    "\n",
    "For automated workflows, define checks in YAML:\n",
    "\n",
    "```yaml\n",
    "run_configs:\n",
    "  - name: field_force_monitoring\n",
    "    input_config:\n",
    "      location: catalog.schema.field_force_activity\n",
    "    \n",
    "    # Traditional checks\n",
    "    quality_checks:\n",
    "      - function: is_not_null\n",
    "        arguments:\n",
    "          columns: [rep_id, region, call_date]\n",
    "      - function: is_in_range\n",
    "        arguments:\n",
    "          column: calls_made\n",
    "          min_value: 0\n",
    "          max_value: 50\n",
    "      - function: is_in_range\n",
    "        arguments:\n",
    "          column: expenses\n",
    "          min_value: 0\n",
    "          max_value: 1000\n",
    "    \n",
    "    # Anomaly detection\n",
    "    anomaly_config:\n",
    "      columns: [calls_made, prescriptions_generated, samples_distributed, expenses]\n",
    "      segment_by: [region]\n",
    "      model_name: field_force_regional\n",
    "      registry_table: catalog.schema.anomaly_model_registry\n",
    "      params:\n",
    "        algorithm_config:\n",
    "          contamination: 0.05\n",
    "          num_trees: 150\n",
    "          random_state: 42\n",
    "        sample_fraction: 1.0\n",
    "    \n",
    "    # Quarantine configuration\n",
    "    quarantine_config:\n",
    "      enabled: true\n",
    "      table: catalog.schema.field_force_quarantine\n",
    "      \n",
    "    # Output configuration\n",
    "    output_config:\n",
    "      location: catalog.schema.field_force_clean\n",
    "      save_mode: overwrite\n",
    "```\n",
    "\n",
    "**Run with:**\n",
    "```bash\n",
    "# Train model (one-time or scheduled)\n",
    "databricks bundle run anomaly_trainer\n",
    "\n",
    "# Run quality checks (scheduled, e.g., daily)\n",
    "databricks bundle run quality_checker\n",
    "```\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udf93 Summary\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "1. \u2705 **Auto-Discovery vs Manual Tuning** - Start with zero-config, refine with domain knowledge\n",
    "2. \u2705 **Parameter Tuning** - contamination, num_trees, max_samples for better performance\n",
    "3. \u2705 **Segment-Based Monitoring** - Regional baselines prevent false positives (US vs EU vs APAC)\n",
    "4. \u2705 **Feature Contributions** - SHAP-based root cause analysis for investigation\n",
    "5. \u2705 **Drift Detection** - Automated signals for when to retrain models\n",
    "6. \u2705 **Multi-Type Features** - Numeric, categorical, datetime, boolean all work together\n",
    "7. \u2705 **Production Integration** - DQEngine + YAML workflows + quarantine handling\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Start simple**: `train(df)` with auto-discovery, then refine\n",
    "- **Tune parameters**: Set contamination to expected anomaly rate, increase num_trees for stability\n",
    "- **Use segments**: Different baselines for different groups prevent false positives\n",
    "- **Enable contributions**: Root cause analysis is critical for business value\n",
    "- **Monitor drift**: Set up drift detection for automated retraining signals\n",
    "- **Combine checks**: Anomaly detection complements traditional DQ rules\n",
    "- **Quarantine workflow**: Automate review process with explanations\n",
    "\n",
    "### Model Comparison Results:\n",
    "\n",
    "| Approach | Columns | Segments | Tuning | Use Case |\n",
    "|----------|---------|----------|--------|----------|\n",
    "| Auto-discovery | Auto (priority-based) | Auto (if applicable) | Default | Quick start, exploration |\n",
    "| Manual tuned | Hand-picked | Manual | Custom hyperparameters | Production, refined monitoring |\n",
    "| Regional | Hand-picked | By region | Tuned contamination | Multi-region with different baselines |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Apply to your data**: `train(df=spark.table(\"your_table\"))`\n",
    "2. **Set up YAML workflows**: Automate training and checking\n",
    "3. **Integrate quarantine**: Build review process with feature contributions\n",
    "4. **Schedule retraining**: Weekly/monthly based on drift monitoring\n",
    "5. **Monitor metrics**: Track anomaly rates, drift scores, false positive rates\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [DQX Anomaly Detection Documentation](https://databrickslabs.github.io/dqx/guide/anomaly_detection)\n",
    "- [API Reference](https://databrickslabs.github.io/dqx/reference/quality_checks#has_no_anomalies)\n",
    "- [GitHub Repository](https://github.com/databrickslabs/dqx)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions? Feedback?** Open an issue on GitHub or contact the DQX team!\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}