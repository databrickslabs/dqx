{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding IsolationForest and SHAP\n",
        "\n",
        "This notebook explains the two core technologies behind DQX's anomaly detection feature:\n",
        "- **IsolationForest**: A tree-based algorithm for detecting anomalies\n",
        "- **SHAP (SHapley Additive exPlanations)**: A method for explaining which features contributed to a prediction\n",
        "\n",
        "We'll use synthetic e-commerce transaction data to demonstrate these concepts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (if needed)\n",
        "# !pip install faker scikit-learn shap pandas numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "fake = Faker()\n",
        "Faker.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Generate Synthetic Data\n",
        "\n",
        "We'll create a dataset of e-commerce transactions with:\n",
        "- Normal transactions: Typical purchase amounts, quantities, and discounts\n",
        "- Anomalous transactions: Unusual patterns (e.g., very high amounts, unusual quantities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_normal_transactions(n=1000):\n",
        "    \"\"\"Generate normal e-commerce transactions.\"\"\"\n",
        "    data = []\n",
        "    for _ in range(n):\n",
        "        amount = np.random.normal(100, 30)  # Mean $100, std $30\n",
        "        quantity = np.random.randint(1, 5)   # 1-4 items\n",
        "        discount = np.random.uniform(0, 0.2) # 0-20% discount\n",
        "        data.append({\n",
        "            'transaction_id': fake.uuid4(),\n",
        "            'amount': max(10, amount),  # Minimum $10\n",
        "            'quantity': quantity,\n",
        "            'discount': discount\n",
        "        })\n",
        "    return data\n",
        "\n",
        "def generate_anomalous_transactions(n=50):\n",
        "    \"\"\"Generate anomalous transactions with unusual patterns.\"\"\"\n",
        "    data = []\n",
        "    for _ in range(n):\n",
        "        anomaly_type = np.random.choice(['high_amount', 'high_quantity', 'high_discount'])\n",
        "        \n",
        "        if anomaly_type == 'high_amount':\n",
        "            amount = np.random.uniform(500, 2000)  # Unusually high\n",
        "            quantity = np.random.randint(1, 5)\n",
        "            discount = np.random.uniform(0, 0.2)\n",
        "        elif anomaly_type == 'high_quantity':\n",
        "            amount = np.random.normal(100, 30)\n",
        "            quantity = np.random.randint(20, 100)  # Unusually high\n",
        "            discount = np.random.uniform(0, 0.2)\n",
        "        else:  # high_discount\n",
        "            amount = np.random.normal(100, 30)\n",
        "            quantity = np.random.randint(1, 5)\n",
        "            discount = np.random.uniform(0.7, 0.99)  # Unusually high\n",
        "        \n",
        "        data.append({\n",
        "            'transaction_id': fake.uuid4(),\n",
        "            'amount': max(10, amount),\n",
        "            'quantity': quantity,\n",
        "            'discount': discount,\n",
        "            'true_label': 'anomaly'  # Ground truth for evaluation\n",
        "        })\n",
        "    return data\n",
        "\n",
        "# Generate data\n",
        "normal_data = generate_normal_transactions(1000)\n",
        "anomalous_data = generate_anomalous_transactions(50)\n",
        "\n",
        "# Add ground truth labels\n",
        "for record in normal_data:\n",
        "    record['true_label'] = 'normal'\n",
        "\n",
        "# Combine and shuffle\n",
        "all_data = normal_data + anomalous_data\n",
        "np.random.shuffle(all_data)\n",
        "\n",
        "df = pd.DataFrame(all_data)\n",
        "print(f\"Generated {len(df)} transactions ({len(normal_data)} normal, {len(anomalous_data)} anomalous)\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"Data Statistics:\\n\")\n",
        "df[['amount', 'quantity', 'discount']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: IsolationForest - How It Works\n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        "1. **Isolation Trees**: The algorithm builds random decision trees that isolate data points\n",
        "2. **Path Length**: Anomalies are easier to isolate (shorter path from root to leaf)\n",
        "3. **Anomaly Score**: Normalized score where values closer to 1 indicate anomalies\n",
        "\n",
        "### Why It's Effective:\n",
        "- No need to define what \"normal\" looks like explicitly\n",
        "- Works well with high-dimensional data\n",
        "- Fast and scalable\n",
        "- Unsupervised: doesn't need labeled training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features for training\n",
        "feature_columns = ['amount', 'quantity', 'discount']\n",
        "X = df[feature_columns].values\n",
        "\n",
        "# Train IsolationForest\n",
        "# contamination: expected proportion of anomalies (5% in our case: 50/1050)\n",
        "model = IsolationForest(\n",
        "    n_estimators=100,      # Number of trees\n",
        "    contamination=0.05,    # Expected % of anomalies\n",
        "    random_state=42,\n",
        "    max_samples='auto'     # Use all samples for training\n",
        ")\n",
        "\n",
        "print(\"Training IsolationForest...\")\n",
        "model.fit(X)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Get predictions\n",
        "# predict() returns: 1 for normal, -1 for anomaly\n",
        "predictions = model.predict(X)\n",
        "df['prediction'] = predictions\n",
        "df['prediction_label'] = df['prediction'].map({1: 'normal', -1: 'anomaly'})\n",
        "\n",
        "# Get anomaly scores\n",
        "# score_samples() returns negative values: more negative = more anomalous\n",
        "# We'll convert to positive scale for easier interpretation\n",
        "anomaly_scores = model.score_samples(X)\n",
        "df['anomaly_score'] = -anomaly_scores  # Negate so higher = more anomalous\n",
        "\n",
        "print(f\"\\nPredicted {(predictions == -1).sum()} anomalies out of {len(df)} transactions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model performance\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Convert labels for evaluation\n",
        "y_true = (df['true_label'] == 'anomaly').astype(int)\n",
        "y_pred = (df['prediction'] == -1).astype(int)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=['Normal', 'Anomaly']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize anomaly score distribution\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "normal_scores = df[df['true_label'] == 'normal']['anomaly_score']\n",
        "anomaly_scores = df[df['true_label'] == 'anomaly']['anomaly_score']\n",
        "\n",
        "plt.hist(normal_scores, bins=50, alpha=0.7, label='Normal (Ground Truth)', color='blue')\n",
        "plt.hist(anomaly_scores, bins=50, alpha=0.7, label='Anomaly (Ground Truth)', color='red')\n",
        "plt.xlabel('Anomaly Score (higher = more anomalous)')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Anomaly Scores by True Label')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(df['amount'], df['quantity'], c=df['anomaly_score'], \n",
        "            cmap='YlOrRd', alpha=0.6, s=30)\n",
        "plt.colorbar(label='Anomaly Score')\n",
        "plt.xlabel('Transaction Amount ($)')\n",
        "plt.ylabel('Quantity')\n",
        "plt.title('Transactions colored by Anomaly Score')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: SHAP - Explaining Model Predictions\n",
        "\n",
        "### The Problem:\n",
        "IsolationForest tells us a transaction is anomalous, but **why**? Which features made it anomalous?\n",
        "\n",
        "### SHAP Solution:\n",
        "SHAP (SHapley Additive exPlanations) is based on game theory and provides:\n",
        "- **Feature attribution**: How much each feature contributed to the prediction\n",
        "- **Consistent**: Contributions sum to the prediction value\n",
        "- **Local explanations**: Per-row feature importance\n",
        "\n",
        "### TreeSHAP:\n",
        "For tree-based models like IsolationForest, SHAP uses an optimized algorithm called TreeSHAP:\n",
        "- Fast computation (polynomial time instead of exponential)\n",
        "- Exact Shapley values (not approximations)\n",
        "- Works by tracing decision paths through the trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize SHAP TreeExplainer\n",
        "print(\"Creating SHAP explainer...\")\n",
        "explainer = shap.TreeExplainer(model)\n",
        "\n",
        "# Calculate SHAP values for all transactions\n",
        "# This tells us which features contributed to each anomaly score\n",
        "print(\"Computing SHAP values...\")\n",
        "shap_values = explainer.shap_values(X)\n",
        "print(f\"Shape of SHAP values: {shap_values.shape}\")\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding SHAP Values:\n",
        "\n",
        "For each transaction and each feature:\n",
        "- **Positive SHAP value**: Feature pushed the score toward \"more anomalous\"\n",
        "- **Negative SHAP value**: Feature pushed the score toward \"more normal\"\n",
        "- **Magnitude**: How strong the contribution was\n",
        "\n",
        "Let's examine some specific examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find top anomalies\n",
        "top_anomalies = df.nlargest(5, 'anomaly_score')\n",
        "\n",
        "print(\"Top 5 Most Anomalous Transactions:\\n\")\n",
        "for idx, row in top_anomalies.iterrows():\n",
        "    print(f\"Transaction {row['transaction_id'][:8]}...\")\n",
        "    print(f\"  Amount: ${row['amount']:.2f}, Quantity: {row['quantity']}, Discount: {row['discount']:.1%}\")\n",
        "    print(f\"  Anomaly Score: {row['anomaly_score']:.3f}\")\n",
        "    print(f\"  Ground Truth: {row['true_label']}\")\n",
        "    \n",
        "    # Get SHAP values for this transaction\n",
        "    shap_vals = shap_values[idx]\n",
        "    \n",
        "    # Calculate absolute contributions (normalized to sum to 1.0)\n",
        "    abs_shap = np.abs(shap_vals)\n",
        "    if abs_shap.sum() > 0:\n",
        "        contributions = abs_shap / abs_shap.sum()\n",
        "    else:\n",
        "        contributions = np.ones(len(feature_columns)) / len(feature_columns)\n",
        "    \n",
        "    print(\"  Feature Contributions:\")\n",
        "    for i, col in enumerate(feature_columns):\n",
        "        print(f\"    {col}: {contributions[i]:.1%}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SHAP Summary Plot: Shows global feature importance\n",
        "# Each point is a transaction, color represents feature value\n",
        "plt.figure(figsize=(10, 6))\n",
        "shap.summary_plot(shap_values, X, feature_names=feature_columns, show=False)\n",
        "plt.title('SHAP Summary Plot: Global Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Interpretation:\")\n",
        "print(\"- X-axis: SHAP value (impact on model output)\")\n",
        "print(\"- Y-axis: Features ranked by importance\")\n",
        "print(\"- Color: Feature value (red=high, blue=low)\")\n",
        "print(\"- Each dot: One transaction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SHAP Waterfall Plot: Detailed explanation for a single anomalous transaction\n",
        "anomaly_idx = top_anomalies.index[0]  # Most anomalous transaction\n",
        "\n",
        "print(f\"Detailed Explanation for Most Anomalous Transaction:\")\n",
        "print(f\"Transaction {df.loc[anomaly_idx, 'transaction_id'][:8]}...\\n\")\n",
        "\n",
        "# Create explanation object\n",
        "explanation = shap.Explanation(\n",
        "    values=shap_values[anomaly_idx],\n",
        "    base_values=explainer.expected_value,\n",
        "    data=X[anomaly_idx],\n",
        "    feature_names=feature_columns\n",
        ")\n",
        "\n",
        "# Waterfall plot shows how features push prediction from base value to final value\n",
        "plt.figure(figsize=(10, 6))\n",
        "shap.waterfall_plot(explanation, show=False)\n",
        "plt.title('SHAP Waterfall: How Features Contributed to Anomaly Score')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Starts from base value (average prediction)\")\n",
        "print(\"- Each row shows how a feature pushed the score up or down\")\n",
        "print(\"- Final value is the anomaly score for this transaction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: How This Maps to DQX\n",
        "\n",
        "DQX's `anomaly.has_no_anomalies()` check uses these same concepts in a distributed Spark environment.\n",
        "\n",
        "### Key Implementation Details:\n",
        "\n",
        "1. **Why Pandas UDFs?**\n",
        "   - Distributes computation across Spark cluster executors\n",
        "   - Each executor loads the model once, scores many rows\n",
        "   - Balances performance vs. model distribution overhead\n",
        "\n",
        "2. **Why scikit-learn instead of Spark ML?**\n",
        "   - IsolationForest not available in open-source PySpark\n",
        "   - scikit-learn works everywhere (local, DBR, Spark Connect)\n",
        "   - Mature, well-tested implementation\n",
        "\n",
        "3. **Why SHAP needs cluster installation?**\n",
        "   - UDF code runs on executors, not just the driver\n",
        "   - Each executor needs `shap` package installed\n",
        "   - That's why users see: \"Install 'shap' on your cluster\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Performance Considerations\n",
        "\n",
        "Understanding the computational costs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Benchmark scoring performance\n",
        "n_rows = 10000\n",
        "X_benchmark = np.random.randn(n_rows, len(feature_columns))\n",
        "\n",
        "# 1. IsolationForest scoring (fast)\n",
        "start = time.time()\n",
        "scores = model.score_samples(X_benchmark)\n",
        "predictions = model.predict(X_benchmark)\n",
        "scoring_time = time.time() - start\n",
        "\n",
        "print(f\"IsolationForest Scoring:\")\n",
        "print(f\"  {n_rows:,} rows in {scoring_time:.3f} seconds\")\n",
        "print(f\"  {n_rows/scoring_time:.0f} rows/second\")\n",
        "\n",
        "# 2. SHAP computation (slower, but provides explanations)\n",
        "start = time.time()\n",
        "shap_vals = explainer.shap_values(X_benchmark)\n",
        "shap_time = time.time() - start\n",
        "\n",
        "print(f\"\\nSHAP Computation:\")\n",
        "print(f\"  {n_rows:,} rows in {shap_time:.3f} seconds\")\n",
        "print(f\"  {n_rows/shap_time:.0f} rows/second\")\n",
        "print(f\"  {shap_time/scoring_time:.1f}x slower than scoring alone\")\n",
        "\n",
        "print(f\"\\nDQX Design Choice:\")\n",
        "print(f\"  - anomaly_score: Always computed (fast)\")\n",
        "print(f\"  - anomaly_contributions: Optional (include_contributions=True)\")\n",
        "print(f\"  - This gives users control over the speed/explainability tradeoff\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### IsolationForest:\n",
        "- **What**: Tree-based unsupervised anomaly detection\n",
        "- **How**: Isolates anomalies using random decision trees\n",
        "- **Output**: Anomaly score (higher = more anomalous) and binary prediction\n",
        "- **Pros**: Fast, scalable, no labeled data needed\n",
        "\n",
        "### SHAP (TreeSHAP):\n",
        "- **What**: Explainability framework based on game theory\n",
        "- **How**: Computes Shapley values by tracing tree paths\n",
        "- **Output**: Per-feature contributions to each prediction\n",
        "- **Pros**: Accurate, consistent, theoretically sound\n",
        "- **Cons**: Computationally expensive (3-10x slower than scoring)\n",
        "\n",
        "### DQX Integration:\n",
        "- Uses scikit-learn IsolationForest for model training\n",
        "- Distributes scoring via Spark Pandas UDFs\n",
        "- Optionally computes SHAP values for explainability\n",
        "- Stores models in Unity Catalog via MLflow\n",
        "- Makes SHAP optional to balance performance vs. explainability\n",
        "\n",
        "### Key Takeaway:\n",
        "DQX gives users the power of IsolationForest anomaly detection at Spark scale, with optional SHAP explanations when they need to understand **why** a row was flagged as anomalous."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
