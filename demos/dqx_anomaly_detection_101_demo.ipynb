{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Simple Anomaly Detection Demo\n",
        "\n",
        "## Learn Anomaly Detection in 15 Minutes\n",
        "\n",
        "This beginner-friendly demo shows you how to:\n",
        "- Understand what anomaly detection is and why it matters\n",
        "- Detect unusual patterns in your data with zero configuration\n",
        "- Tune detection sensitivity to your needs\n",
        "- Understand why specific records are flagged\n",
        "- Integrate anomaly detection into production workflows\n",
        "\n",
        "**Dataset**: Simple sales transactions (universally relatable, no domain expertise required)\n",
        "\n",
        "**Time**: 12-17 minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 0: What is Anomaly Detection in Data Quality?\n",
        "\n",
        "Before we dive into the code, let's understand what anomaly detection is and why it's valuable.\n",
        "\n",
        "### The Data Quality Challenge: Known vs Unknown Issues\n",
        "\n",
        "#### üéØ Known Unknowns (Traditional Data Quality)\n",
        "\n",
        "These are issues **you can anticipate** and write rules for:\n",
        "\n",
        "| Issue Type | Example | Solution |\n",
        "|------------|---------|----------|\n",
        "| Null values | `amount` is NULL | `is_not_null(column=\"amount\")` |\n",
        "| Out of range | Price is negative | `is_in_range(column=\"price\", min=0)` |\n",
        "| Invalid format | Email without @ symbol | Regex validation |\n",
        "\n",
        "**Works great when you know what to look for!**\n",
        "\n",
        "#### üîç Unknown Unknowns (Anomaly Detection)\n",
        "\n",
        "These are issues **you DON'T know to look for**:\n",
        "\n",
        "- Unusual **patterns** across multiple columns\n",
        "- Outlier **combinations** that are individually valid\n",
        "- Subtle **data corruption** that passes all rules\n",
        "\n",
        "**Problem**: You can't write rules for things you haven't thought of!\n",
        "\n",
        "**Solution**: ML-based anomaly detection learns \"normal\" patterns from your data and flags deviations.\n",
        "\n",
        "### Concrete Example\n",
        "\n",
        "```\n",
        "Known Unknown:  \"Amount must be positive\"\n",
        "                ‚Üí is_in_range(min=0)\n",
        "                ‚úÖ Catches: amount = -50\n",
        "\n",
        "Unknown Unknown: \"Transaction for $47,283 at 3am on Sunday for 2 items\"\n",
        "                 ‚Üí Anomaly detection\n",
        "                 ‚úÖ Catches: All fields valid individually, but pattern is unusual\n",
        "```\n",
        "\n",
        "### Why Anomaly Detection Matters\n",
        "\n",
        "- ‚úÖ **Catches issues before they become problems** - Early warning system\n",
        "- ‚úÖ **No need to anticipate every failure mode** - Adapts to your data\n",
        "- ‚úÖ **Learns patterns automatically** - No manual rule writing\n",
        "- ‚úÖ **Complements rule-based checks** - Use both together for comprehensive quality\n",
        "\n",
        "### Unity Catalog Integration\n",
        "\n",
        "#### Built-in Quality Monitoring (Unity Catalog)\n",
        "\n",
        "Unity Catalog includes **table-level** anomaly detection:\n",
        "- Monitors column statistics and distributions\n",
        "- Alerts on schema changes, cardinality shifts\n",
        "- Tracks null rate changes over time\n",
        "- Great for monitoring table health\n",
        "\n",
        "#### When to Use DQX Anomaly Detection\n",
        "\n",
        "DQX provides **row-level** anomaly detection:\n",
        "- Detect unusual individual **records/transactions**\n",
        "- **Multi-column pattern** detection (e.g., price + quantity + time)\n",
        "- **Custom models per segment** (e.g., different regions, categories)\n",
        "- **Feature contributions** to understand WHY records are anomalous\n",
        "- **Integration** with existing DQX quality pipelines\n",
        "\n",
        "#### Complementary Approach\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Rule-Based Checks (Known Unknowns)                     ‚îÇ\n",
        "‚îÇ  ‚Ä¢ is_not_null, is_in_range, regex validation          ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Schema validation, referential integrity             ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                           +\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  ML Anomaly Detection (Unknown Unknowns)                ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Pattern detection, outlier identification            ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Multi-column relationship validation                 ‚îÇ\n",
        "‚îÇ  DQX: Row-level anomaly detection                       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                           +\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Unity Catalog Monitoring (Table Health)                ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Schema drift, cardinality changes                    ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Column statistics, metadata tracking                 ‚îÇ\n",
        "‚îÇ  UC: Table-level anomaly detection                      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- üí° **Anomaly detection finds issues you didn't know to look for**\n",
        "- üí° **Complements (doesn't replace) rule-based checks - use both!**\n",
        "- üí° **Unity Catalog monitors tables, DQX monitors individual records**\n",
        "- üí° **Together, they provide comprehensive quality coverage**\n",
        "\n",
        "Let's see how easy it is to add anomaly detection to your pipeline! üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1: Setup & Data Generation\n",
        "\n",
        "First, let's set up our environment and create simple sales transaction data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from databricks.labs.dqx.anomaly import AnomalyEngine, has_no_anomalies\n",
        "from databricks.labs.dqx.engine import DQEngine\n",
        "from databricks.labs.dqx.rule import DQDatasetRule\n",
        "from databricks.labs.dqx.check_funcs import is_not_null, is_in_range\n",
        "from databricks.sdk import WorkspaceClient\n",
        "\n",
        "# Initialize\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "ws = WorkspaceClient()\n",
        "anomaly_engine = AnomalyEngine(ws)\n",
        "dq_engine = DQEngine(ws)\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "print(f\"   Spark version: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate simple sales transaction data\n",
        "def generate_sales_data(num_rows=1000, anomaly_rate=0.04):\n",
        "    \"\"\"\n",
        "    Generate sales transaction data with injected anomalies.\n",
        "    \n",
        "    Normal patterns:\n",
        "    - Amount: $10-500 per transaction\n",
        "    - Quantity: 1-10 items\n",
        "    - Business hours: 9am-6pm weekdays\n",
        "    - Regional consistency\n",
        "    \n",
        "    Anomalies (4%):\n",
        "    - Pricing errors (extremely high/low amounts)\n",
        "    - Quantity spikes (bulk orders 50-100 items)\n",
        "    - Timing anomalies (3am transactions, weekend B2B)\n",
        "    - Regional outliers (unusual amounts for region)\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    categories = [\"Electronics\", \"Clothing\", \"Food\", \"Books\", \"Home\"]\n",
        "    regions = [\"North\", \"South\", \"East\", \"West\"]\n",
        "    \n",
        "    # Regional pricing patterns (normal baseline)\n",
        "    region_patterns = {\n",
        "        \"North\": {\"base_amount\": 200, \"quantity\": 5},\n",
        "        \"South\": {\"base_amount\": 150, \"quantity\": 4},\n",
        "        \"East\": {\"base_amount\": 180, \"quantity\": 4},\n",
        "        \"West\": {\"base_amount\": 220, \"quantity\": 6},\n",
        "    }\n",
        "    \n",
        "    start_date = datetime(2024, 1, 1, 9, 0)  # Jan 1, 2024, 9am\n",
        "    \n",
        "    for i in range(num_rows):\n",
        "        transaction_id = f\"TXN{i:06d}\"\n",
        "        category = random.choice(categories)\n",
        "        region = random.choice(regions)\n",
        "        pattern = region_patterns[region]\n",
        "        \n",
        "        # Generate timestamp (mostly business hours weekdays)\n",
        "        days_offset = random.randint(0, 90)  # 3 months of data\n",
        "        hours_offset = random.randint(0, 9)  # 9am-6pm = 9 hours\n",
        "        date = start_date + timedelta(days=days_offset, hours=hours_offset)\n",
        "        \n",
        "        # Skip weekends for normal transactions\n",
        "        if date.weekday() >= 5:  # Saturday=5, Sunday=6\n",
        "            date = date - timedelta(days=date.weekday() - 4)  # Move to Friday\n",
        "        \n",
        "        # Inject anomalies\n",
        "        if random.random() < anomaly_rate:\n",
        "            anomaly_type = random.choice([\"pricing\", \"quantity\", \"timing\", \"regional\"])\n",
        "            \n",
        "            if anomaly_type == \"pricing\":\n",
        "                # Pricing error: extreme amounts\n",
        "                amount = round(random.choice([pattern[\"base_amount\"] * 10, pattern[\"base_amount\"] / 10]), 2)\n",
        "                quantity = int(np.random.normal(pattern[\"quantity\"], 1))\n",
        "            \n",
        "            elif anomaly_type == \"quantity\":\n",
        "                # Bulk order spike\n",
        "                amount = round(pattern[\"base_amount\"] * random.uniform(0.9, 1.1), 2)\n",
        "                quantity = random.randint(50, 100)  # 10-20x normal\n",
        "            \n",
        "            elif anomaly_type == \"timing\":\n",
        "                # Off-hours or weekend transaction\n",
        "                amount = round(pattern[\"base_amount\"] * random.uniform(0.9, 1.1), 2)\n",
        "                quantity = int(np.random.normal(pattern[\"quantity\"], 1))\n",
        "                date = date.replace(hour=random.choice([2, 3, 4, 22, 23]))  # Late night/early morning\n",
        "                # Or make it weekend\n",
        "                if random.random() > 0.5:\n",
        "                    date = date + timedelta(days=(5 - date.weekday()))  # Move to Saturday\n",
        "            \n",
        "            else:  # regional outlier\n",
        "                # Amount unusual for this region (but normal for another)\n",
        "                other_region = random.choice([r for r in regions if r != region])\n",
        "                amount = round(region_patterns[other_region][\"base_amount\"] * random.uniform(0.9, 1.1), 2)\n",
        "                quantity = int(np.random.normal(pattern[\"quantity\"], 1))\n",
        "        \n",
        "        else:\n",
        "            # Normal transaction\n",
        "            amount = round(pattern[\"base_amount\"] * random.uniform(0.7, 1.3), 2)\n",
        "            quantity = max(1, int(np.random.normal(pattern[\"quantity\"], 2)))\n",
        "        \n",
        "        # Ensure valid ranges\n",
        "        amount = max(10, min(10000, amount))\n",
        "        quantity = max(1, min(100, quantity))\n",
        "        \n",
        "        data.append((transaction_id, date, amount, quantity, category, region))\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Generate data\n",
        "print(\"üîÑ Generating sales transaction data...\\n\")\n",
        "sales_data = generate_sales_data(num_rows=1000, anomaly_rate=0.04)\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"transaction_id\", StringType(), False),\n",
        "    StructField(\"date\", TimestampType(), False),\n",
        "    StructField(\"amount\", DoubleType(), False),\n",
        "    StructField(\"quantity\", IntegerType(), False),\n",
        "    StructField(\"category\", StringType(), False),\n",
        "    StructField(\"region\", StringType(), False),\n",
        "])\n",
        "\n",
        "df_sales = spark.createDataFrame(sales_data, schema)\n",
        "\n",
        "print(\"üìä Sample of sales transactions:\")\n",
        "display(df_sales.orderBy(\"date\"))\n",
        "\n",
        "print(f\"\\n‚úÖ Generated {df_sales.count()} sales transactions\")\n",
        "print(f\"   Expected anomalies: ~{int(df_sales.count() * 0.04)} (4%)\")\n",
        "print(f\"\\nüí° Data includes:\")\n",
        "print(f\"   ‚Ä¢ Normal patterns: Business hours, typical amounts, reasonable quantities\")\n",
        "print(f\"   ‚Ä¢ Injected anomalies: Pricing errors, bulk orders, off-hours, regional outliers\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to table for later reference\n",
        "catalog = spark.sql(\"SELECT current_catalog()\").first()[0]\n",
        "schema_name = \"dqx_demo\"\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema_name}\")\n",
        "\n",
        "table_name = f\"{catalog}.{schema_name}.sales_transactions\"\n",
        "df_sales.write.mode(\"overwrite\").saveAsTable(table_name)\n",
        "\n",
        "print(f\"‚úÖ Data saved to: {table_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define unique registry table for this demo (to avoid conflicts with other demos)\n",
        "registry_table = f\"{catalog}.{schema_name}.anomaly_model_registry_101\"\n",
        "print(f\"üìã Model registry: {registry_table}\")\n",
        "\n",
        "# Clean up old table if it exists (ensures new nested schema)\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {registry_table}\")\n",
        "print(f\"üóëÔ∏è  Cleaned up old registry table (if existed)\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2: Quick Start - Auto-Discovery\n",
        "\n",
        "Let's detect anomalies with **ZERO configuration**. The system will:\n",
        "1. Automatically select relevant columns\n",
        "2. Auto-detect if segmentation is needed (e.g., separate models per region)\n",
        "3. Train model(s) on normal patterns\n",
        "4. Score all transactions for anomalies\n",
        "\n",
        "**You provide**: Just the data  \n",
        "**DQX provides**: Everything else!\n",
        "\n",
        "**Note**: The system may auto-create segmented models if it detects distinct groups in your data. You'll see this in the registry table below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Train model with auto-discovery (zero config!)\n",
        "print(\"üéØ Training anomaly detection model...\")\n",
        "print(\"   (Auto-discovering columns, segments, and patterns)\\n\")\n",
        "\n",
        "model_uri_auto = anomaly_engine.train(\n",
        "    df=spark.table(table_name),\n",
        "    model_name=\"sales_auto\",\n",
        "    registry_table=registry_table\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model trained successfully!\")\n",
        "print(f\"   Model URI: {model_uri_auto}\")\n",
        "\n",
        "# Show the registry table to see what was created\n",
        "print(f\"\\nüìã Model Registry Contents:\")\n",
        "print(f\"   Explore the registry table to see all trained models and their configurations:\\n\")\n",
        "\n",
        "display(\n",
        "    spark.table(registry_table)\n",
        "    .filter(F.col(\"model_name\").startswith(\"sales_auto\"))\n",
        "    .select(\n",
        "        \"model_name\",\n",
        "        \"columns\", \n",
        "        \"segment_by\",\n",
        "        \"segment_values\",\n",
        "        \"training_rows\",\n",
        "        \"training_time\",\n",
        "        \"status\"\n",
        "    )\n",
        "    .orderBy(\"model_name\")\n",
        ")\n",
        "\n",
        "print(\"\\nüí° What you're seeing:\")\n",
        "print(\"   ‚Ä¢ If segment_by is populated, DQX auto-created separate models per segment\")\n",
        "print(\"   ‚Ä¢ Each row is a trained model (global or segment-specific)\")\n",
        "print(\"   ‚Ä¢ Status 'active' means this model is used for scoring\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Score transactions for anomalies\n",
        "print(\"üîç Scoring transactions for anomalies...\\n\")\n",
        "\n",
        "checks_auto = [\n",
        "    DQDatasetRule(\n",
        "        check_func=has_no_anomalies,\n",
        "        check_func_kwargs={\n",
        "            \"merge_columns\": [\"transaction_id\"],\n",
        "            \"model\": \"sales_auto\",\n",
        "            \"registry_table\": registry_table\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "df_scored = dq_engine.apply_checks(df_sales, checks_auto)\n",
        "\n",
        "# Filter to anomalies (score >= 0.5 is considered anomalous)\n",
        "anomalies = df_scored.filter(F.col(\"_info.anomaly.score\") >= 0.5)\n",
        "\n",
        "print(f\"‚ö†Ô∏è  Found {anomalies.count()} anomalies out of {df_scored.count()} transactions\")\n",
        "print(f\"   Detection rate: {(anomalies.count() / df_scored.count()) * 100:.1f}%\")\n",
        "print(f\"\\nüîù Top 10 anomalies (by score):\\n\")\n",
        "\n",
        "display(anomalies.orderBy(F.col(\"_info.anomaly.score\").desc()).select(\n",
        "    \"transaction_id\", \"date\", \"amount\", \"quantity\", \"category\", \"region\",\n",
        "    F.round(\"_info.anomaly.score\", 3).alias(\"anomaly_score\")\n",
        ").limit(10))\n",
        "\n",
        "print(\"\\nüí° Key Point: Anomaly score ranges from 0 to 1\")\n",
        "print(\"   ‚Ä¢ Score >= 0.5: Considered anomalous (flagged)\")\n",
        "print(\"   ‚Ä¢ Score < 0.5: Normal transaction\")\n",
        "print(\"   ‚Ä¢ Higher score = more unusual\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3: Understanding the Results\n",
        "\n",
        "Let's dig deeper into what we found and how to interpret anomaly scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze score distribution\n",
        "print(\"üìä Anomaly Score Distribution:\\n\")\n",
        "\n",
        "score_stats = df_scored.select(\"_info.anomaly.score\").describe()\n",
        "display(score_stats)\n",
        "\n",
        "# Show score ranges\n",
        "print(\"üìà Score Range Breakdown:\\n\")\n",
        "\n",
        "score_ranges = df_scored.select(\n",
        "    F.count(F.when(F.col(\"_info.anomaly.score\") < 0.3, 1)).alias(\"normal_0.0_0.3\"),\n",
        "    F.count(F.when((F.col(\"_info.anomaly.score\") >= 0.3) & (F.col(\"_info.anomaly.score\") < 0.5), 1)).alias(\"borderline_0.3_0.5\"),\n",
        "    F.count(F.when((F.col(\"_info.anomaly.score\") >= 0.5) & (F.col(\"_info.anomaly.score\") < 0.7), 1)).alias(\"anomalous_0.5_0.7\"),\n",
        "    F.count(F.when(F.col(\"_info.anomaly.score\") >= 0.7, 1)).alias(\"highly_anomalous_0.7_1.0\"),\n",
        ").first()\n",
        "\n",
        "total = df_scored.count()\n",
        "print(f\"Normal (0.0-0.3):           {score_ranges['normal_0.0_0.3']:4d} ({score_ranges['normal_0.0_0.3']/total*100:5.1f}%)\")\n",
        "print(f\"Borderline (0.3-0.5):       {score_ranges['borderline_0.3_0.5']:4d} ({score_ranges['borderline_0.3_0.5']/total*100:5.1f}%)\")\n",
        "print(f\"Anomalous (0.5-0.7):        {score_ranges['anomalous_0.5_0.7']:4d} ({score_ranges['anomalous_0.5_0.7']/total*100:5.1f}%)\")\n",
        "print(f\"Highly Anomalous (0.7-1.0): {score_ranges['highly_anomalous_0.7_1.0']:4d} ({score_ranges['highly_anomalous_0.7_1.0']/total*100:5.1f}%)\")\n",
        "\n",
        "print(f\"\\nüí° Interpretation:\")\n",
        "print(f\"   ‚Ä¢ Most transactions score low (normal behavior)\")\n",
        "print(f\"   ‚Ä¢ Threshold of 0.5 separates normal from anomalous\")\n",
        "print(f\"   ‚Ä¢ You can adjust this threshold based on your needs!\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare normal vs anomalous transactions\n",
        "print(\"üîç Normal vs Anomalous Transaction Comparison:\\n\")\n",
        "\n",
        "normal_stats = df_scored.filter(F.col(\"_info.anomaly.score\") < 0.5).agg(\n",
        "    F.avg(\"amount\").alias(\"avg_amount\"),\n",
        "    F.avg(\"quantity\").alias(\"avg_quantity\"),\n",
        "    F.count(\"*\").alias(\"count\")\n",
        ").first()\n",
        "\n",
        "anomaly_stats = df_scored.filter(F.col(\"_info.anomaly.score\") >= 0.5).agg(\n",
        "    F.avg(\"amount\").alias(\"avg_amount\"),\n",
        "    F.avg(\"quantity\").alias(\"avg_quantity\"),\n",
        "    F.count(\"*\").alias(\"count\")\n",
        ").first()\n",
        "\n",
        "print(\"Normal Transactions:\")\n",
        "print(f\"   Count: {normal_stats['count']}\")\n",
        "print(f\"   Avg Amount: ${normal_stats['avg_amount']:.2f}\")\n",
        "print(f\"   Avg Quantity: {normal_stats['avg_quantity']:.1f}\")\n",
        "\n",
        "print(\"\\nAnomalous Transactions:\")\n",
        "print(f\"   Count: {anomaly_stats['count']}\")\n",
        "print(f\"   Avg Amount: ${anomaly_stats['avg_amount']:.2f}\")\n",
        "print(f\"   Avg Quantity: {anomaly_stats['avg_quantity']:.1f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Anomalies have different patterns - mission accomplished!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4: Tuning the Threshold\n",
        "\n",
        "The threshold (default 0.5) controls how sensitive anomaly detection is:\n",
        "- **Lower threshold** (e.g., 0.3): More sensitive, flags more anomalies (higher recall)\n",
        "- **Higher threshold** (e.g., 0.7): Less sensitive, flags only severe anomalies (higher precision)\n",
        "\n",
        "Let's see how changing the threshold affects results!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try different thresholds\n",
        "print(\"üéöÔ∏è  Testing Different Thresholds:\\n\")\n",
        "print(\"Threshold | Anomalies | % of Data | Interpretation\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "total_count = df_scored.count()\n",
        "\n",
        "for threshold in thresholds:\n",
        "    anomaly_count = df_scored.filter(F.col(\"_info.anomaly.score\") >= threshold).count()\n",
        "    percentage = (anomaly_count / total_count) * 100\n",
        "    \n",
        "    if threshold <= 0.3:\n",
        "        interpretation = \"Very sensitive (many alerts)\"\n",
        "    elif threshold <= 0.5:\n",
        "        interpretation = \"Balanced (recommended start)\"\n",
        "    elif threshold <= 0.7:\n",
        "        interpretation = \"Conservative (fewer alerts)\"\n",
        "    else:\n",
        "        interpretation = \"Very strict (critical only)\"\n",
        "    \n",
        "    print(f\"   {threshold:.1f}   |   {anomaly_count:4d}    |  {percentage:5.1f}%  | {interpretation}\")\n",
        "\n",
        "print(\"\\nüí° How to Choose Your Threshold:\")\n",
        "print(\"   ‚Ä¢ Start with 0.5 (balanced)\")\n",
        "print(\"   ‚Ä¢ Too many false positives? ‚Üí Increase threshold (0.6, 0.7)\")\n",
        "print(\"   ‚Ä¢ Missing real issues? ‚Üí Decrease threshold (0.4, 0.3)\")\n",
        "print(\"   ‚Ä¢ Adjust based on investigation capacity and tolerance for risk\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's look at borderline cases\n",
        "print(\"üîç Examining Borderline Cases (scores 0.45-0.55):\\n\")\n",
        "\n",
        "borderline = df_scored.filter(\n",
        "    (F.col(\"_info.anomaly.score\") >= 0.45) & \n",
        "    (F.col(\"_info.anomaly.score\") <= 0.55)\n",
        ").orderBy(F.col(\"_info.anomaly.score\").desc())\n",
        "\n",
        "print(f\"Found {borderline.count()} borderline transactions:\\n\")\n",
        "display(borderline.select(\n",
        "    \"transaction_id\", \"amount\", \"quantity\", \"category\", \"region\",\n",
        "    F.round(\"_info.anomaly.score\", 3).alias(\"score\")\n",
        ").limit(10))\n",
        "\n",
        "print(\"\\nüí° These are on the edge - slight threshold changes will include/exclude them\")\n",
        "print(\"   Review these to calibrate your threshold for your use case\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 5: Manual Column Selection\n",
        "\n",
        "Auto-discovery is great for exploration, but for production you might want explicit control.\n",
        "\n",
        "Let's train a model with **manually selected columns**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with manual column selection\n",
        "print(\"üéØ Training model with manual column selection...\\n\")\n",
        "\n",
        "model_uri_manual = anomaly_engine.train(\n",
        "    df=spark.table(table_name),\n",
        "    columns=[\"amount\", \"quantity\", \"date\"],  # Explicitly specify columns\n",
        "    model_name=\"sales_manual\",\n",
        "    registry_table=registry_table\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Manual model trained!\")\n",
        "print(f\"   Model URI: {model_uri_manual}\")\n",
        "\n",
        "# Compare auto vs manual in the registry\n",
        "print(f\"\\nüìä Auto vs Manual Comparison:\")\n",
        "print(f\"   View both models side-by-side in the registry:\\n\")\n",
        "\n",
        "display(\n",
        "    spark.table(registry_table)\n",
        "    .filter(\n",
        "        (F.col(\"model_name\") == \"sales_auto\") | \n",
        "        (F.col(\"model_name\") == \"sales_manual\")\n",
        "    )\n",
        "    .select(\n",
        "        \"model_name\",\n",
        "        \"columns\",\n",
        "        \"segment_by\",\n",
        "        \"training_rows\",\n",
        "        \"status\"\n",
        "    )\n",
        "    .orderBy(\"model_name\", \"training_time\")\n",
        ")\n",
        "\n",
        "print(f\"\\nüí° Key Differences:\")\n",
        "print(f\"   ‚Ä¢ Auto model: Discovered columns automatically + may have segmentation\")\n",
        "print(f\"   ‚Ä¢ Manual model: You explicitly chose 3 columns (amount, quantity, date)\")\n",
        "print(f\"\\nüí° When to use each approach:\")\n",
        "print(f\"   ‚Ä¢ Auto-discovery: Exploration, quick start, don't know what matters\")\n",
        "print(f\"   ‚Ä¢ Manual selection: Production, control features, domain knowledge\")\n",
        "print(f\"   ‚Ä¢ Both are valid! Start with auto, refine with manual\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score with manual model\n",
        "print(\"üîç Scoring with manual model...\\n\")\n",
        "\n",
        "checks_manual = [\n",
        "    DQDatasetRule(\n",
        "        check_func=has_no_anomalies,\n",
        "        check_func_kwargs={\n",
        "            \"merge_columns\": [\"transaction_id\"],\n",
        "            \"model\": \"sales_manual\",\n",
        "            \"score_threshold\": 0.5,\n",
        "            \"registry_table\": registry_table\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "df_scored_manual = dq_engine.apply_checks(df_sales, checks_manual)\n",
        "anomalies_manual = df_scored_manual.filter(F.col(\"_info.anomaly.score\") >= 0.5)\n",
        "\n",
        "print(f\"‚ö†Ô∏è  Manual model found {anomalies_manual.count()} anomalies\")\n",
        "print(f\"   (Auto model found {anomalies.count()} anomalies)\")\n",
        "print(f\"\\nüîù Top 5 anomalies from manual model:\\n\")\n",
        "\n",
        "display(anomalies_manual.orderBy(F.col(\"_info.anomaly.score\").desc()).select(\n",
        "    \"transaction_id\", \"amount\", \"quantity\", \"date\",\n",
        "    F.round(\"_info.anomaly.score\", 3).alias(\"score\")\n",
        ").limit(5))\n",
        "\n",
        "print(\"\\nüí° Results may differ slightly because we're using different features\")\n",
        "print(\"   This is normal and expected!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6: Why Is This Anomalous?\n",
        "\n",
        "Finding anomalies is great, but **understanding WHY** they're anomalous is crucial for investigation!\n",
        "\n",
        "Let's add **feature contributions** to see which columns drove each anomaly score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score with feature contributions\n",
        "print(\"üîç Scoring with feature contributions (explainability)...\\n\")\n",
        "\n",
        "checks_with_contrib = [\n",
        "    DQDatasetRule(\n",
        "        check_func=has_no_anomalies,\n",
        "        check_func_kwargs={\n",
        "            \"merge_columns\": [\"transaction_id\"],\n",
        "            \"model\": \"sales_manual\",\n",
        "            \"score_threshold\": 0.5,\n",
        "            \"include_contributions\": True,  # Add this to get explanations!\n",
        "            \"registry_table\": registry_table\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "df_with_contrib = dq_engine.apply_checks(df_sales, checks_with_contrib)\n",
        "\n",
        "print(\"‚úÖ Scored with feature contributions!\")\n",
        "print(\"\\nüéØ Top Anomalies with Explanations:\\n\")\n",
        "\n",
        "anomalies_explained = df_with_contrib.filter(\n",
        "    F.col(\"_info.anomaly.score\") >= 0.5\n",
        ").orderBy(F.col(\"_info.anomaly.score\").desc()).limit(5)\n",
        "\n",
        "display(anomalies_explained.select(\n",
        "    \"transaction_id\",\n",
        "    \"amount\",\n",
        "    \"quantity\",\n",
        "    F.date_format(\"date\", \"yyyy-MM-dd HH:mm\").alias(\"date\"),\n",
        "    F.round(\"_info.anomaly.score\", 3).alias(\"score\"),\n",
        "    F.col(\"_info.anomaly.contributions\").alias(\"contributions\")\n",
        "))\n",
        "\n",
        "print(\"\\nüí° How to Read Contributions:\")\n",
        "print(\"   ‚Ä¢ Contributions show which features made this transaction unusual\")\n",
        "print(\"   ‚Ä¢ Higher contribution = that feature is more responsible for the anomaly\")\n",
        "print(\"   ‚Ä¢ Use this to triage and investigate efficiently!\")\n",
        "print(\"\\n   Example: If 'amount' has high contribution ‚Üí pricing issue\")\n",
        "print(\"            If 'quantity' has high contribution ‚Üí bulk order anomaly\")\n",
        "print(\"            If 'date' has high contribution ‚Üí timing anomaly\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show one detailed example\n",
        "print(\"üîé Detailed Example - Top Anomaly:\\n\")\n",
        "\n",
        "top_anomaly = anomalies_explained.first()\n",
        "\n",
        "print(f\"Transaction ID: {top_anomaly['transaction_id']}\")\n",
        "print(f\"Anomaly Score: {top_anomaly['score']:.3f}\")\n",
        "print(f\"\\nTransaction Details:\")\n",
        "print(f\"   Amount: ${top_anomaly['amount']:.2f}\")\n",
        "print(f\"   Quantity: {top_anomaly['quantity']}\")\n",
        "print(f\"   Date: {top_anomaly['date']}\")\n",
        "print(f\"\\nFeature Contributions:\")\n",
        "\n",
        "contributions = top_anomaly['contributions']\n",
        "if contributions:\n",
        "    # Sort by contribution value\n",
        "    sorted_contrib = sorted(contributions.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "    for feature, value in sorted_contrib[:3]:  # Top 3\n",
        "        print(f\"   {feature}: {abs(value)*100:.1f}% contribution\")\n",
        "    \n",
        "    print(f\"\\nüéØ Investigation Tip:\")\n",
        "    top_feature = sorted_contrib[0][0]\n",
        "    if \"amount\" in top_feature:\n",
        "        print(f\"   ‚Üí Check for pricing errors or incorrect price feeds\")\n",
        "    elif \"quantity\" in top_feature:\n",
        "        print(f\"   ‚Üí Investigate bulk order or inventory issue\")\n",
        "    elif \"date\" in top_feature or \"hour\" in top_feature:\n",
        "        print(f\"   ‚Üí Review transaction timing - off-hours activity?\")\n",
        "else:\n",
        "    print(\"   (No detailed contributions available)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 7: Production Integration\n",
        "\n",
        "Now that you understand anomaly detection, let's see how to use it in production workflows.\n",
        "\n",
        "Two common patterns:\n",
        "1. **Quarantine anomalies** for review\n",
        "2. **Combine with traditional DQ checks** for comprehensive quality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pattern 1: Quarantine anomalies\n",
        "print(\"üì¶ Pattern 1: Quarantine Anomalies for Investigation\\n\")\n",
        "\n",
        "# Filter anomalies and save to quarantine table\n",
        "quarantine_table = f\"{catalog}.{schema_name}.sales_anomalies_quarantine\"\n",
        "\n",
        "anomalies_to_quarantine = df_with_contrib.filter(\n",
        "    F.col(\"_info.anomaly.score\") >= 0.5\n",
        ").select(\n",
        "    \"*\",\n",
        "    F.current_timestamp().alias(\"quarantine_timestamp\"),\n",
        "    F.lit(\"anomaly_detected\").alias(\"quarantine_reason\")\n",
        ")\n",
        "\n",
        "anomalies_to_quarantine.write.mode(\"overwrite\").saveAsTable(quarantine_table)\n",
        "\n",
        "print(f\"‚úÖ Quarantined {anomalies_to_quarantine.count()} anomalies to: {quarantine_table}\")\n",
        "print(f\"\\nüí° Use Case:\")\n",
        "print(f\"   ‚Ä¢ Automatically route unusual transactions for manual review\")\n",
        "print(f\"   ‚Ä¢ Prevent bad data from reaching downstream systems\")\n",
        "print(f\"   ‚Ä¢ Build investigation workflow around quarantine table\")\n",
        "print(f\"\\nüìã Access quarantined records:\")\n",
        "print(f\"   spark.table('{quarantine_table}')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pattern 2: Combine with traditional DQ checks\n",
        "print(\"üîÑ Pattern 2: Combine Rule-Based + ML Anomaly Detection\\n\")\n",
        "\n",
        "# Comprehensive quality checks\n",
        "checks_combined = [\n",
        "    # Traditional rule-based checks (known unknowns)\n",
        "    is_not_null(columns=[\"transaction_id\", \"amount\", \"quantity\"]),\n",
        "    is_in_range(column=\"amount\", min_value=0, max_value=100000),\n",
        "    is_in_range(column=\"quantity\", min_value=1, max_value=1000),\n",
        "    \n",
        "    # ML-based anomaly detection (unknown unknowns)\n",
        "    DQDatasetRule(\n",
        "        check_func=has_no_anomalies,\n",
        "        check_func_kwargs={\n",
        "            \"merge_columns\": [\"transaction_id\"],\n",
        "            \"model\": \"sales_manual\",\n",
        "            \"score_threshold\": 0.5,\n",
        "            \"include_contributions\": True,\n",
        "            \"registry_table\": registry_table\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "# Apply all checks in one pass\n",
        "df_full_quality = dq_engine.apply_checks(df_sales, checks_combined)\n",
        "\n",
        "print(\"‚úÖ Applied all quality checks (rule-based + ML)!\")\n",
        "print(f\"\\nüìä Quality Summary:\")\n",
        "\n",
        "total = df_full_quality.count()\n",
        "anomalies_found = df_full_quality.filter(F.col(\"_info.anomaly.score\") >= 0.5).count()\n",
        "clean_records = total - anomalies_found\n",
        "\n",
        "print(f\"   Total records: {total}\")\n",
        "print(f\"   Clean records: {clean_records} ({clean_records/total*100:.1f}%)\")\n",
        "print(f\"   Anomalies: {anomalies_found} ({anomalies_found/total*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüí° Best Practice:\")\n",
        "print(f\"   ‚úÖ Use rule-based checks for known issues (nulls, ranges, formats)\")\n",
        "print(f\"   ‚úÖ Use anomaly detection for unknown patterns\")\n",
        "print(f\"   ‚úÖ Apply both together for comprehensive quality coverage!\")\n",
        "\n",
        "# Show combined results\n",
        "print(f\"\\nüîù Records with issues (either rule violations or anomalies):\\n\")\n",
        "issues = df_full_quality.filter(\n",
        "    (F.col(\"_info.anomaly.score\") >= 0.5) |\n",
        "    (F.size(F.col(\"_info.failed_checks\")) > 0)\n",
        ")\n",
        "\n",
        "if issues.count() > 0:\n",
        "    display(issues.select(\n",
        "        \"transaction_id\", \"amount\", \"quantity\",\n",
        "        F.round(\"_info.anomaly.score\", 3).alias(\"anomaly_score\"),\n",
        "        F.size(\"_info.failed_checks\").alias(\"rule_violations\")\n",
        "    ).limit(10))\n",
        "else:\n",
        "    print(\"   No issues found! All data passed quality checks.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary & Next Steps\n",
        "\n",
        "### üéì What You Learned\n",
        "\n",
        "1. **‚úÖ Anomaly Detection Concepts**\n",
        "   - Known unknowns (rule-based checks) vs Unknown unknowns (ML anomaly detection)\n",
        "   - Unity Catalog monitors tables, DQX monitors individual records\n",
        "   - Use both approaches together for comprehensive quality\n",
        "\n",
        "2. **‚úÖ Zero-Config Quick Start**\n",
        "   - Train models with auto-discovery (no column selection needed)\n",
        "   - Score data with one function call\n",
        "   - Detect unusual patterns automatically\n",
        "\n",
        "3. **‚úÖ Interpret Results**\n",
        "   - Anomaly scores range 0-1 (0.5 threshold)\n",
        "   - Adjust threshold based on precision/recall needs\n",
        "   - Compare normal vs anomalous patterns\n",
        "\n",
        "4. **‚úÖ Control & Tune**\n",
        "   - Manual column selection for production\n",
        "   - Threshold tuning for sensitivity\n",
        "   - Feature contributions for investigation\n",
        "\n",
        "5. **‚úÖ Production Integration**\n",
        "   - Quarantine workflow for anomalies\n",
        "   - Combine with traditional DQ checks\n",
        "   - Easy integration with existing pipelines\n",
        "\n",
        "### üí° Key Takeaways\n",
        "\n",
        "- **Start simple**: Use auto-discovery first, then refine with manual selection\n",
        "- **Threshold matters**: Adjust based on your tolerance for false positives\n",
        "- **Contributions are crucial**: Use them to triage and investigate efficiently\n",
        "- **Complement, don't replace**: Use both rule-based checks and anomaly detection\n",
        "- **Unity Catalog + DQX**: Together they provide comprehensive data quality coverage\n",
        "\n",
        "### üöÄ Next Steps\n",
        "\n",
        "#### 1. Apply to Your Data\n",
        "```python\n",
        "# Replace with your table\n",
        "model = anomaly_engine.train(\n",
        "    df=spark.table(\"your_catalog.your_schema.your_table\"),\n",
        "    model_name=\"your_model_name\"\n",
        ")\n",
        "\n",
        "checks = [\n",
        "    has_no_anomalies(\n",
        "        merge_columns=[\"your_id_column\"],\n",
        "        model=\"your_model_name\"\n",
        "    )\n",
        "]\n",
        "df_scored = dq_engine.apply_checks(your_df, checks)\n",
        "```\n",
        "\n",
        "#### 2. Explore Advanced Features\n",
        "- **Segmented models**: Train separate models per region, category, etc.\n",
        "- **Drift detection**: Monitor when models become stale\n",
        "- **Ensemble models**: Get confidence intervals on scores\n",
        "- See the pharma and investment banking demos for examples!\n",
        "\n",
        "#### 3. Set Up Production Workflows\n",
        "- Automate model training (weekly/monthly)\n",
        "- Schedule scoring (hourly/daily)\n",
        "- Build investigation workflow around quarantine table\n",
        "- Integrate with alerting (Slack, PagerDuty, etc.)\n",
        "\n",
        "#### 4. Monitor & Iterate\n",
        "- Review flagged anomalies regularly\n",
        "- Adjust thresholds based on false positive rate\n",
        "- Retrain models as patterns change\n",
        "- Combine with Unity Catalog's table-level monitoring\n",
        "\n",
        "### üìö Resources\n",
        "\n",
        "- [DQX Anomaly Detection Documentation](https://databrickslabs.github.io/dqx/guide/anomaly_detection)\n",
        "- [API Reference](https://databrickslabs.github.io/dqx/reference/quality_checks#has_no_anomalies)\n",
        "- [Unity Catalog Anomaly Detection](https://docs.databricks.com/aws/en/data-quality-monitoring/anomaly-detection/#-table-quality-details)\n",
        "- [GitHub Repository](https://github.com/databrickslabs/dqx)\n",
        "\n",
        "### üéâ You're Ready!\n",
        "\n",
        "You now understand:\n",
        "- ‚úÖ What anomaly detection is and when to use it\n",
        "- ‚úÖ How to implement it with minimal configuration\n",
        "- ‚úÖ How to interpret and tune results\n",
        "- ‚úÖ How to integrate it into production\n",
        "\n",
        "**Start detecting anomalies in your data today!** üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "*Questions? Feedback? Open an issue on [GitHub](https://github.com/databrickslabs/dqx) or contact the DQX team!*\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}