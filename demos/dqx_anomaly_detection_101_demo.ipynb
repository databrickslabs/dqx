{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Simple Anomaly Detection Demo\n",
        "\n",
        "## Learn Anomaly Detection in 15 Minutes\n",
        "\n",
        "This beginner-friendly demo shows you how to:\n",
        "- Understand what anomaly detection is and why it matters\n",
        "- Detect unusual patterns in your data with zero configuration\n",
        "- Tune detection sensitivity to your needs\n",
        "- Understand why specific records are flagged\n",
        "- Integrate anomaly detection into production workflows\n",
        "\n",
        "**Dataset**: Simple sales transactions (universally relatable, no domain expertise required)\n",
        "\n",
        "**Time**: 12-17 minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 0: What is Anomaly Detection in Data Quality?\n",
        "\n",
        "Before we dive into the code, let's understand what anomaly detection is and why it's valuable.\n",
        "\n",
        "### The Data Quality Challenge: Known vs Unknown Issues\n",
        "\n",
        "#### üéØ Known Unknowns (Traditional Data Quality)\n",
        "\n",
        "These are issues **you can anticipate** and write rules for:\n",
        "\n",
        "| Issue Type | Example | Solution |\n",
        "|------------|---------|----------|\n",
        "| Null values | `amount` is NULL | `is_not_null(column=\"amount\")` |\n",
        "| Out of range | Price is negative | `is_in_range(column=\"price\", min=0)` |\n",
        "| Invalid format | Email without @ symbol | Regex validation |\n",
        "\n",
        "**Works great when you know what to look for!**\n",
        "\n",
        "#### üîç Unknown Unknowns (Anomaly Detection)\n",
        "\n",
        "These are issues **you DON'T know to look for**:\n",
        "\n",
        "- Unusual **patterns** across multiple columns\n",
        "- Outlier **combinations** that are individually valid\n",
        "- Subtle **data corruption** that passes all rules\n",
        "\n",
        "**Problem**: You can't write rules for things you haven't thought of!\n",
        "\n",
        "**Solution**: ML-based anomaly detection learns \"normal\" patterns from your data and flags deviations.\n",
        "\n",
        "### Concrete Example\n",
        "\n",
        "```\n",
        "Known Unknown:  \"Amount must be positive\"\n",
        "                ‚Üí is_in_range(min=0)\n",
        "                ‚úÖ Catches: amount = -50\n",
        "\n",
        "Unknown Unknown: \"Transaction for $47,283 at 3am on Sunday for 2 items\"\n",
        "                 ‚Üí Anomaly detection\n",
        "                 ‚úÖ Catches: All fields valid individually, but pattern is unusual\n",
        "```\n",
        "\n",
        "### Why Anomaly Detection Matters\n",
        "\n",
        "- ‚úÖ **Catches issues before they become problems** - Early warning system\n",
        "- ‚úÖ **No need to anticipate every failure mode** - Adapts to your data\n",
        "- ‚úÖ **Learns patterns automatically** - No manual rule writing\n",
        "- ‚úÖ **Complements rule-based checks** - Use both together for comprehensive quality\n",
        "\n",
        "### Unity Catalog Integration\n",
        "\n",
        "#### Built-in Quality Monitoring (Unity Catalog)\n",
        "\n",
        "Unity Catalog includes **table-level** anomaly detection:\n",
        "- Monitors column statistics and distributions\n",
        "- Alerts on schema changes, cardinality shifts\n",
        "- Tracks null rate changes over time\n",
        "- Great for monitoring table health\n",
        "\n",
        "#### When to Use DQX Anomaly Detection\n",
        "\n",
        "DQX provides **row-level** anomaly detection:\n",
        "- Detect unusual individual **records/transactions**\n",
        "- **Multi-column pattern** detection (e.g., price + quantity + time)\n",
        "- **Custom models per segment** (e.g., different regions, categories)\n",
        "- **Feature contributions** to understand WHY records are anomalous\n",
        "- **Integration** with existing DQX quality pipelines\n",
        "\n",
        "#### Complementary Approach\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Rule-Based Checks (Known Unknowns)                     ‚îÇ\n",
        "‚îÇ  ‚Ä¢ is_not_null, is_in_range, regex validation          ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Schema validation, referential integrity             ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                           +\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  ML Anomaly Detection (Unknown Unknowns)                ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Pattern detection, outlier identification            ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Multi-column relationship validation                 ‚îÇ\n",
        "‚îÇ  DQX: Row-level anomaly detection                       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                           +\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Unity Catalog Monitoring (Table Health)                ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Schema drift, cardinality changes                    ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Column statistics, metadata tracking                 ‚îÇ\n",
        "‚îÇ  UC: Table freshness and completeness                   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- üí° **Anomaly detection finds issues you didn't know to look for**\n",
        "- üí° **Complements (doesn't replace) rule-based checks - use both!**\n",
        "- üí° **Unity Catalog monitors table freshness and completeness, DQX monitors data inside the tables**\n",
        "- üí° **Together, they provide comprehensive quality coverage**\n",
        "\n",
        "Let's see how easy it is to add anomaly detection to your pipeline! üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Prerequisites: Install DQX with Anomaly Support\n",
        "\n",
        "Before running this demo, install DQX with anomaly detection extras:\n",
        "\n",
        "```python\n",
        "%pip install 'databricks-labs-dqx[anomaly]'\n",
        "dbutils.library.restartPython()\n",
        "```\n",
        "\n",
        "**What's included in `[anomaly]` extras:**\n",
        "- `scikit-learn` - Machine learning algorithms (Isolation Forest)\n",
        "- `mlflow` - Model tracking and registry\n",
        "- `shap` - Feature contributions for explainability\n",
        "- `cloudpickle` - Model serialization\n",
        "\n",
        "**Note**: On ML Runtimes and Serverless compute, most dependencies are already pre-installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTIONAL: Install DQX with anomaly extras\n",
        "# Uncomment and run if you haven't installed DQX yet\n",
        "\n",
        "# %pip install 'databricks-labs-dqx[anomaly]'\n",
        "# dbutils.library.restartPython()\n",
        "\n",
        "# Configure widgets for catalog and schema\n",
        "dbutils.widgets.text(\"demo_catalog\", \"main\", \"Catalog Name\")\n",
        "dbutils.widgets.text(\"demo_schema\", \"dqx_demo\", \"Schema Name\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Configuration: Catalog and Schema\n",
        "\n",
        "Configure where to store demo data and models. By default, uses `main` catalog and `dqx_demo` schema.\n",
        "You can change these using the widgets above if needed.\n",
        "\n",
        "---\n",
        "\n",
        "## Section 1: Setup & Data Generation\n",
        "\n",
        "First, let's set up our environment and create simple sales transaction data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from databricks.labs.dqx.anomaly import AnomalyEngine, has_no_anomalies\n",
        "from databricks.labs.dqx.engine import DQEngine\n",
        "from databricks.labs.dqx.rule import DQDatasetRule, DQRowRule\n",
        "from databricks.labs.dqx.check_funcs import is_not_null, is_in_range\n",
        "from databricks.sdk import WorkspaceClient\n",
        "\n",
        "# Initialize\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "ws = WorkspaceClient()\n",
        "anomaly_engine = AnomalyEngine(ws)\n",
        "dq_engine = DQEngine(ws)\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "print(f\"   Spark version: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate simple sales transaction data\n",
        "def generate_sales_data(num_rows=1000, anomaly_rate=0.05):\n",
        "    \"\"\"\n",
        "    Generate sales transaction data with injected anomalies.\n",
        "    \n",
        "    Normal patterns:\n",
        "    - Amount: $10-500 per transaction\n",
        "    - Quantity: 1-10 items\n",
        "    - Business hours: 9am-6pm weekdays\n",
        "    - Regional consistency\n",
        "    \n",
        "    Anomalies (5% - matches default expected_anomaly_rate):\n",
        "    - Pricing errors (VERY extreme: 40-50x or 1/40 of normal amounts)\n",
        "    - Quantity spikes (bulk orders 100-150 items = 20-30x normal)\n",
        "    - Timing anomalies (off-hours + 5-8x amount + 25-40 quantity)\n",
        "    - Multi-factor (6-10x amount + 35-60 quantity + always off-hours)\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    categories = [\"Electronics\", \"Clothing\", \"Food\", \"Books\", \"Home\"]\n",
        "    regions = [\"North\", \"South\", \"East\", \"West\"]\n",
        "    \n",
        "    # Regional pricing patterns (normal baseline)\n",
        "    region_patterns = {\n",
        "        \"North\": {\"base_amount\": 200, \"quantity\": 5},\n",
        "        \"South\": {\"base_amount\": 150, \"quantity\": 4},\n",
        "        \"East\": {\"base_amount\": 180, \"quantity\": 4},\n",
        "        \"West\": {\"base_amount\": 220, \"quantity\": 6},\n",
        "    }\n",
        "    \n",
        "    start_date = datetime(2024, 1, 1, 9, 0)  # Jan 1, 2024, 9am\n",
        "    \n",
        "    for i in range(num_rows):\n",
        "        transaction_id = f\"TXN{i:06d}\"\n",
        "        category = random.choice(categories)\n",
        "        region = random.choice(regions)\n",
        "        pattern = region_patterns[region]\n",
        "        \n",
        "        # Generate timestamp (mostly business hours weekdays)\n",
        "        days_offset = random.randint(0, 90)  # 3 months of data\n",
        "        hours_offset = random.randint(0, 9)  # 9am-6pm = 9 hours\n",
        "        date = start_date + timedelta(days=days_offset, hours=hours_offset)\n",
        "        \n",
        "        # Skip weekends for normal transactions\n",
        "        if date.weekday() >= 5:  # Saturday=5, Sunday=6\n",
        "            date = date - timedelta(days=date.weekday() - 4)  # Move to Friday\n",
        "        \n",
        "        # Inject anomalies (VERY extreme to reliably exceed 0.60 threshold)\n",
        "        if random.random() < anomaly_rate:\n",
        "            # Bias towards more extreme anomaly types for better detection\n",
        "            anomaly_type = random.choices(\n",
        "                [\"pricing\", \"quantity\", \"timing\", \"multi_factor\"],\n",
        "                weights=[2, 2, 1, 3]  # Favor pricing, quantity, and multi-factor\n",
        "            )[0]\n",
        "            \n",
        "            if anomaly_type == \"pricing\":\n",
        "                # Pricing error: VERY extreme amounts (40-50x or 1/40 of normal)\n",
        "                multiplier = random.choice([random.uniform(40, 50), 1.0 / random.uniform(35, 45)])\n",
        "                amount = round(pattern[\"base_amount\"] * multiplier, 2)\n",
        "                quantity = int(np.random.normal(pattern[\"quantity\"], 0.3))  # Near-normal quantity\n",
        "            \n",
        "            elif anomaly_type == \"quantity\":\n",
        "                # Bulk order spike (100-150 items = 20-30x normal) \n",
        "                amount = round(pattern[\"base_amount\"] * random.uniform(0.95, 1.05), 2)  # Normal amount\n",
        "                quantity = random.randint(100, 150)\n",
        "            \n",
        "            elif anomaly_type == \"timing\":\n",
        "                # Off-hours transaction WITH very unusual amount (multi-factor)\n",
        "                amount = round(pattern[\"base_amount\"] * random.uniform(5.0, 8.0), 2)  # 5-8x normal\n",
        "                quantity = random.randint(25, 40)  # 5-8x normal\n",
        "                date = date.replace(hour=random.choice([2, 3, 4, 22, 23]))  # Late night/early morning\n",
        "                # Or make it weekend\n",
        "                if random.random() > 0.5:\n",
        "                    date = date + timedelta(days=(5 - date.weekday()))  # Move to Saturday\n",
        "            \n",
        "            else:  # multi-factor: EXTREME multi-dimensional anomaly\n",
        "                # Extreme regional mismatch + very unusual quantity + off-hours\n",
        "                other_region = random.choice([r for r in regions if r != region])\n",
        "                amount = round(region_patterns[other_region][\"base_amount\"] * random.uniform(6.0, 10.0), 2)\n",
        "                quantity = random.randint(35, 60)  # 7-12x normal\n",
        "                date = date.replace(hour=random.choice([2, 3, 4, 22, 23]))  # Always off-hours\n",
        "        \n",
        "        else:\n",
        "            # Normal transaction (tighter variance for more consistent patterns)\n",
        "            amount = round(pattern[\"base_amount\"] * random.uniform(0.85, 1.15), 2)\n",
        "            quantity = max(1, int(np.random.normal(pattern[\"quantity\"], 1)))\n",
        "        \n",
        "        # Ensure valid ranges\n",
        "        amount = max(10, min(10000, amount))\n",
        "        quantity = max(1, min(150, quantity))  # Allow bulk orders up to 150\n",
        "        \n",
        "        data.append((transaction_id, date, amount, quantity, category, region))\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Generate data\n",
        "print(\"üîÑ Generating sales transaction data...\\n\")\n",
        "sales_data = generate_sales_data(num_rows=1000, anomaly_rate=0.05)\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"transaction_id\", StringType(), False),\n",
        "    StructField(\"date\", TimestampType(), False),\n",
        "    StructField(\"amount\", DoubleType(), False),\n",
        "    StructField(\"quantity\", IntegerType(), False),\n",
        "    StructField(\"category\", StringType(), False),\n",
        "    StructField(\"region\", StringType(), False),\n",
        "])\n",
        "\n",
        "df_sales = spark.createDataFrame(sales_data, schema)\n",
        "\n",
        "print(\"üìä Sample of sales transactions:\")\n",
        "display(df_sales.orderBy(\"date\"))\n",
        "\n",
        "print(f\"\\n‚úÖ Generated {df_sales.count()} sales transactions\")\n",
        "print(f\"   Expected anomalies: ~{int(df_sales.count() * 0.05)} (5%)\")\n",
        "print(f\"\\nüí° Data includes:\")\n",
        "print(f\"   ‚Ä¢ Normal patterns: Business hours, typical amounts (170-230), reasonable quantities (4-6)\")\n",
        "print(f\"   ‚Ä¢ Injected anomalies: VERY extreme deviations (40-50x pricing, 100-150 quantity, multi-factor)\")\n",
        "print(f\"\\nüéØ Anomaly detection will identify patterns that deviate significantly from normal behavior\")\n",
        "print(f\"\\nüìå Note: 5% anomaly rate matches the model's default 'expected_anomaly_rate' parameter\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get catalog and schema from widgets\n",
        "catalog = dbutils.widgets.get(\"demo_catalog\")\n",
        "schema_name = dbutils.widgets.get(\"demo_schema\")\n",
        "\n",
        "print(f\"üìÇ Using catalog: {catalog}\")\n",
        "print(f\"üìÇ Using schema: {schema_name}\\n\")\n",
        "\n",
        "# Create schema if it doesn't exist\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema_name}\")\n",
        "\n",
        "# Save data to table\n",
        "table_name = f\"{catalog}.{schema_name}.sales_transactions\"\n",
        "df_sales.write.mode(\"overwrite\").saveAsTable(table_name)\n",
        "\n",
        "print(f\"‚úÖ Data saved to: {table_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up registry table for tracking trained models\n",
        "registry_table = f\"{catalog}.{schema_name}.anomaly_model_registry_101\"\n",
        "print(f\"üìã Model registry table: {registry_table}\")\n",
        "\n",
        "# Clean up any existing registry from previous runs\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {registry_table}\")\n",
        "print(f\"‚úÖ Registry ready for new models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2: Combined Quality Checks - Rule-Based + ML Anomaly Detection\n",
        "\n",
        "Let's build a comprehensive quality pipeline that combines:\n",
        "1. **Rule-based checks** (known unknowns): nulls, ranges, formats\n",
        "2. **ML anomaly detection** (unknown unknowns): unusual patterns\n",
        "\n",
        "With **ZERO configuration**, the system will:\n",
        "- Automatically select relevant columns for anomaly detection\n",
        "- Auto-detect if segmentation is needed (e.g., separate models per region)\n",
        "- Train ensemble models (2 models by default for robustness)\n",
        "- Score all transactions with both rule-based AND anomaly checks\n",
        "- Provide feature contributions to explain WHY records are flagged\n",
        "\n",
        "**You provide**: Just the data and rules  \n",
        "**DQX provides**: Everything else, optimized for performance!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train anomaly detection model with zero configuration\n",
        "print(\"üéØ Training anomaly detection model...\")\n",
        "print(\"   DQX will automatically discover patterns in your data\\n\")\n",
        "\n",
        "model_uri_auto = anomaly_engine.train(\n",
        "    df=spark.table(table_name),\n",
        "    model_name=\"sales_auto\",\n",
        "    registry_table=registry_table\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model trained successfully!\")\n",
        "print(f\"   Model URI: {model_uri_auto}\")\n",
        "\n",
        "# View what DQX created for you\n",
        "print(f\"\\nüìã Trained Models:\\n\")\n",
        "\n",
        "display(\n",
        "    spark.table(registry_table)\n",
        "    .filter(F.col(\"identity.model_name\").startswith(\"sales_auto\"))\n",
        "    .select(\n",
        "        \"identity.model_name\",\n",
        "        \"training.columns\", \n",
        "        \"segmentation.segment_by\",\n",
        "        \"segmentation.segment_values\",\n",
        "        \"training.training_rows\",\n",
        "        \"training.training_time\",\n",
        "        \"identity.status\"\n",
        "    )\n",
        "    .orderBy(\"identity.model_name\")\n",
        ")\n",
        "\n",
        "print(\"\\nüí° Understanding the Results:\")\n",
        "print(\"   ‚Ä¢ DQX automatically found patterns in your data\")\n",
        "print(\"   ‚Ä¢ If 'segment_by' has values, DQX created separate models for different groups\")\n",
        "print(\"   ‚Ä¢ Each row is a trained model ready to score new data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Viewing Models in Databricks UI\n",
        "\n",
        "Your trained models are automatically registered in **Unity Catalog Model Registry**. Here's how to view them:\n",
        "\n",
        "**Option 1: Catalog Explorer**\n",
        "1. Click **Catalog** in the left sidebar\n",
        "2. Navigate to your catalog ‚Üí schema\n",
        "3. Look for models named `sales_auto` (or `sales_auto_ensemble_0`, `sales_auto_ensemble_1` for ensemble models)\n",
        "4. Click on a model to see:\n",
        "   - Model versions\n",
        "   - MLflow run details (parameters, metrics)\n",
        "   - Model lineage and schema\n",
        "\n",
        "**Option 2: MLflow Experiments**\n",
        "1. Click **Experiments** in the left sidebar\n",
        "2. Find your notebook's experiment (automatically created per notebook)\n",
        "3. View all training runs with:\n",
        "   - Hyperparameters (contamination, num_trees, etc.)\n",
        "   - Validation metrics (precision, recall, F1)\n",
        "   - Model artifacts and signatures\n",
        "\n",
        "**What DQX Logs Automatically:**\n",
        "- ‚úÖ **Parameters**: contamination, num_trees, subsampling_rate, random_seed\n",
        "- ‚úÖ **Metrics**: precision, recall, F1 score, validation accuracy\n",
        "- ‚úÖ **Model Signature**: Input/output schemas for Unity Catalog\n",
        "- ‚úÖ **Model Artifacts**: Serialized sklearn model + feature metadata\n",
        "\n",
        "**Model URI Format:**\n",
        "```\n",
        "models:/<catalog>.<schema>.<model_name>/<version>\n",
        "```\n",
        "Example: `models:/main.dqx_demo.sales_auto/1`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    # Apply quality checks: combine rule-based + ML anomaly detection\n",
        "    print(\"üîç Applying quality checks to all transactions...\\n\")\n",
        "\n",
        "    # Define all quality checks\n",
        "    checks_combined = [\n",
        "        # Rule-based checks for known issues\n",
        "        DQRowRule(check_func=is_not_null, check_func_kwargs={\"column\": \"transaction_id\"}),\n",
        "        DQRowRule(check_func=is_not_null, check_func_kwargs={\"column\": \"amount\"}),\n",
        "        DQRowRule(check_func=is_in_range, check_func_kwargs={\"column\": \"amount\", \"min_limit\": 0, \"max_limit\": 100000}),\n",
        "        DQRowRule(check_func=is_in_range, check_func_kwargs={\"column\": \"quantity\", \"min_limit\": 1, \"max_limit\": 1000}),\n",
        "        \n",
        "        # ML anomaly detection for unusual patterns\n",
        "        DQDatasetRule(\n",
        "            check_func=has_no_anomalies,\n",
        "            check_func_kwargs={\n",
        "                \"merge_columns\": [\"transaction_id\"],\n",
        "                \"model\": \"sales_auto\",\n",
        "                \"registry_table\": registry_table\n",
        "                # Default: 2 models for confidence, explains why data is anomalous, threshold 0.60\n",
        "            }\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    df_scored = dq_engine.apply_checks(df_sales, checks_combined)\n",
        "\n",
        "    # Get records flagged as anomalies\n",
        "    anomalies = df_scored.filter(F.size(F.col(\"_errors\")) > 0)\n",
        "\n",
        "    print(f\"‚úÖ Quality checks complete!\")\n",
        "    print(f\"\\nüìä Results:\")\n",
        "    print(f\"   Total transactions: {df_scored.count()}\")\n",
        "    print(f\"   Anomalies found: {anomalies.count()} ({(anomalies.count() / df_scored.count()) * 100:.1f}%)\")\n",
        "    print(f\"\\nüîù Top 10 anomalies:\\n\")\n",
        "\n",
        "    display(anomalies.orderBy(F.col(\"_info.anomaly.score\").desc()).select(\n",
        "        \"transaction_id\", \"date\", \"amount\", \"quantity\", \"category\", \"region\",\n",
        "        F.round(\"_info.anomaly.score\", 3).alias(\"anomaly_score\"),\n",
        "        F.col(\"_info.anomaly.contributions\").alias(\"why_anomalous\")\n",
        "    ).limit(10))\n",
        "\n",
        "    print(\"\\nüí° What Just Happened:\")\n",
        "    print(\"   ‚Ä¢ Rule-based checks caught known issues (nulls, out-of-range values)\")\n",
        "    print(\"   ‚Ä¢ Anomaly detection found unusual patterns you didn't explicitly define\")\n",
        "    print(\"   ‚Ä¢ The 'why_anomalous' column explains what made each record unusual\")\n",
        "    print(\"   ‚Ä¢ Threshold of 0.60 balances finding issues vs false alarms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3: Understanding Your Results\n",
        "\n",
        "Let's explore the anomalies we found and learn how to interpret anomaly scores.\n",
        "\n",
        "**What you'll learn:**\n",
        "- How anomaly scores work (0 to 1 scale, based on Isolation Forest)\n",
        "- What makes a score \"high\" vs \"normal\"  \n",
        "- Why certain records were flagged as unusual\n",
        "\n",
        "**Important**: Anomaly scores are NOT probabilities or confidence levels! They measure how \"easy\" it is to separate a record from the rest of your data. Think of it as: \"How different is this record from normal patterns?\"\n",
        "\n",
        "### üìä How Isolation Forest Works\n",
        "\n",
        "The algorithm builds decision trees and measures how many \"splits\" are needed to isolate each record:\n",
        "\n",
        "- **Anomalies** (shown in the image diagram): Isolated near the top with few splits ‚Üí High score\n",
        "- **Normal data**: Requires many splits deep in the tree ‚Üí Low score\n",
        "\n",
        "*[Image: Add Isolation Forest visualization here showing anomaly isolation vs normal data]*\n",
        "\n",
        "This is why the score represents \"isolation ease\" rather than statistical confidence!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze score distribution\n",
        "print(\"üìä Anomaly Score Distribution:\\n\")\n",
        "\n",
        "score_stats = df_scored.select(\"_info.anomaly.score\").describe()\n",
        "display(score_stats)\n",
        "\n",
        "# Show score ranges (aligned with 0.60 threshold)\n",
        "print(\"üìà Score Range Breakdown:\\n\")\n",
        "\n",
        "score_ranges = df_scored.select(\n",
        "    F.count(F.when(F.col(\"_info.anomaly.score\") < 0.4, 1)).alias(\"normal_0.0_0.4\"),\n",
        "    F.count(F.when((F.col(\"_info.anomaly.score\") >= 0.4) & (F.col(\"_info.anomaly.score\") < 0.6), 1)).alias(\"borderline_0.4_0.6\"),\n",
        "    F.count(F.when((F.col(\"_info.anomaly.score\") >= 0.6) & (F.col(\"_info.anomaly.score\") < 0.75), 1)).alias(\"flagged_0.6_0.75\"),\n",
        "    F.count(F.when(F.col(\"_info.anomaly.score\") >= 0.75, 1)).alias(\"highly_anomalous_0.75_1.0\"),\n",
        ").first()\n",
        "\n",
        "total = df_scored.count()\n",
        "print(f\"Normal (0.0-0.4):             {score_ranges['normal_0.0_0.4']:4d} ({score_ranges['normal_0.0_0.4']/total*100:5.1f}%) ‚Üê Not flagged\")\n",
        "print(f\"Borderline (0.4-0.6):         {score_ranges['borderline_0.4_0.6']:4d} ({score_ranges['borderline_0.4_0.6']/total*100:5.1f}%) ‚Üê Near threshold (not flagged)\")\n",
        "print(f\"Flagged (0.6-0.75):           {score_ranges['flagged_0.6_0.75']:4d} ({score_ranges['flagged_0.6_0.75']/total*100:5.1f}%) ‚Üê ANOMALIES (flagged)\")\n",
        "print(f\"Highly Anomalous (0.75-1.0):  {score_ranges['highly_anomalous_0.75_1.0']:4d} ({score_ranges['highly_anomalous_0.75_1.0']/total*100:5.1f}%) ‚Üê ANOMALIES (extreme)\")\n",
        "\n",
        "print(f\"\\nüí° What Do These Scores Mean?\")\n",
        "print(f\"   ‚Ä¢ Scores are based on how 'isolated' a record is from normal patterns\")\n",
        "print(f\"   ‚Ä¢ Low scores (0.0-0.4): Blends in with normal data (NOT flagged)\")\n",
        "print(f\"   ‚Ä¢ Borderline (0.4-0.6): Near the 0.60 threshold (NOT flagged)\")\n",
        "print(f\"   ‚Ä¢ High scores (‚â•0.6): Stands out as different (FLAGGED as anomalies)\")\n",
        "print(f\"   ‚Ä¢ This is NOT a probability - it's based on how many 'splits' are needed to isolate the record\")\n",
        "print(f\"   ‚Ä¢ The threshold (0.60) is tuned empirically, not a statistical significance level\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare normal vs anomalous transactions (using 0.60 threshold)\n",
        "print(\"üîç Normal vs Anomalous Transaction Comparison:\\n\")\n",
        "\n",
        "normal_stats = df_scored.filter(F.col(\"_info.anomaly.score\") < 0.6).agg(\n",
        "    F.avg(\"amount\").alias(\"avg_amount\"),\n",
        "    F.avg(\"quantity\").alias(\"avg_quantity\"),\n",
        "    F.count(\"*\").alias(\"count\")\n",
        ").first()\n",
        "\n",
        "anomaly_stats = df_scored.filter(F.col(\"_info.anomaly.score\") >= 0.6).agg(\n",
        "    F.avg(\"amount\").alias(\"avg_amount\"),\n",
        "    F.avg(\"quantity\").alias(\"avg_quantity\"),\n",
        "    F.count(\"*\").alias(\"count\")\n",
        ").first()\n",
        "\n",
        "print(\"Normal Transactions (score < 0.60):\")\n",
        "print(f\"   Count: {normal_stats['count']} ({normal_stats['count']/df_scored.count()*100:.1f}%)\")\n",
        "print(f\"   Avg Amount: ${normal_stats['avg_amount']:.2f}\")\n",
        "print(f\"   Avg Quantity: {normal_stats['avg_quantity']:.1f}\")\n",
        "\n",
        "print(\"\\nFlagged Anomalies (score ‚â• 0.60):\")\n",
        "print(f\"   Count: {anomaly_stats['count']} ({anomaly_stats['count']/df_scored.count()*100:.1f}%)\")\n",
        "print(f\"   Avg Amount: ${anomaly_stats['avg_amount']:.2f}\")\n",
        "print(f\"   Avg Quantity: {anomaly_stats['avg_quantity']:.1f}\")\n",
        "\n",
        "print(\"\\nüí° Expected Results:\")\n",
        "print(\"   ‚Ä¢ Normal transactions should be ~95% of data with typical amounts/quantities\")\n",
        "print(\"   ‚Ä¢ Anomalies should be ~5% with extreme or unusual patterns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4: Tuning the Threshold\n",
        "\n",
        "The threshold controls which records get flagged as anomalies. It's like setting a \"sensitivity dial\":\n",
        "\n",
        "- **Lower threshold** (e.g., 0.4-0.5): More sensitive, flags more records as unusual\n",
        "- **Higher threshold** (e.g., 0.65-0.75): Less sensitive, flags only very unusual records\n",
        "\n",
        "**The default of 0.60** was chosen through testing across various datasets. It balances:\n",
        "- Finding real issues (recall)\n",
        "- Avoiding false alarms (precision)\n",
        "\n",
        "**Remember**: This is NOT a statistical confidence level! It's a cutoff on the \"isolation score\" that determines what's unusual enough to investigate.\n",
        "\n",
        "Let's see how changing the threshold affects results!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try different thresholds\n",
        "print(\"üéöÔ∏è  Testing Different Thresholds:\\n\")\n",
        "print(\"Threshold | Anomalies | % of Data | Interpretation\")\n",
        "print(\"-\" * 78)\n",
        "\n",
        "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "total_count = df_scored.count()\n",
        "\n",
        "for threshold in thresholds:\n",
        "    anomaly_count = df_scored.filter(F.col(\"_info.anomaly.score\") >= threshold).count()\n",
        "    percentage = (anomaly_count / total_count) * 100\n",
        "    \n",
        "    # Interpretation based on actual detection rate (data-driven)\n",
        "    if percentage >= 90:\n",
        "        interpretation = \"Extremely sensitive (flags almost everything)\"\n",
        "    elif percentage >= 50:\n",
        "        interpretation = \"Too sensitive (flags majority of data)\"\n",
        "    elif percentage >= 10:\n",
        "        interpretation = \"Moderate (may need adjustment)\"\n",
        "    elif percentage >= 1:\n",
        "        interpretation = \"Balanced (default 0.60 - good starting point)\"\n",
        "    elif percentage > 0:\n",
        "        interpretation = \"Very strict (only extreme cases)\"\n",
        "    else:\n",
        "        interpretation = \"Too strict (misses all anomalies)\"\n",
        "    \n",
        "    print(f\"   {threshold:.1f}   |   {anomaly_count:4d}    |  {percentage:5.1f}%  | {interpretation}\")\n",
        "\n",
        "print(\"\\nüí° How to Choose Your Threshold:\")\n",
        "print(\"   ‚Ä¢ Start with default 0.60 (typically catches 1-5% of data)\")\n",
        "print(\"   ‚Ä¢ Too many alerts to investigate? ‚Üí Increase to 0.65 or 0.70\")\n",
        "print(\"   ‚Ä¢ Missing real issues? ‚Üí Decrease to 0.50 or 0.55\")\n",
        "print(\"   ‚Ä¢ The 'right' threshold depends on:\")\n",
        "print(\"     - Your investigation capacity (how many alerts can you handle?)\")\n",
        "print(\"     - Your risk tolerance (cost of missing an issue vs false alarm)\")\n",
        "print(\"\\nüí° This table shows how YOUR data responds to different thresholds\")\n",
        "print(\"   Use it to find the sweet spot for your use case!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's look at borderline cases near the 0.60 threshold\n",
        "print(\"üîç Examining Borderline Cases (scores 0.55-0.65):\\n\")\n",
        "\n",
        "borderline = df_scored.filter(\n",
        "    (F.col(\"_info.anomaly.score\") >= 0.55) & \n",
        "    (F.col(\"_info.anomaly.score\") <= 0.65)\n",
        ").orderBy(F.col(\"_info.anomaly.score\").desc())\n",
        "\n",
        "borderline_count = borderline.count()\n",
        "print(f\"Found {borderline_count} borderline transactions near the 0.60 threshold:\\n\")\n",
        "\n",
        "if borderline_count > 0:\n",
        "    display(borderline.select(\n",
        "        \"transaction_id\", \"amount\", \"quantity\", \"category\", \"region\",\n",
        "        F.round(\"_info.anomaly.score\", 3).alias(\"score\")\n",
        "    ).limit(10))\n",
        "    \n",
        "    print(\"\\nüí° These are on the edge - slight threshold changes will include/exclude them\")\n",
        "    print(\"   Review these to calibrate your threshold for your use case\")\n",
        "else:\n",
        "    print(\"   No records in this range - try adjusting the range!\")\n",
        "    print(\"   This suggests a clear separation between normal and anomalous data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 5: Manual Column Selection (Optional - Advanced)\n",
        "\n",
        "**Note**: This section is optional and shows advanced features. Feel free to skip to Section 7 for production patterns!\n",
        "\n",
        "Auto-discovery is great for exploration, but for production you might want explicit control over which features the model uses.\n",
        "\n",
        "Let's train a model with **manually selected columns**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with manual column selection\n",
        "print(\"üéØ Training model with manual column selection...\\n\")\n",
        "\n",
        "model_uri_manual = anomaly_engine.train(\n",
        "    df=spark.table(table_name),\n",
        "    columns=[\"amount\", \"quantity\", \"date\"],  # Explicitly specify which columns to use\n",
        "    model_name=\"sales_manual\",\n",
        "    registry_table=registry_table\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Manual model trained!\")\n",
        "print(f\"   Model URI: {model_uri_manual}\")\n",
        "\n",
        "# Compare auto vs manual in the registry\n",
        "print(f\"\\nüìä Auto vs Manual Comparison:\")\n",
        "print(f\"   View both models side-by-side in the registry:\\n\")\n",
        "\n",
        "display(\n",
        "    spark.table(registry_table)\n",
        "    .filter(\n",
        "        (F.col(\"identity.model_name\") == \"sales_auto\") | \n",
        "        (F.col(\"identity.model_name\") == \"sales_manual\")\n",
        "    )\n",
        "    .select(\n",
        "        \"identity.model_name\",\n",
        "        \"training.columns\",\n",
        "        \"segmentation.segment_by\",\n",
        "        \"training.training_rows\",\n",
        "        \"identity.status\"\n",
        "    )\n",
        "    .orderBy(\"identity.model_name\", \"training.training_time\")\n",
        ")\n",
        "\n",
        "print(f\"\\nüí° Key Differences:\")\n",
        "print(f\"   ‚Ä¢ Auto model: Discovered columns automatically + may have segmentation\")\n",
        "print(f\"   ‚Ä¢ Manual model: You explicitly chose 3 columns (amount, quantity, date)\")\n",
        "print(f\"\\nüí° When to use each approach:\")\n",
        "print(f\"   ‚Ä¢ Auto-discovery: Exploration, quick start, don't know what matters\")\n",
        "print(f\"   ‚Ä¢ Manual selection: Production, control features, domain knowledge\")\n",
        "print(f\"   ‚Ä¢ Both are valid! Start with auto, refine with manual\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score with manual model\n",
        "print(\"üîç Scoring with manual model...\\n\")\n",
        "\n",
        "checks_manual = [\n",
        "    DQDatasetRule(\n",
        "        check_func=has_no_anomalies,\n",
        "        check_func_kwargs={\n",
        "            \"merge_columns\": [\"transaction_id\"],\n",
        "            \"model\": \"sales_manual\",\n",
        "            \"score_threshold\": 0.5,\n",
        "            \"registry_table\": registry_table\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "df_scored_manual = dq_engine.apply_checks(df_sales, checks_manual)\n",
        "# Filter by _errors column (standard DQX pattern) to get flagged anomalies\n",
        "anomalies_manual = df_scored_manual.filter(F.size(F.col(\"_errors\")) > 0)\n",
        "\n",
        "print(f\"‚ö†Ô∏è  Manual model found {anomalies_manual.count()} anomalies\")\n",
        "print(f\"   (Auto model found {anomalies.count()} anomalies)\")\n",
        "print(f\"\\nüîù Top 5 anomalies from manual model:\\n\")\n",
        "\n",
        "display(anomalies_manual.orderBy(F.col(\"_info.anomaly.score\").desc()).select(\n",
        "    \"transaction_id\", \"amount\", \"quantity\", \"date\",\n",
        "    F.round(\"_info.anomaly.score\", 3).alias(\"score\")\n",
        ").limit(5))\n",
        "\n",
        "print(\"\\nüí° Results may differ slightly because we're using different features\")\n",
        "print(\"   This is normal and expected!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6: Deep Dive - Feature Contributions (Optional - Advanced)\n",
        "\n",
        "**Note**: This section shows detailed analysis of contributions. Skip to Section 7 for production patterns!\n",
        "\n",
        "**Reminder**: Contributions are now enabled by default (you already saw them in Section 2), but this section shows how to analyze them in depth.\n",
        "\n",
        "Finding anomalies is great, but **understanding WHY** they're anomalous is crucial for investigation. Feature contributions show which columns drove each anomaly score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score with feature contributions\n",
        "print(\"üîç Scoring with feature contributions (explainability)...\\n\")\n",
        "\n",
        "checks_with_contrib = [\n",
        "    DQDatasetRule(\n",
        "        check_func=has_no_anomalies,\n",
        "        check_func_kwargs={\n",
        "            \"merge_columns\": [\"transaction_id\"],\n",
        "            \"model\": \"sales_manual\",\n",
        "            \"score_threshold\": 0.5,\n",
        "            \"include_contributions\": True,  # Add this to get explanations!\n",
        "            \"registry_table\": registry_table\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "df_with_contrib = dq_engine.apply_checks(df_sales, checks_with_contrib)\n",
        "\n",
        "print(\"‚úÖ Scored with feature contributions!\")\n",
        "print(\"\\nüéØ Top Anomalies with Explanations:\\n\")\n",
        "\n",
        "# Filter by _errors column (standard DQX pattern) to get flagged anomalies\n",
        "anomalies_explained = df_with_contrib.filter(\n",
        "    F.size(F.col(\"_errors\")) > 0\n",
        ").orderBy(F.col(\"_info.anomaly.score\").desc()).limit(5)\n",
        "\n",
        "display(anomalies_explained.select(\n",
        "    \"transaction_id\",\n",
        "    \"amount\",\n",
        "    \"quantity\",\n",
        "    F.date_format(\"date\", \"yyyy-MM-dd HH:mm\").alias(\"date\"),\n",
        "    F.round(\"_info.anomaly.score\", 3).alias(\"score\"),\n",
        "    F.col(\"_info.anomaly.contributions\").alias(\"contributions\")\n",
        "))\n",
        "\n",
        "print(\"\\nüí° How to Read Contributions:\")\n",
        "print(\"   ‚Ä¢ Contributions show which features made this transaction unusual\")\n",
        "print(\"   ‚Ä¢ Higher contribution = that feature is more responsible for the anomaly\")\n",
        "print(\"   ‚Ä¢ Use this to triage and investigate efficiently!\")\n",
        "print(\"\\n   Example: If 'amount' has high contribution ‚Üí pricing issue\")\n",
        "print(\"            If 'quantity' has high contribution ‚Üí bulk order anomaly\")\n",
        "print(\"            If 'date' has high contribution ‚Üí timing anomaly\")\n",
        "print(\"\\nüí° About Category Contributions:\")\n",
        "print(\"   ‚Ä¢ You'll see contributions from ALL category features (one-hot encoded)\")\n",
        "print(\"   ‚Ä¢ Non-matching categories (e.g., 'category_Electronics' when item is Clothing)\")\n",
        "print(\"     show small contributions representing the feature's ABSENCE\")\n",
        "print(\"   ‚Ä¢ Focus on features with >5% contribution for investigation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show one detailed example\n",
        "print(\"üîé Detailed Example - Top Anomaly:\\n\")\n",
        "\n",
        "# Extract flat columns for easier access\n",
        "anomalies_flattened = anomalies_explained.select(\n",
        "    \"transaction_id\",\n",
        "    \"amount\",\n",
        "    \"quantity\",\n",
        "    \"date\",\n",
        "    F.col(\"_info.anomaly.score\").alias(\"score\"),\n",
        "    F.col(\"_info.anomaly.contributions\").alias(\"contributions\")\n",
        ")\n",
        "\n",
        "top_anomaly = anomalies_flattened.first()\n",
        "\n",
        "print(f\"Transaction ID: {top_anomaly['transaction_id']}\")\n",
        "print(f\"Anomaly Score: {top_anomaly['score']:.3f}\")\n",
        "print(f\"\\nTransaction Details:\")\n",
        "print(f\"   Amount: ${top_anomaly['amount']:.2f}\")\n",
        "print(f\"   Quantity: {top_anomaly['quantity']}\")\n",
        "print(f\"   Date: {top_anomaly['date']}\")\n",
        "print(f\"\\nFeature Contributions:\")\n",
        "\n",
        "contributions = top_anomaly['contributions']\n",
        "if contributions:\n",
        "    # Sort by contribution value\n",
        "    sorted_contrib = sorted(contributions.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "    for feature, value in sorted_contrib[:3]:  # Top 3\n",
        "        print(f\"   {feature}: {abs(value)*100:.1f}% contribution\")\n",
        "    \n",
        "    print(f\"\\nüéØ Investigation Tip:\")\n",
        "    top_feature = sorted_contrib[0][0]\n",
        "    if \"amount\" in top_feature:\n",
        "        print(f\"   ‚Üí Check for pricing errors or incorrect price feeds\")\n",
        "    elif \"quantity\" in top_feature:\n",
        "        print(f\"   ‚Üí Investigate bulk order or inventory issue\")\n",
        "    elif \"date\" in top_feature or \"hour\" in top_feature:\n",
        "        print(f\"   ‚Üí Review transaction timing - off-hours activity?\")\n",
        "else:\n",
        "    print(\"   (No detailed contributions available)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 7: Using in Production\n",
        "\n",
        "Ready to use anomaly detection in real pipelines? This section shows you how.\n",
        "\n",
        "**What you'll learn:**\n",
        "- How to automatically separate good data from bad data\n",
        "- How to route anomalies to a quarantine table for investigation\n",
        "- Best practices for production data quality pipelines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Automatically separate clean data from anomalies\n",
        "print(\"üì¶ Separating clean data from anomalies...\\n\")\n",
        "\n",
        "# Split data into good and bad records\n",
        "good_df, bad_df = dq_engine.apply_checks_and_split(df_sales, checks_combined)\n",
        "\n",
        "# Save quarantined records (anomalies + rule violations)\n",
        "quarantine_table = f\"{catalog}.{schema_name}.sales_anomalies_quarantine\"\n",
        "bad_df_with_metadata = bad_df.select(\n",
        "    \"*\",\n",
        "    F.current_timestamp().alias(\"quarantine_timestamp\"),\n",
        "    F.lit(\"quality_check_failed\").alias(\"quarantine_reason\")\n",
        ")\n",
        "bad_df_with_metadata.write.mode(\"overwrite\").saveAsTable(quarantine_table)\n",
        "\n",
        "print(f\"‚úÖ Automatically split data using apply_checks_and_split():\")\n",
        "print(f\"   Clean records: {good_df.count()}\")\n",
        "print(f\"   Quarantined: {bad_df.count()}\")\n",
        "print(f\"\\nüí° Benefits of apply_checks_and_split():\")\n",
        "print(f\"   ‚Ä¢ Automatically routes failed checks to quarantine\")\n",
        "print(f\"   ‚Ä¢ Handles both anomalies AND rule violations\")\n",
        "print(f\"   ‚Ä¢ No manual filtering needed - just specify the checks!\")\n",
        "print(f\"\\nüìã Access quarantined records:\")\n",
        "print(f\"   spark.table('{quarantine_table}')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pattern 2: Use in downstream pipelines\n",
        "print(\"üîÑ Pattern 2: Integrate with Downstream Pipelines\\n\")\n",
        "\n",
        "# Use the clean data (good_df) for downstream processing\n",
        "print(\"üí° Best Practices:\")\n",
        "print(\"   ‚úÖ Use good_df for downstream analytics, ML training, reporting\")\n",
        "print(\"   ‚úÖ Route bad_df to investigation/remediation workflows\")\n",
        "print(\"   ‚úÖ Monitor quarantine table for trends and retraining signals\")\n",
        "print(\"   ‚úÖ Combine rule-based + anomaly checks (shown in Section 2)\")\n",
        "print(f\"\\nüìä Production Flow:\")\n",
        "print(f\"   1. Apply checks (Section 2) ‚Üí rule-based + anomaly detection\")\n",
        "print(f\"   2. Split data (this section) ‚Üí good vs bad records\")\n",
        "print(f\"   3. Process good_df ‚Üí downstream systems\")\n",
        "print(f\"   4. Investigate bad_df ‚Üí manual review or auto-remediation\")\n",
        "print(f\"\\n‚ú® With new defaults, you get:\")\n",
        "print(f\"   ‚Ä¢ Ensemble models (confidence scores)\")\n",
        "print(f\"   ‚Ä¢ Feature contributions (explainability)\")\n",
        "print(f\"   ‚Ä¢ Optimized performance (10-15x faster than baseline)\")\n",
        "print(f\"   ‚Ä¢ Production-ready out-of-the-box!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary & Next Steps\n",
        "\n",
        "### üéì What You Learned\n",
        "\n",
        "1. **‚úÖ Anomaly Detection Concepts**\n",
        "   - Known unknowns (rule-based checks) vs Unknown unknowns (ML anomaly detection)\n",
        "   - Unity Catalog monitors tables, DQX monitors individual records\n",
        "   - Use both approaches together for comprehensive quality\n",
        "\n",
        "2. **‚úÖ Zero-Config Quick Start**\n",
        "   - Train models with auto-discovery (no column selection needed)\n",
        "   - Score data with one function call\n",
        "   - Detect unusual patterns automatically\n",
        "\n",
        "3. **‚úÖ Interpret Results**\n",
        "   - Anomaly scores range 0-1 (0.5 threshold)\n",
        "   - Adjust threshold based on precision/recall needs\n",
        "   - Compare normal vs anomalous patterns\n",
        "\n",
        "4. **‚úÖ Control & Tune**\n",
        "   - Manual column selection for production\n",
        "   - Threshold tuning for sensitivity\n",
        "   - Feature contributions for investigation\n",
        "\n",
        "5. **‚úÖ Production Integration**\n",
        "   - Quarantine workflow for anomalies\n",
        "   - Combine with traditional DQ checks\n",
        "   - Easy integration with existing pipelines\n",
        "\n",
        "### üí° Key Takeaways\n",
        "\n",
        "- **Start simple**: Use auto-discovery first, then refine with manual selection\n",
        "- **Threshold matters**: Adjust based on your tolerance for false positives\n",
        "- **Contributions are crucial**: Use them to triage and investigate efficiently\n",
        "- **Complement, don't replace**: Use both rule-based checks and anomaly detection\n",
        "- **Unity Catalog + DQX**: Together they provide comprehensive data quality coverage\n",
        "\n",
        "### üöÄ Next Steps\n",
        "\n",
        "#### 1. Apply to Your Data\n",
        "```python\n",
        "# Replace with your table\n",
        "model = anomaly_engine.train(\n",
        "    df=spark.table(\"your_catalog.your_schema.your_table\"),\n",
        "    model_name=\"your_model_name\"\n",
        ")\n",
        "\n",
        "checks = [\n",
        "    has_no_anomalies(\n",
        "        merge_columns=[\"your_id_column\"],\n",
        "        model=\"your_model_name\"\n",
        "    )\n",
        "]\n",
        "df_scored = dq_engine.apply_checks(your_df, checks)\n",
        "```\n",
        "\n",
        "#### 2. Explore Advanced Features\n",
        "- **Segmented models**: Train separate models per region, category, etc.\n",
        "- **Drift detection**: Monitor when models become stale\n",
        "- **Ensemble models**: Get confidence intervals on scores\n",
        "- See the pharma and investment banking demos for examples!\n",
        "\n",
        "#### 3. Set Up Production Workflows\n",
        "- Automate model training (weekly/monthly)\n",
        "- Schedule scoring (hourly/daily)\n",
        "- Build investigation workflow around quarantine table\n",
        "- Integrate with alerting (Slack, PagerDuty, etc.)\n",
        "\n",
        "#### 4. Monitor & Iterate\n",
        "- Review flagged anomalies regularly\n",
        "- Adjust thresholds based on false positive rate\n",
        "- Retrain models as patterns change\n",
        "- Combine with Unity Catalog's table-level monitoring\n",
        "\n",
        "### üìö Resources\n",
        "\n",
        "- [DQX Anomaly Detection Documentation](https://databrickslabs.github.io/dqx/guide/anomaly_detection)\n",
        "- [API Reference](https://databrickslabs.github.io/dqx/reference/quality_checks#has_no_anomalies)\n",
        "- [Unity Catalog Anomaly Detection](https://docs.databricks.com/aws/en/data-quality-monitoring/anomaly-detection/#-table-quality-details)\n",
        "- [GitHub Repository](https://github.com/databrickslabs/dqx)\n",
        "\n",
        "### üéâ You're Ready!\n",
        "\n",
        "You now understand:\n",
        "- ‚úÖ What anomaly detection is and when to use it\n",
        "- ‚úÖ How to implement it with minimal configuration\n",
        "- ‚úÖ How to interpret and tune results\n",
        "- ‚úÖ How to integrate it into production\n",
        "\n",
        "**Start detecting anomalies in your data today!** üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "*Questions? Feedback? Open an issue on [GitHub](https://github.com/databrickslabs/dqx) or contact the DQX team!*\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
