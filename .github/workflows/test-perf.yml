name: perf_test

on:
  pull_request:
    types: [ opened, synchronize, ready_for_review ]
  merge_group:
    types: [ checks_requested ]
  push:
    branches:
      - main

permissions:
  id-token: write
  contents: read
  pull-requests: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    if: github.event_name == 'pull_request'
    environment: tool
    runs-on: larger
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: pip
          cache-dependency-path: '**/pyproject.toml'

      - name: Install hatch
        run: pip install hatch==1.9.4

      - name: Detect existing baseline
        id: detect
        shell: bash
        run: |
          shopt -s nullglob
          files=(tests/perf/.benchmarks/*/*_baseline.json)
          if (( ${#files[@]} == 0 )); then
            echo "missing=true"
            echo "missing=true"  >> $GITHUB_OUTPUT
          else
            echo "missing=false"
            echo "missing=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate initial baseline if missing
        uses: databrickslabs/sandbox/acceptance@acceptance/v0.4.4
        #if: steps.detect.outputs.missing == 'true'
        with:
          vault_uri: ${{ secrets.VAULT_URI }}
          timeout: 2h
          codegen_path: tests/perf/.codegen.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          DATABRICKS_SERVERLESS_COMPUTE_ID: ${{ env.DATABRICKS_SERVERLESS_COMPUTE_ID }}
          #PYTEST_ADDOPTS: "--benchmark-save=baseline"
          PYTEST_ADDOPTS: "--benchmark-json=.benchmarks/baseline.json"

      - name: Print baseline benchmarks if missing
        #if: steps.detect.outputs.missing == 'true'
        run: |
          cat tests/perf/.benchmarks/baseline.json
          # echo "----- Benchmark Files -----"
          # find tests/perf/.benchmarks -type d
          # echo "----- Benchmark Content -----"
          # for file in tests/perf/.benchmarks/*/*_baseline.json; do
          #   echo "----- $file -----"
          #   cat "$file"
          # done

      - name: Generate current perf results and compare against baseline
        uses: databrickslabs/sandbox/acceptance@acceptance/v0.4.4
        with:
          vault_uri: ${{ secrets.VAULT_URI }}
          timeout: 2h
          codegen_path: tests/perf/.codegen.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          DATABRICKS_SERVERLESS_COMPUTE_ID: ${{ env.DATABRICKS_SERVERLESS_COMPUTE_ID }}
          PYTEST_ADDOPTS: "--benchmark-json=.benchmarks/current.json"
          #PYTEST_ADDOPTS: "-v -s --tb=long -o log_cli=true -o log_cli_level=DEBUG --benchmark-save=current --benchmark-compare=1 --benchmark-compare-fail=mean:20%"

      - name: Compare benchmark JSON results
        run: |
          cat tests/perf/.benchmarks/current.json
          hatch run compare_benchmarks

#      - name: Print current benchmarks
#        run: |
#          echo "----- Benchmark Files -----"
#          find tests/perf/.benchmarks
#          echo "----- Benchmark Content -----"
#          for file in tests/perf/.benchmarks/*/*_current.json; do
#            echo "----- $file -----"
#            cat "$file"
#          done
#
#      - name: Compare Benchmark Results
#        run: |
#          echo "Comparing current benchmark results with the saved baseline ..."
#          # allow 20% of performance degradation
#          # compare current run with previous run (benchmark), diff of JSON files
#          hatch run pytest tests/perf --benchmark-compare=1 --benchmark-compare-fail=mean:20%