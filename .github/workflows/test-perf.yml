name: perf_test

on:
  pull_request:
    types: [ opened, synchronize, ready_for_review ]
  merge_group:
    types: [ checks_requested ]
  push:
    branches:
      - main

permissions:
  id-token: write
  contents: write # <-- required for pushing commits
  pull-requests: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    if: github.event_name == 'pull_request'
    environment: tool
    runs-on: larger
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: pip
          cache-dependency-path: '**/pyproject.toml'

      - name: Install hatch
        run: pip install hatch==1.9.4

      # if wanting to recreate baseline, remove tests/perf/.benchmarks folder
      - name: Detect existing benchmark baseline
        id: detect
        shell: bash
        run: |
          shopt -s nullglob
          files=(tests/perf/.benchmarks/*/*_baseline.json)
          if (( ${#files[@]} == 0 )); then
            echo "missing=true"
            echo "missing=true"  >> $GITHUB_OUTPUT
          else
            echo "missing=false"
            echo "missing=false" >> $GITHUB_OUTPUT
          fi

      # create benchmarks under: tests/perf/.benchmarks/*/*_baseline.json
      - name: Generate initial benchmark baseline if missing
        uses: databrickslabs/sandbox/acceptance@acceptance/v0.4.4
        if: steps.detect.outputs.missing == 'true'
        with:
          vault_uri: ${{ secrets.VAULT_URI }}
          timeout: 2h
          codegen_path: tests/perf/.codegen.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          DATABRICKS_SERVERLESS_COMPUTE_ID: ${{ env.DATABRICKS_SERVERLESS_COMPUTE_ID }}
          PYTEST_ADDOPTS: "--benchmark-save=baseline"

      - name: Commit benchmark baseline to repo if missing
        if: steps.detect.outputs.missing == 'true'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "${{ github.actor }}"
          git config user.email "${{ github.actor }}@users.noreply.github.com"
          
          # Make sure we are on the PR branch
          git fetch origin ${{ github.head_ref }}
          git checkout ${{ github.head_ref }}
          
          # Stage & commit changes
          git add tests/perf/.benchmarks/*/*_baseline.json
          git commit -m "Add pytest-benchmark baseline"
          
          # Push back to the same branch using GITHUB_TOKEN
          git remote set-url origin "https://x-access-token:${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY}.git"
          git push origin HEAD:${{ github.head_ref }}

      - name: Generate current benchmark
        uses: databrickslabs/sandbox/acceptance@acceptance/v0.4.4
        with:
          vault_uri: ${{ secrets.VAULT_URI }}
          timeout: 2h
          codegen_path: tests/perf/.codegen.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          DATABRICKS_SERVERLESS_COMPUTE_ID: ${{ env.DATABRICKS_SERVERLESS_COMPUTE_ID }}
          PYTEST_ADDOPTS: "--benchmark-save=current"

      - name: Azure login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.ARM_CLIENT_ID }}
          tenant-id: ${{ secrets.ARM_TENANT_ID }}
          allow-no-subscriptions: true

      - name: Compare Benchmark Results (baseline vs current)
        run: |
          val=$(az keyvault secret show --id "${{ secrets.VAULT_URI }}/secrets/DATABRICKS-HOST" --query value -o tsv)
          echo "DATABRICKS_HOST=$val" >> $GITHUB_ENV
          echo "DATABRICKS_AUTH_TYPE=azure-cli" >> $GITHUB_ENV
          echo "Comparing current benchmark results with the saved baseline ..."
          # allow 20% of performance degradation
          hatch run pytest tests/perf --benchmark-storage=tests/perf/.benchmarks --benchmark-compare=1 --benchmark-compare-fail=mean:20%