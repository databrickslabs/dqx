name: perf_test

on:
  pull_request:
    types: [ opened, synchronize, ready_for_review ]
  merge_group:
    types: [ checks_requested ]
  push:
    branches:
      - main

permissions:
  id-token: write
  contents: write # <-- required for pushing commits
  pull-requests: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    if: github.event_name == 'pull_request'
    environment: tool
    runs-on: larger
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: pip
          cache-dependency-path: '**/pyproject.toml'

      - name: Install hatch
        run: pip install hatch==1.9.4

      # if wanting to recreate baseline, remove tests/perf/.benchmarks folder
      - name: Detect existing baseline
        id: detect
        shell: bash
        run: |
          shopt -s nullglob
          files=(tests/perf/.benchmarks/*/*_baseline.json)
          if (( ${#files[@]} == 0 )); then
            echo "missing=true"
            echo "missing=true"  >> $GITHUB_OUTPUT
          else
            echo "missing=false"
            echo "missing=false" >> $GITHUB_OUTPUT
          fi

      # create benchmarks under: tests/perf/.benchmarks/*/*_baseline.json
      - name: Generate initial baseline if missing
        uses: databrickslabs/sandbox/acceptance@acceptance/v0.4.4
        if: steps.detect.outputs.missing == 'true'
        with:
          vault_uri: ${{ secrets.VAULT_URI }}
          timeout: 2h
          codegen_path: tests/perf/.codegen.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          DATABRICKS_SERVERLESS_COMPUTE_ID: ${{ env.DATABRICKS_SERVERLESS_COMPUTE_ID }}
          PYTEST_ADDOPTS: "--benchmark-save=baseline"

      - name: Commit baseline to repo
        if: steps.detect.outputs.missing == 'true'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "${{ github.actor }}"
          git config user.email "${{ github.actor }}@users.noreply.github.com"
          
          # Make sure we are on the PR branch
          git fetch origin ${{ github.head_ref }}
          git checkout ${{ github.head_ref }}
          
          # Stage & commit changes
          git add tests/perf/.benchmarks/*/*_baseline.json
          git commit -m "Add pytest-benchmark baseline"
          
          # Push back to the same branch using GITHUB_TOKEN
          git remote set-url origin "https://x-access-token:${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY}.git"
          git push origin HEAD:${{ github.head_ref }}

      - name: Generate current perf results and compare against baseline
        uses: databrickslabs/sandbox/acceptance@acceptance/v0.4.4
        with:
          vault_uri: ${{ secrets.VAULT_URI }}
          timeout: 2h
          codegen_path: tests/perf/.codegen.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          DATABRICKS_SERVERLESS_COMPUTE_ID: ${{ env.DATABRICKS_SERVERLESS_COMPUTE_ID }}
          PYTEST_ADDOPTS: "--benchmark-save=current"

      - name: Compare Benchmark Results
        run: |
          echo "Comparing current benchmark results with the saved baseline ..."
          # allow 20% of performance degradation
          hatch run pytest tests/perf --benchmark-compare=1 --benchmark-compare-fail=mean:20%