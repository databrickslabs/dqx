name: perf_test

on:
  pull_request:
    types: [ opened, synchronize, ready_for_review ]
  merge_group:
    types: [ checks_requested ]
  push:
    branches:
      - main

permissions:
  id-token: write
  contents: read
  pull-requests: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    if: github.event_name == 'pull_request'
    environment: tool
    runs-on: larger
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: pip
          cache-dependency-path: '**/pyproject.toml'

      - name: Install hatch
        run: pip install hatch==1.9.4

      - name: Detect existing baseline
        id: detect
        shell: bash
        run: |
          shopt -s nullglob
          files=(tests/perf/.benchmarks/*/*_benchmark.json)
          if (( ${#files[@]} == 0 )); then
            echo "missing=true"
            echo "missing=true"  >> $GITHUB_OUTPUT
          else
            echo "missing=false"
            echo "missing=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate initial baseline if missing
        uses: databrickslabs/sandbox/acceptance@acceptance/v0.4.4
        if: steps.detect.outputs.missing == 'true'
        with:
          vault_uri: ${{ secrets.VAULT_URI }}
          timeout: 2h
          codegen_path: tests/perf/.codegen.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          DATABRICKS_SERVERLESS_COMPUTE_ID: ${{ env.DATABRICKS_SERVERLESS_COMPUTE_ID }}
          PYTEST_ADDOPTS: "--benchmark-save=benchmark"

      - name: Print baseline benchmarks if missing
        if: steps.detect.outputs.missing == 'true'
        run: |
          echo "----- Benchmark Files -----"
          find tests/perf/.benchmarks -type d
          echo "----- Benchmark Content -----"
          for file in tests/perf/.benchmarks/*/*_benchmark.json; do
            echo "----- $file -----"
            cat "$file"
          done

      - name: Generate current perf results
        uses: databrickslabs/sandbox/acceptance@acceptance/v0.4.4
        with:
          vault_uri: ${{ secrets.VAULT_URI }}
          timeout: 2h
          codegen_path: tests/perf/.codegen.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          DATABRICKS_SERVERLESS_COMPUTE_ID: ${{ env.DATABRICKS_SERVERLESS_COMPUTE_ID }}
          PYTEST_ADDOPTS: "--benchmark-save=current"

      - name: Print current benchmarks
        run: |
          echo "----- Benchmark Files -----"
          find tests/perf/.benchmarks
          echo "----- Benchmark Content -----"
          for file in tests/perf/.benchmarks/*/*_current.json; do
            echo "----- $file -----"
            cat "$file"
          done  

      - name: Compare Benchmark Results
        run: |
          echo "Comparing current benchmark results with the saved baseline ..."
          # allow 20% of performance degradation
          hatch run pytest tests/perf --benchmark-compare=1 --benchmark-compare-fail=1.2