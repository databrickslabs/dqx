name: acceptance

on:
  pull_request:
    types: [ opened, synchronize, ready_for_review ]
  merge_group:
    types: [ checks_requested ]
  push:
    branches:
      - main

permissions:
  id-token: write
  contents: read
  pull-requests: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false  # don't cancel ongoing runs to ensure fixtures are completed and resources terminated

jobs:
  # Detect changes to anomaly module to conditionally run anomaly-specific tests
  detect_changes:
    if: github.event_name == 'pull_request' && !github.event.pull_request.draft && !github.event.pull_request.head.repo.fork
    runs-on: ubuntu-latest
    outputs:
      anomaly: ${{ steps.filter.outputs.anomaly }}
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Check for anomaly module changes
        uses: dorny/paths-filter@v3
        id: filter
        with:
          base: ${{ github.event.pull_request.base.sha }}
          ref: ${{ github.event.pull_request.head.sha }}
          initial-fetch-depth: 0
          token: ""
          filters: |
            anomaly:
              - 'src/databricks/labs/dqx/anomaly/**'
              - 'tests/integration/test_anomaly_*.py'

  integration:
    # Temporarily disabled to speed up anomaly-only debugging.
    if: false
    # Only run this job for PRs from branches on the main repository and not from forks.
    # Workflows triggered by PRs from forks don't have access to the tool environment.
    # PRs from forks to be tested by the reviewer(s) / maintainer(s) before merging.
    needs: detect_changes
    environment: tool
    runs-on: larger
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          cache: 'pip'
          cache-dependency-path: '**/pyproject.toml'
          python-version: '3.12'

      - name: Install hatch
        run: pip install hatch==1.15.0

      # Integration tests are run from within tests/integration folder.
      # Create .coveragerc with correct relative path to source code.
      - name: Prepare code coverage configuration for integration tests
        run: |
          cat > tests/integration/.coveragerc << EOF
          [run]
          source = ../../src
          relative_files = true
          EOF

      - name: Azure login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.ARM_CLIENT_ID }}
          tenant-id: ${{ secrets.ARM_TENANT_ID }}
          allow-no-subscriptions: true

      - name: Set env vars for Azure CLI auth
        shell: bash
        run: |
          val=$(az keyvault secret show --id "${{ secrets.VAULT_URI }}/secrets/DATABRICKS-HOST" --query value -o tsv)
          # Ensure host has https:// prefix (SDK and MLflow expect full URL)
          if [[ ! "$val" =~ ^https?:// ]]; then
            val="https://$val"
          fi
          echo "DATABRICKS_HOST=$val" >> $GITHUB_ENV
          echo "DATABRICKS_AUTH_TYPE=azure-cli" >> $GITHUB_ENV

      - name: Run integration tests and generate test coverage report (excluding anomaly tests)
        uses: databrickslabs/sandbox/acceptance@acceptance/v0.4.4
        with:
          vault_uri: ${{ secrets.VAULT_URI }}
          timeout: 2h
          codegen_path: tests/integration/.codegen.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          COVERAGE_FILE: ${{ github.workspace }}/.coverage
          # Exclude anomaly tests from regular integration tests to avoid duplication
          PYTEST_ADDOPTS: "-m 'not anomaly'"

          # Belt + braces: pass through explicitly to the action step
          DATABRICKS_HOST: ${{ env.DATABRICKS_HOST }}
          DATABRICKS_AUTH_TYPE: ${{ env.DATABRICKS_AUTH_TYPE }}

      - name: Merge coverage reports and convert them to XML
        run: |
          hatch run combine_coverage

      # Recursively search the entire workspace directory for all coverage reports.
      # All uploaded test coverage reports will be used even if publish is done multiple time.
      - name: Publish test coverage
        uses: codecov/codecov-action@v5
        with:
          use_oidc: true

  integration_serverless:
    # Temporarily disabled to speed up anomaly-only debugging.
    if: false
    # Only run this job for PRs from branches on the main repository and not from forks.
    # Workflows triggered by PRs from forks don't have access to the tool environment.
    # PRs from forks to be tested by the reviewer(s) / maintainer(s) before merging.
    needs: detect_changes
    environment: tool
    runs-on: larger
    env:
      DATABRICKS_SERVERLESS_COMPUTE_ID: auto
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          cache: 'pip'
          cache-dependency-path: '**/pyproject.toml'
          python-version: '3.12'

      - name: Install hatch
        run: pip install hatch==1.15.0

      # Integration tests are run from within tests/integration folder.
      # Create .coveragerc with correct relative path to source code.
      - name: Prepare code coverage configuration for integration tests
        run: |
          cat > tests/integration/.coveragerc << EOF
          [run]
          source = ../../src
          relative_files = true
          EOF

      - name: Azure login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.ARM_CLIENT_ID }}
          tenant-id: ${{ secrets.ARM_TENANT_ID }}
          allow-no-subscriptions: true

      - name: Set env vars for Azure CLI auth
        shell: bash
        run: |
          val=$(az keyvault secret show --id "${{ secrets.VAULT_URI }}/secrets/DATABRICKS-HOST" --query value -o tsv)
          # Ensure host has https:// prefix (SDK and MLflow expect full URL)
          if [[ ! "$val" =~ ^https?:// ]]; then
            val="https://$val"
          fi
          echo "DATABRICKS_HOST=$val" >> $GITHUB_ENV
          echo "DATABRICKS_AUTH_TYPE=azure-cli" >> $GITHUB_ENV

      - name: Run integration tests on serverless cluster (excluding anomaly tests)
        uses: databrickslabs/sandbox/acceptance@acceptance/v0.4.4
        with:
          vault_uri: ${{ secrets.VAULT_URI }}
          timeout: 2h
          codegen_path: tests/integration/.codegen.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          COVERAGE_FILE: ${{ github.workspace }}/.coverage
          DATABRICKS_SERVERLESS_COMPUTE_ID: ${{ env.DATABRICKS_SERVERLESS_COMPUTE_ID }}
          # Exclude anomaly tests from regular integration tests to avoid duplication
          PYTEST_ADDOPTS: "-m 'not anomaly'"

          DATABRICKS_HOST: ${{ env.DATABRICKS_HOST }}
          DATABRICKS_AUTH_TYPE: ${{ env.DATABRICKS_AUTH_TYPE }}

      - name: Merge coverage reports and convert them to XML
        run: |
          hatch run combine_coverage

      # collects all coverage reports
      - name: Publish test coverage
        uses: codecov/codecov-action@v5
        with:
          use_oidc: true

  integration_anomaly:
    # Run anomaly-specific tests only when anomaly module changes
    needs: detect_changes
    if: github.event_name == 'pull_request' && !github.event.pull_request.draft && !github.event.pull_request.head.repo.fork && needs.detect_changes.outputs.anomaly == 'true'
    environment: tool
    runs-on: larger
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          cache: 'pip'
          cache-dependency-path: '**/pyproject.toml'
          python-version: '3.12'

      - name: Install hatch
        run: pip install hatch==1.15.0

      # Integration tests are run from within tests/integration folder.
      # Create .coveragerc with correct relative path to source code.
      - name: Prepare code coverage configuration for integration tests
        run: |
          cat > tests/integration/.coveragerc << EOF
          [run]
          source = ../../src
          relative_files = true
          EOF

      - name: Azure login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.ARM_CLIENT_ID }}
          tenant-id: ${{ secrets.ARM_TENANT_ID }}
          allow-no-subscriptions: true

      - name: Set env vars for Azure CLI auth + MLflow
        shell: bash
        run: |
          val=$(az keyvault secret show --id "${{ secrets.VAULT_URI }}/secrets/DATABRICKS-HOST" --query value -o tsv)
          # Ensure host has https:// prefix (SDK and MLflow expect full URL)
          if [[ ! "$val" =~ ^https?:// ]]; then
            val="https://$val"
          fi
          # Workaround for MLflow OIDC auth: MLflow requires a profile to exist even when it uses SDK auth.
          dummy_profile="${RUNNER_TEMP}/databricks_profile"
          cat > "$dummy_profile" << EOF
          [DEFAULT]
          host = $val
          token = dummy
          EOF
          echo "DATABRICKS_HOST=$val" >> $GITHUB_ENV
          echo "DATABRICKS_AUTH_TYPE=azure-cli" >> $GITHUB_ENV
          echo "DATABRICKS_CONFIG_FILE=$dummy_profile" >> $GITHUB_ENV

          # MLflow: Use databricks scheme so MLflow uses SDK auth (with dummy profile present).
          echo "MLFLOW_ENABLE_DB_SDK=true" >> $GITHUB_ENV
          echo "MLFLOW_TRACKING_URI=databricks" >> $GITHUB_ENV
          echo "MLFLOW_REGISTRY_URI=databricks-uc" >> $GITHUB_ENV

      - name: Run anomaly integration tests and generate test coverage report
        uses: databrickslabs/sandbox/acceptance@acceptance/v0.4.4
        with:
          vault_uri: ${{ secrets.VAULT_URI }}
          timeout: 2h
          codegen_path: tests/integration/.codegen.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          COVERAGE_FILE: ${{ github.workspace }}/.coverage
          # Run only anomaly tests (stop on first failure for clearer logs)
          PYTEST_ADDOPTS: "-k test_anomaly_ -x -vv -n 0 --maxfail=1"
          MLFLOW_HTTP_REQUEST_TIMEOUT: "600"
          MLFLOW_HTTP_REQUEST_MAX_RETRIES: "10"

          # Belt + braces: pass through explicitly to the action step
          DATABRICKS_HOST: ${{ env.DATABRICKS_HOST }}
          DATABRICKS_AUTH_TYPE: ${{ env.DATABRICKS_AUTH_TYPE }}
          DATABRICKS_CONFIG_FILE: ${{ env.DATABRICKS_CONFIG_FILE }}
          MLFLOW_ENABLE_DB_SDK: "true"
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
          MLFLOW_REGISTRY_URI: "databricks-uc"
      - name: Debug MLflow/Databricks env (anomaly)
        shell: bash
        run: |
          echo "DATABRICKS_HOST=${DATABRICKS_HOST}"
          echo "DATABRICKS_AUTH_TYPE=${DATABRICKS_AUTH_TYPE}"
          echo "DATABRICKS_CONFIG_FILE=${DATABRICKS_CONFIG_FILE}"
          echo "MLFLOW_ENABLE_DB_SDK=${MLFLOW_ENABLE_DB_SDK}"
          echo "MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI}"
          echo "MLFLOW_REGISTRY_URI=${MLFLOW_REGISTRY_URI}"

      - name: Merge coverage reports and convert them to XML
        run: |
          hatch run combine_coverage

      # Recursively search the entire workspace directory for all coverage reports.
      # All uploaded test coverage reports will be used even if publish is done multiple time.
      - name: Publish test coverage
        uses: codecov/codecov-action@v5
        with:
          use_oidc: true

  integration_anomaly_serverless:
    # Run anomaly-specific tests only when anomaly module changes (serverless)
    needs: detect_changes
    if: false
    environment: tool
    runs-on: larger
    env:
      DATABRICKS_SERVERLESS_COMPUTE_ID: auto
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          cache: 'pip'
          cache-dependency-path: '**/pyproject.toml'
          python-version: '3.12'

      - name: Install hatch
        run: pip install hatch==1.15.0

      # Integration tests are run from within tests/integration folder.
      # Create .coveragerc with correct relative path to source code.
      - name: Prepare code coverage configuration for integration tests
        run: |
          cat > tests/integration/.coveragerc << EOF
          [run]
          source = ../../src
          relative_files = true
          EOF

      - name: Azure login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.ARM_CLIENT_ID }}
          tenant-id: ${{ secrets.ARM_TENANT_ID }}
          allow-no-subscriptions: true

      - name: Set env vars for Azure CLI auth + MLflow
        shell: bash
        run: |
          val=$(az keyvault secret show --id "${{ secrets.VAULT_URI }}/secrets/DATABRICKS-HOST" --query value -o tsv)
          # Ensure host has https:// prefix (SDK and MLflow expect full URL)
          if [[ ! "$val" =~ ^https?:// ]]; then
            val="https://$val"
          fi
          # Workaround for MLflow OIDC auth: MLflow requires a profile to exist even when it uses SDK auth.
          dummy_profile="${RUNNER_TEMP}/databricks_profile"
          cat > "$dummy_profile" << EOF
          [DEFAULT]
          host = $val
          token = dummy
          EOF
          echo "DATABRICKS_HOST=$val" >> $GITHUB_ENV
          echo "DATABRICKS_AUTH_TYPE=azure-cli" >> $GITHUB_ENV
          echo "DATABRICKS_CONFIG_FILE=$dummy_profile" >> $GITHUB_ENV

          # MLflow: Use databricks scheme so MLflow uses SDK auth (with dummy profile present).
          echo "MLFLOW_ENABLE_DB_SDK=true" >> $GITHUB_ENV
          echo "MLFLOW_TRACKING_URI=databricks" >> $GITHUB_ENV
          echo "MLFLOW_REGISTRY_URI=databricks-uc" >> $GITHUB_ENV

      - name: Run anomaly integration tests on serverless cluster
        uses: databrickslabs/sandbox/acceptance@acceptance/v0.4.4
        with:
          vault_uri: ${{ secrets.VAULT_URI }}
          timeout: 2h
          codegen_path: tests/integration/.codegen.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          COVERAGE_FILE: ${{ github.workspace }}/.coverage
          DATABRICKS_SERVERLESS_COMPUTE_ID: ${{ env.DATABRICKS_SERVERLESS_COMPUTE_ID }}
          # Run only anomaly tests (stop on first failure for clearer logs)
          PYTEST_ADDOPTS: "-k test_anomaly_ -x -vv -n 0 --maxfail=1"
          MLFLOW_HTTP_REQUEST_TIMEOUT: "600"
          MLFLOW_HTTP_REQUEST_MAX_RETRIES: "10"

          DATABRICKS_HOST: ${{ env.DATABRICKS_HOST }}
          DATABRICKS_AUTH_TYPE: ${{ env.DATABRICKS_AUTH_TYPE }}
          DATABRICKS_CONFIG_FILE: ${{ env.DATABRICKS_CONFIG_FILE }}
          MLFLOW_ENABLE_DB_SDK: "true"
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
          MLFLOW_REGISTRY_URI: "databricks-uc"
      - name: Debug MLflow/Databricks env (anomaly serverless)
        shell: bash
        run: |
          echo "DATABRICKS_HOST=${DATABRICKS_HOST}"
          echo "DATABRICKS_AUTH_TYPE=${DATABRICKS_AUTH_TYPE}"
          echo "DATABRICKS_CONFIG_FILE=${DATABRICKS_CONFIG_FILE}"
          echo "MLFLOW_ENABLE_DB_SDK=${MLFLOW_ENABLE_DB_SDK}"
          echo "MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI}"
          echo "MLFLOW_REGISTRY_URI=${MLFLOW_REGISTRY_URI}"

      - name: Merge coverage reports and convert them to XML
        run: |
          hatch run combine_coverage

      # collects all coverage reports
      - name: Publish test coverage
        uses: codecov/codecov-action@v5
        with:
          use_oidc: true

  e2e:
    # Temporarily disabled to speed up anomaly-only debugging.
    if: false
    environment: tool
    runs-on: larger
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          cache: 'pip'
          cache-dependency-path: '**/pyproject.toml'
          python-version: '3.12'

      - name: Install hatch
        run: pip install hatch==1.15.0
      
      - name: Install dbt
        run: |
          pip install dbt-core==1.10.9 dbt-databricks==1.10.9

      - name: Install Databricks CLI
        run: |
          curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
          databricks --version

      - name: Azure login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.ARM_CLIENT_ID }}
          tenant-id: ${{ secrets.ARM_TENANT_ID }}
          allow-no-subscriptions: true

      - name: Set env vars for Azure CLI auth
        run: |
          val=$(az keyvault secret show --id "${{ secrets.VAULT_URI }}/secrets/DATABRICKS-HOST" --query value -o tsv)
          # Ensure host has https:// prefix (SDK expects full URL)
          if [[ ! "$val" =~ ^https?:// ]]; then
            val="https://$val"
          fi
          echo "DATABRICKS_HOST=$val" >> $GITHUB_ENV
          echo "DATABRICKS_AUTH_TYPE=azure-cli" >> $GITHUB_ENV

      - name: Run e2e tests
        uses: databrickslabs/sandbox/acceptance@acceptance/v0.4.4
        with:
          vault_uri: ${{ secrets.VAULT_URI }}
          timeout: 2h
          codegen_path: tests/e2e/.codegen.json
        env:
          REF_NAME: ${{ github.ref_name }} # NOTE: end-to-end tests use this to pip install from the current PR branch
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          DATABRICKS_HOST: ${{ env.DATABRICKS_HOST }}
          DATABRICKS_AUTH_TYPE: ${{ env.DATABRICKS_AUTH_TYPE }}

  e2e_serverless:
    # Temporarily disabled to speed up anomaly-only debugging.
    if: false
    environment: tool
    runs-on: larger
    env:
      DATABRICKS_SERVERLESS_COMPUTE_ID: auto
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          cache: 'pip'
          cache-dependency-path: '**/pyproject.toml'
          python-version: '3.12'

      - name: Install hatch
        run: pip install hatch==1.15.0

      - name: Install dbt
        run: |
          pip install dbt-core==1.10.9 dbt-databricks==1.10.9

      - name: Install Databricks CLI
        run: |
          curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
          databricks --version

      - name: Azure login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.ARM_CLIENT_ID }}
          tenant-id: ${{ secrets.ARM_TENANT_ID }}
          allow-no-subscriptions: true

      - name: Set env vars for Azure CLI auth
        run: |
          val=$(az keyvault secret show --id "${{ secrets.VAULT_URI }}/secrets/DATABRICKS-HOST" --query value -o tsv)
          # Ensure host has https:// prefix (SDK expects full URL)
          if [[ ! "$val" =~ ^https?:// ]]; then
            val="https://$val"
          fi
          echo "DATABRICKS_HOST=$val" >> $GITHUB_ENV
          echo "DATABRICKS_AUTH_TYPE=azure-cli" >> $GITHUB_ENV

      - name: Run e2e tests on serverless cluster
        uses: databrickslabs/sandbox/acceptance@acceptance/v0.4.4
        with:
          vault_uri: ${{ secrets.VAULT_URI }}
          timeout: 2h
          codegen_path: tests/e2e/.codegen.json
        env:
          REF_NAME: ${{ github.ref_name }} # NOTE: end-to-end tests use this to pip install from the current PR branch
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          DATABRICKS_SERVERLESS_COMPUTE_ID: ${{ env.DATABRICKS_SERVERLESS_COMPUTE_ID }}
          DATABRICKS_HOST: ${{ env.DATABRICKS_HOST }}
          DATABRICKS_AUTH_TYPE: ${{ env.DATABRICKS_AUTH_TYPE }}
