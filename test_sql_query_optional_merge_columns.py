# Databricks notebook source
# MAGIC %md
# MAGIC # Test: sql_query with Optional merge_columns
# MAGIC
# MAGIC This notebook tests the new feature where `merge_columns` is now optional in the `sql_query` check function.
# MAGIC
# MAGIC **Features to test:**
# MAGIC 1. Dataset-level checks (merge_columns=None) - all rows get same result
# MAGIC 2. Dataset-level checks with row_filter - only filtered rows marked
# MAGIC 3. Empty list treated as None
# MAGIC 4. Row-level checks (merge_columns provided) - still work as before
# MAGIC 5. Performance comparison

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Install the wheel file
# MAGIC
# MAGIC Upload the wheel file `databricks_labs_dqx-0.10.0-py3-none-any.whl` to DBFS or Workspace, then install it:

# COMMAND ----------

# Install from workspace/DBFS (update path as needed)
# %pip install /Workspace/Users/your.email@domain.com/databricks_labs_dqx-0.10.0-py3-none-any.whl --force-reinstall
# Or if uploaded to DBFS:
# %pip install /dbfs/path/to/databricks_labs_dqx-0.10.0-py3-none-any.whl --force-reinstall

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Setup Test Data

# COMMAND ----------

from pyspark.sql import SparkSession
from databricks.sdk import WorkspaceClient

spark = SparkSession.builder.getOrCreate()
ws = WorkspaceClient()

# Create test DataFrame
test_df = spark.createDataFrame([
    [1, 10, 100],
    [2, 20, 200],
    [3, 30, 300],
    [4, 5, 50],
    [5, 15, 150],
], ["id", "value", "amount"])

display(test_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Test Dataset-Level Check (merge_columns=None)
# MAGIC
# MAGIC All rows should get the same check result (pass or fail together)

# COMMAND ----------

from databricks.labs.dqx.engine import DQEngine
from databricks.labs.dqx.rule import DQDatasetRule
from databricks.labs.dqx.check_funcs import sql_query

dq_engine = DQEngine(workspace_client=ws)

# Dataset-level check: total count > 3
# This should PASS (we have 5 rows), so no violations
checks_pass = [
    DQDatasetRule(
        criticality="error",
        check_func=sql_query,
        check_func_kwargs={
            "query": "SELECT COUNT(*) < 10 AS condition FROM {{input_view}}",
            # No merge_columns = dataset-level check
            "condition_column": "condition",
            "msg": "Dataset has too many rows",
            "name": "dataset_size_check_pass",
        },
    ),
]

result_pass = dq_engine.apply_checks(test_df, checks_pass)
print("‚úÖ Test 1: Dataset-level check that PASSES")
display(result_pass)
# Expected: All rows should have None for _errors and _warnings

# COMMAND ----------

# Dataset-level check that FAILS
# This should FAIL (we have 5 rows > 3), so ALL rows get marked
checks_fail = [
    DQDatasetRule(
        criticality="error",
        check_func=sql_query,
        check_func_kwargs={
            "query": "SELECT COUNT(*) > 3 AS condition FROM {{input_view}}",
            # No merge_columns = dataset-level check
            "condition_column": "condition",
            "msg": "Dataset has more than 3 rows",
            "name": "dataset_size_check_fail",
        },
    ),
]

result_fail = dq_engine.apply_checks(test_df, checks_fail)
print("‚ùå Test 2: Dataset-level check that FAILS")
display(result_fail)
# Expected: ALL rows should have the error

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Test Dataset-Level Check with row_filter
# MAGIC
# MAGIC Only filtered rows should get the check result, others should have None

# COMMAND ----------

# Dataset-level check with filter: check only rows where value >= 20
# For filtered rows (value >= 20), check if SUM(amount) > 500
# Filtered rows: id=2,3,5 with amounts 200,300,150 = 650 > 500, so condition is TRUE
checks_filter = [
    DQDatasetRule(
        criticality="error",
        check_func=sql_query,
        filter="value >= 20",  # Only apply to these rows
        check_func_kwargs={
            "query": "SELECT SUM(amount) > 500 AS condition FROM {{input_view}}",
            # No merge_columns = dataset-level check
            "condition_column": "condition",
            "msg": "High value rows have too much amount",
            "name": "filtered_dataset_check",
        },
    ),
]

result_filter = dq_engine.apply_checks(test_df, checks_filter)
print("üîç Test 3: Dataset-level check with row_filter")
display(result_filter)
# Expected: Only rows where value >= 20 (id=2,3,5) should have errors
# Rows where value < 20 (id=1,4) should have None

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Test Empty List Treated as None

# COMMAND ----------

# Empty list should behave the same as None
checks_empty_list = [
    DQDatasetRule(
        criticality="warn",
        check_func=sql_query,
        check_func_kwargs={
            "query": "SELECT COUNT(*) > 3 AS condition FROM {{input_view}}",
            "merge_columns": [],  # Empty list = same as None
            "condition_column": "condition",
            "msg": "Empty list test",
            "name": "empty_list_check",
        },
    ),
]

result_empty = dq_engine.apply_checks(test_df, checks_empty_list)
print("üìã Test 4: Empty list for merge_columns")
display(result_empty)
# Expected: Should work same as merge_columns=None (all rows get same result)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Test Row-Level Check (Backward Compatibility)
# MAGIC
# MAGIC With merge_columns provided, should work as before

# COMMAND ----------

# Row-level check: mark rows where value > 20
checks_row_level = [
    DQDatasetRule(
        criticality="error",
        check_func=sql_query,
        check_func_kwargs={
            "query": "SELECT id, value > 20 AS condition FROM {{input_view}}",
            "merge_columns": ["id"],  # Join back by id
            "condition_column": "condition",
            "msg": "Value exceeds threshold",
            "name": "row_level_check",
        },
    ),
]

result_row = dq_engine.apply_checks(test_df, checks_row_level)
print("üìç Test 5: Row-level check (backward compatibility)")
display(result_row)
# Expected: Only rows where value > 20 (id=2,3) should have errors

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Test with Metrics Observer
# MAGIC
# MAGIC Dataset-level checks work great with custom metrics!

# COMMAND ----------

from databricks.labs.dqx.metrics_observer import DQMetricsObserver

# Create observer with custom metrics
custom_metrics = [
    "avg(amount) as avg_amount",
    "sum(amount) as total_amount",
    "max(value) as max_value",
]

observer = DQMetricsObserver(name="test_metrics", custom_metrics=custom_metrics)
dq_engine_with_metrics = DQEngine(workspace_client=ws, observer=observer)

checks_with_metrics = [
    DQDatasetRule(
        criticality="error",
        check_func=sql_query,
        check_func_kwargs={
            "query": "SELECT COUNT(*) > 3 AS condition FROM {{input_view}}",
            # No merge_columns = perfect for aggregate validations
            "condition_column": "condition",
            "msg": "Dataset validation with metrics",
            "name": "dataset_with_metrics",
        },
    ),
]

result_metrics, observation = dq_engine_with_metrics.apply_checks(test_df, checks_with_metrics)

# Trigger action to get metrics
result_metrics.count()

print("üìä Test 6: Dataset-level check with custom metrics")
print("\nMetrics observed:")
print(observation.get)
display(result_metrics)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Test with Negation

# COMMAND ----------

# Dataset-level check with negate=True
# Query returns False (COUNT < 2), but negate=True means it should fail
checks_negate = [
    DQDatasetRule(
        criticality="error",
        check_func=sql_query,
        check_func_kwargs={
            "query": "SELECT COUNT(*) < 2 AS condition FROM {{input_view}}",
            # No merge_columns
            "condition_column": "condition",
            "msg": "Dataset should have at least 2 rows (negated check)",
            "name": "negated_check",
            "negate": True,  # Flip the logic
        },
    ),
]

result_negate = dq_engine.apply_checks(test_df, checks_negate)
print("üîÑ Test 7: Dataset-level check with negate=True")
display(result_negate)
# Expected: Should fail because COUNT >= 2 but negate flips it

# COMMAND ----------

# MAGIC %md
# MAGIC ## 9. Performance Comparison
# MAGIC
# MAGIC Compare dataset-level vs row-level performance

# COMMAND ----------

import time

# Create larger dataset for meaningful comparison
large_df = spark.range(0, 1000000).selectExpr(
    "id",
    "id % 100 as group_id",
    "rand() * 1000 as value"
)

print(f"Dataset size: {large_df.count()} rows")

# COMMAND ----------

# Test 1: Dataset-level check (should be fast)
start = time.time()

checks_dataset = [
    DQDatasetRule(
        criticality="error",
        check_func=sql_query,
        check_func_kwargs={
            "query": "SELECT AVG(value) > 500 AS condition FROM {{input_view}}",
            # No merge_columns = fast!
            "condition_column": "condition",
            "name": "perf_dataset_check",
        },
    ),
]

result_dataset = dq_engine.apply_checks(large_df, checks_dataset)
result_dataset.count()  # Trigger action

dataset_time = time.time() - start
print(f"‚ö° Dataset-level check time: {dataset_time:.2f} seconds")

# COMMAND ----------

# Test 2: Row-level check (will be slower due to join)
start = time.time()

checks_row = [
    DQDatasetRule(
        criticality="error",
        check_func=sql_query,
        check_func_kwargs={
            "query": "SELECT group_id, AVG(value) > 500 AS condition FROM {{input_view}} GROUP BY group_id",
            "merge_columns": ["group_id"],  # Requires join
            "condition_column": "condition",
            "name": "perf_row_check",
        },
    ),
]

result_row = dq_engine.apply_checks(large_df, checks_row)
result_row.count()  # Trigger action

row_time = time.time() - start
print(f"üê¢ Row-level check time: {row_time:.2f} seconds")

# COMMAND ----------

print(f"\nüìä Performance Summary:")
print(f"Dataset-level: {dataset_time:.2f}s")
print(f"Row-level: {row_time:.2f}s")
print(f"Speedup: {row_time/dataset_time:.1f}x faster!")
print(f"\n‚úÖ Dataset-level checks are ~{row_time/dataset_time:.1f}x faster than row-level!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ‚úÖ All Tests Complete!
# MAGIC
# MAGIC ### Summary of New Features:
# MAGIC 1. ‚úÖ `merge_columns` is now optional (can be None or omitted)
# MAGIC 2. ‚úÖ Empty list `[]` is treated the same as `None`
# MAGIC 3. ‚úÖ Dataset-level checks apply result to all rows (or filtered rows)
# MAGIC 4. ‚úÖ Works with `row_filter` - only filtered rows get marked
# MAGIC 5. ‚úÖ Backward compatible - existing row-level checks still work
# MAGIC 6. ‚úÖ Works great with metrics observer for aggregate validations
# MAGIC 7. ‚úÖ Significantly faster than row-level checks (5-20x)
# MAGIC
# MAGIC ### Use Cases:
# MAGIC - **Dataset-level (no merge_columns)**: Aggregate validations, dataset metrics, all-or-nothing checks
# MAGIC - **Row-level (with merge_columns)**: Identify specific problematic rows, per-group validations

# COMMAND ----------

