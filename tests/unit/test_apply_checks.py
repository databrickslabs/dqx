import pyspark.sql.functions as F
from chispa.dataframe_comparer import assert_df_equality  # type: ignore
from pyspark.sql import Column, SparkSession

from databricks.labs.dqx.dq_engine import (
    DQRule,
    apply_checks,
    apply_checks_and_split,
    apply_checks_by_metadata,
    apply_checks_by_metadata_and_split,
)
from databricks.labs.dqx.dq_functions import (
    col_is_not_null_and_not_empty,
    make_condition_col,
)

schema = "a: int, b: int, c: int"
checked_schema = schema + ", _errors: map<string,string>, _warnings: map<string,string>"


def test_apply_checks_on_empty_checks(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, None], [2, 4, None]], schema)

    good = apply_checks(test_df, [])

    assert_df_equality(good, test_df)


def test_apply_checks_and_split_on_empty_checks(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, None], [2, 4, None]], schema)

    good, bad = apply_checks_and_split(test_df, [])

    expected_df = spark_session.createDataFrame([], checked_schema)

    assert_df_equality(good, test_df)
    assert_df_equality(bad, expected_df)


def test_apply_checks_passed(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3]], schema)

    checks = [
        DQRule(name="col_a_is_null_or_empty", criticality="warn", check=col_is_not_null_and_not_empty("a")),
        DQRule(name="col_b_is_null_or_empty", criticality="error", check=col_is_not_null_and_not_empty("b")),
    ]

    checked = apply_checks(test_df, checks)

    expected = spark_session.createDataFrame([[1, 3, 3, None, None]], checked_schema)

    assert_df_equality(checked, expected, ignore_nullable=True)


def test_apply_checks(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3], [2, None, 4], [None, 4, None], [None, None, None]], schema)

    checks = [
        DQRule(name="col_a_is_null_or_empty", criticality="warn", check=col_is_not_null_and_not_empty("a")),
        DQRule(name="col_b_is_null_or_empty", criticality="error", check=col_is_not_null_and_not_empty("b")),
        DQRule(name="col_c_is_null_or_empty", criticality="error", check=col_is_not_null_and_not_empty("c")),
    ]

    checked = apply_checks(test_df, checks)

    expected = spark_session.createDataFrame(
        [
            [1, 3, 3, None, None],
            [2, None, 4, {"col_b_is_null_or_empty": "Column b is null or empty"}, None],
            [
                None,
                4,
                None,
                {"col_c_is_null_or_empty": "Column c is null or empty"},
                {"col_a_is_null_or_empty": "Column a is null or empty"},
            ],
            [
                None,
                None,
                None,
                {
                    "col_b_is_null_or_empty": "Column b is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
                {"col_a_is_null_or_empty": "Column a is null or empty"},
            ],
        ],
        checked_schema,
    )

    assert_df_equality(checked, expected, ignore_nullable=True)


def test_apply_checks_with_autogenerated_col_names(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3], [2, None, 4], [None, 4, None], [None, None, None]], schema)

    checks = [
        DQRule(criticality="warn", check=col_is_not_null_and_not_empty("a")),
        DQRule(criticality="error", check=col_is_not_null_and_not_empty("b")),
        DQRule(criticality="error", check=col_is_not_null_and_not_empty("c")),
    ]

    checked = apply_checks(test_df, checks)

    expected = spark_session.createDataFrame(
        [
            [1, 3, 3, None, None],
            [2, None, 4, {"col_b_is_null_or_empty": "Column b is null or empty"}, None],
            [
                None,
                4,
                None,
                {"col_c_is_null_or_empty": "Column c is null or empty"},
                {"col_a_is_null_or_empty": "Column a is null or empty"},
            ],
            [
                None,
                None,
                None,
                {
                    "col_b_is_null_or_empty": "Column b is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
                {"col_a_is_null_or_empty": "Column a is null or empty"},
            ],
        ],
        checked_schema,
    )

    assert_df_equality(checked, expected, ignore_nullable=True)


def test_apply_checks_and_split(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3], [2, None, 4], [None, 4, None], [None, None, None]], schema)

    checks = [
        DQRule(name="col_a_is_null_or_empty", criticality="warn", check=col_is_not_null_and_not_empty("a")),
        DQRule(name="col_b_is_null_or_empty", criticality="error", check=col_is_not_null_and_not_empty("b")),
        DQRule(name="col_c_is_null_or_empty", criticality="warn", check=col_is_not_null_and_not_empty("c")),
    ]

    good, bad = apply_checks_and_split(test_df, checks)

    expected_good = spark_session.createDataFrame([[1, 3, 3], [None, 4, None]], schema)
    assert_df_equality(good, expected_good)

    expected_bad = spark_session.createDataFrame(
        [
            [2, None, 4, {"col_b_is_null_or_empty": "Column b is null or empty"}, None],
            [
                None,
                4,
                None,
                None,
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
            [
                None,
                None,
                None,
                {"col_b_is_null_or_empty": "Column b is null or empty"},
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
        ],
        checked_schema,
    )

    assert_df_equality(bad, expected_bad, ignore_nullable=True)


def test_apply_checks_and_split_by_metadata(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3], [2, None, 4], [None, 4, None], [None, None, None]], schema)

    checks = [
        {
            "name": "col_a_is_null_or_empty",
            "criticality": "warn",
            "check": {"function": "col_is_not_null_and_not_empty", "arguments": {"col_name": "a"}},
        },
        {
            "name": "col_b_is_null_or_empty",
            "criticality": "error",
            "check": {"function": "col_is_not_null_and_not_empty", "arguments": {"col_name": "b"}},
        },
        {
            "name": "col_c_is_null_or_empty",
            "criticality": "warn",
            "check": {"function": "col_is_not_null_and_not_empty", "arguments": {"col_name": "c"}},
        },
        {
            "name": "col_a_value_is_not_in_the_list",
            "criticality": "warn",
            "check": {"function": "col_value_is_in_list", "arguments": {"col_name": "a", "allowed": [1, 3, 4]}},
        },
        {
            "name": "col_c_value_is_not_in_the_list",
            "criticality": "warn",
            "check": {"function": "col_value_is_in_list", "arguments": {"col_name": "c", "allowed": [1, 3, 4]}},
        },
    ]

    good, bad = apply_checks_by_metadata_and_split(test_df, checks)

    expected_good = spark_session.createDataFrame([[1, 3, 3], [None, 4, None]], schema)
    assert_df_equality(good, expected_good)

    expected_bad = spark_session.createDataFrame(
        [
            [
                2,
                None,
                4,
                {"col_b_is_null_or_empty": "Column b is null or empty"},
                {"col_a_value_is_not_in_the_list": "Value 2 is not in the allowed list: [1, 3, 4]"},
            ],
            [
                None,
                4,
                None,
                None,
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
            [
                None,
                None,
                None,
                {"col_b_is_null_or_empty": "Column b is null or empty"},
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
        ],
        checked_schema,
    )

    assert_df_equality(bad, expected_bad, ignore_nullable=True)


def test_apply_checks_and_split_by_metadata_with_autogenerated_col_names(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3], [2, None, 4], [None, 4, None], [None, None, None]], schema)

    checks = [
        {
            "criticality": "warn",
            "check": {"function": "col_is_not_null_and_not_empty", "arguments": {"col_names": ["a", "c"]}},
        },
        {
            "criticality": "error",
            "check": {"function": "col_is_not_null_and_not_empty", "arguments": {"col_name": "b"}},
        },
        {
            "criticality": "warn",
            "check": {"function": "col_value_is_in_list", "arguments": {"col_names": ["a", "c"], "allowed": [1, 3, 4]}},
        },
    ]

    good, bad = apply_checks_by_metadata_and_split(test_df, checks)

    expected_good = spark_session.createDataFrame([[1, 3, 3], [None, 4, None]], schema)
    assert_df_equality(good, expected_good)

    expected_bad = spark_session.createDataFrame(
        [
            [
                2,
                None,
                4,
                {"col_b_is_null_or_empty": "Column b is null or empty"},
                {"col_a_value_is_not_in_the_list": "Value 2 is not in the allowed list: [1, 3, 4]"},
            ],
            [
                None,
                4,
                None,
                None,
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
            [
                None,
                None,
                None,
                {"col_b_is_null_or_empty": "Column b is null or empty"},
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
        ],
        checked_schema,
    )

    assert_df_equality(bad, expected_bad, ignore_nullable=True)


def test_apply_checks_by_metadata(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3], [2, None, 4], [None, 4, None], [None, None, None]], schema)

    checks = [
        {
            "criticality": "warn",
            "check": {"function": "col_is_not_null_and_not_empty", "arguments": {"col_names": ["a", "c"]}},
        },
        {
            "criticality": "error",
            "check": {"function": "col_is_not_null_and_not_empty", "arguments": {"col_name": "b"}},
        },
        {
            "criticality": "warn",
            "check": {"function": "col_value_is_in_list", "arguments": {"col_names": ["a", "c"], "allowed": [1, 3, 4]}},
        },
    ]

    checked = apply_checks_by_metadata(test_df, checks)

    expected = spark_session.createDataFrame(
        [
            [1, 3, 3, None, None],
            [
                2,
                None,
                4,
                {"col_b_is_null_or_empty": "Column b is null or empty"},
                {"col_a_value_is_not_in_the_list": "Value 2 is not in the allowed list: [1, 3, 4]"},
            ],
            [
                None,
                4,
                None,
                None,
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
            [
                None,
                None,
                None,
                {"col_b_is_null_or_empty": "Column b is null or empty"},
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
        ],
        checked_schema,
    )

    assert_df_equality(checked, expected, ignore_nullable=True)


def col_test_check_func(col_name: str) -> Column:
    cl = F.col(col_name)
    condition = cl.isNull() | (cl == "") | (cl == "null")
    return make_condition_col(condition, "new check failed", f"{col_name}_is_null_or_empty")


def test_apply_checks_by_metadata_with_func_defined_outside_framework(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3], [2, None, 4], [None, 4, None], [None, None, None]], schema)

    checks = [{"criticality": "warn", "check": {"function": "col_test_check_func", "arguments": {"col_name": "a"}}}]

    checked = apply_checks_by_metadata(test_df, checks, globals())

    expected = spark_session.createDataFrame(
        [
            [1, 3, 3, None, None],
            [2, None, 4, None, None],
            [None, 4, None, None, {"col_a_is_null_or_empty": "new check failed"}],
            [None, None, None, None, {"col_a_is_null_or_empty": "new check failed"}],
        ],
        checked_schema,
    )

    assert_df_equality(checked, expected, ignore_nullable=True)
