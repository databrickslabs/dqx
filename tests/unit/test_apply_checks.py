from pathlib import Path
import pyspark.sql.functions as F
from chispa.dataframe_comparer import assert_df_equality  # type: ignore
from pyspark.sql import Column, SparkSession
from databricks.labs.dqx.col_functions import is_not_null_and_not_empty, make_condition
from databricks.labs.dqx.engine import (
    DQRule,
    apply_checks,
    apply_checks_and_split,
    apply_checks_by_metadata,
    apply_checks_by_metadata_and_split,
    load_checks_from_local_file,
)

SCHEMA = "a: int, b: int, c: int"
EXPECTED_SCHEMA = SCHEMA + ", _errors: map<string,string>, _warnings: map<string,string>"


def test_apply_checks_on_empty_checks(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, None], [2, 4, None]], SCHEMA)

    good = apply_checks(test_df, [])

    expected_df = spark_session.createDataFrame([[1, 3, None, None, None], [2, 4, None, None, None]], EXPECTED_SCHEMA)
    assert_df_equality(good, expected_df)


def test_apply_checks_and_split_on_empty_checks(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, None], [2, 4, None]], SCHEMA)

    good, bad = apply_checks_and_split(test_df, [])

    expected_df = spark_session.createDataFrame([], EXPECTED_SCHEMA)

    assert_df_equality(good, test_df)
    assert_df_equality(bad, expected_df)


def test_apply_checks_passed(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3]], SCHEMA)

    checks = [
        DQRule(name="col_a_is_null_or_empty", criticality="warn", check=is_not_null_and_not_empty("a")),
        DQRule(name="col_b_is_null_or_empty", criticality="error", check=is_not_null_and_not_empty("b")),
    ]

    checked = apply_checks(test_df, checks)

    expected = spark_session.createDataFrame([[1, 3, 3, None, None]], EXPECTED_SCHEMA)

    assert_df_equality(checked, expected, ignore_nullable=True)


def test_apply_checks(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3], [2, None, 4], [None, 4, None], [None, None, None]], SCHEMA)

    checks = [
        DQRule(name="col_a_is_null_or_empty", criticality="warn", check=is_not_null_and_not_empty("a")),
        DQRule(name="col_b_is_null_or_empty", criticality="error", check=is_not_null_and_not_empty("b")),
        DQRule(name="col_c_is_null_or_empty", criticality="error", check=is_not_null_and_not_empty("c")),
    ]

    checked = apply_checks(test_df, checks)

    expected = spark_session.createDataFrame(
        [
            [1, 3, 3, None, None],
            [2, None, 4, {"col_b_is_null_or_empty": "Column b is null or empty"}, None],
            [
                None,
                4,
                None,
                {"col_c_is_null_or_empty": "Column c is null or empty"},
                {"col_a_is_null_or_empty": "Column a is null or empty"},
            ],
            [
                None,
                None,
                None,
                {
                    "col_b_is_null_or_empty": "Column b is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
                {"col_a_is_null_or_empty": "Column a is null or empty"},
            ],
        ],
        EXPECTED_SCHEMA,
    )

    assert_df_equality(checked, expected, ignore_nullable=True)


def test_apply_checks_with_autogenerated_col_names(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3], [2, None, 4], [None, 4, None], [None, None, None]], SCHEMA)

    checks = [
        DQRule(criticality="warn", check=is_not_null_and_not_empty("a")),
        DQRule(criticality="error", check=is_not_null_and_not_empty("b")),
        DQRule(criticality="error", check=is_not_null_and_not_empty("c")),
    ]

    checked = apply_checks(test_df, checks)

    expected = spark_session.createDataFrame(
        [
            [1, 3, 3, None, None],
            [2, None, 4, {"col_b_is_null_or_empty": "Column b is null or empty"}, None],
            [
                None,
                4,
                None,
                {"col_c_is_null_or_empty": "Column c is null or empty"},
                {"col_a_is_null_or_empty": "Column a is null or empty"},
            ],
            [
                None,
                None,
                None,
                {
                    "col_b_is_null_or_empty": "Column b is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
                {"col_a_is_null_or_empty": "Column a is null or empty"},
            ],
        ],
        EXPECTED_SCHEMA,
    )

    assert_df_equality(checked, expected, ignore_nullable=True)


def test_apply_checks_and_split(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3], [2, None, 4], [None, 4, None], [None, None, None]], SCHEMA)

    checks = [
        DQRule(name="col_a_is_null_or_empty", criticality="warn", check=is_not_null_and_not_empty("a")),
        DQRule(name="col_b_is_null_or_empty", criticality="error", check=is_not_null_and_not_empty("b")),
        DQRule(name="col_c_is_null_or_empty", criticality="warn", check=is_not_null_and_not_empty("c")),
    ]

    good, bad = apply_checks_and_split(test_df, checks)

    expected_good = spark_session.createDataFrame([[1, 3, 3], [None, 4, None]], SCHEMA)
    assert_df_equality(good, expected_good)

    expected_bad = spark_session.createDataFrame(
        [
            [2, None, 4, {"col_b_is_null_or_empty": "Column b is null or empty"}, None],
            [
                None,
                4,
                None,
                None,
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
            [
                None,
                None,
                None,
                {"col_b_is_null_or_empty": "Column b is null or empty"},
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
        ],
        EXPECTED_SCHEMA,
    )

    assert_df_equality(bad, expected_bad, ignore_nullable=True)


def test_apply_checks_and_split_by_metadata(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3], [2, None, 4], [None, 4, None], [None, None, None]], SCHEMA)

    checks = [
        {
            "name": "col_a_is_null_or_empty",
            "criticality": "warn",
            "check": {"function": "is_not_null_and_not_empty", "arguments": {"col_name": "a"}},
        },
        {
            "name": "col_b_is_null_or_empty",
            "criticality": "error",
            "check": {"function": "is_not_null_and_not_empty", "arguments": {"col_name": "b"}},
        },
        {
            "name": "col_c_is_null_or_empty",
            "criticality": "warn",
            "check": {"function": "is_not_null_and_not_empty", "arguments": {"col_name": "c"}},
        },
        {
            "name": "col_a_value_is_not_in_the_list",
            "criticality": "warn",
            "check": {"function": "value_is_in_list", "arguments": {"col_name": "a", "allowed": [1, 3, 4]}},
        },
        {
            "name": "col_c_value_is_not_in_the_list",
            "criticality": "warn",
            "check": {"function": "value_is_in_list", "arguments": {"col_name": "c", "allowed": [1, 3, 4]}},
        },
    ]

    good, bad = apply_checks_by_metadata_and_split(test_df, checks)

    expected_good = spark_session.createDataFrame([[1, 3, 3], [None, 4, None]], SCHEMA)
    assert_df_equality(good, expected_good)

    expected_bad = spark_session.createDataFrame(
        [
            [
                2,
                None,
                4,
                {"col_b_is_null_or_empty": "Column b is null or empty"},
                {"col_a_value_is_not_in_the_list": "Value 2 is not in the allowed list: [1, 3, 4]"},
            ],
            [
                None,
                4,
                None,
                None,
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
            [
                None,
                None,
                None,
                {"col_b_is_null_or_empty": "Column b is null or empty"},
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
        ],
        EXPECTED_SCHEMA,
    )

    assert_df_equality(bad, expected_bad, ignore_nullable=True)


def test_apply_checks_and_split_by_metadata_with_autogenerated_col_names(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3], [2, None, 4], [None, 4, None], [None, None, None]], SCHEMA)

    checks = [
        {
            "criticality": "warn",
            "check": {"function": "is_not_null_and_not_empty", "arguments": {"col_names": ["a", "c"]}},
        },
        {
            "criticality": "error",
            "check": {"function": "is_not_null_and_not_empty", "arguments": {"col_name": "b"}},
        },
        {
            "criticality": "warn",
            "check": {"function": "value_is_in_list", "arguments": {"col_names": ["a", "c"], "allowed": [1, 3, 4]}},
        },
    ]

    good, bad = apply_checks_by_metadata_and_split(test_df, checks)

    expected_good = spark_session.createDataFrame([[1, 3, 3], [None, 4, None]], SCHEMA)
    assert_df_equality(good, expected_good)

    expected_bad = spark_session.createDataFrame(
        [
            [
                2,
                None,
                4,
                {"col_b_is_null_or_empty": "Column b is null or empty"},
                {"col_a_value_is_not_in_the_list": "Value 2 is not in the allowed list: [1, 3, 4]"},
            ],
            [
                None,
                4,
                None,
                None,
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
            [
                None,
                None,
                None,
                {"col_b_is_null_or_empty": "Column b is null or empty"},
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
        ],
        EXPECTED_SCHEMA,
    )

    assert_df_equality(bad, expected_bad, ignore_nullable=True)


def test_apply_checks_by_metadata(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3], [2, None, 4], [None, 4, None], [None, None, None]], SCHEMA)

    checks = [
        {
            "criticality": "warn",
            "check": {"function": "is_not_null_and_not_empty", "arguments": {"col_names": ["a", "c"]}},
        },
        {
            "criticality": "error",
            "check": {"function": "is_not_null_and_not_empty", "arguments": {"col_name": "b"}},
        },
        {
            "criticality": "warn",
            "check": {"function": "value_is_in_list", "arguments": {"col_names": ["a", "c"], "allowed": [1, 3, 4]}},
        },
    ]

    checked = apply_checks_by_metadata(test_df, checks)

    expected = spark_session.createDataFrame(
        [
            [1, 3, 3, None, None],
            [
                2,
                None,
                4,
                {"col_b_is_null_or_empty": "Column b is null or empty"},
                {"col_a_value_is_not_in_the_list": "Value 2 is not in the allowed list: [1, 3, 4]"},
            ],
            [
                None,
                4,
                None,
                None,
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
            [
                None,
                None,
                None,
                {"col_b_is_null_or_empty": "Column b is null or empty"},
                {
                    "col_a_is_null_or_empty": "Column a is null or empty",
                    "col_c_is_null_or_empty": "Column c is null or empty",
                },
            ],
        ],
        EXPECTED_SCHEMA,
    )

    assert_df_equality(checked, expected, ignore_nullable=True)


def test_apply_checks_from_json_file_by_metadata(spark_session: SparkSession):
    schema = "col1: int, col2: int, col3: int, col4 int"
    test_df = spark_session.createDataFrame([[1, 3, 3, 1], [2, None, 4, 1]], schema)

    base_path = str(Path(__file__).resolve().parent.parent)
    checks = load_checks_from_local_file(base_path + "/test_data/checks.json")

    actual = apply_checks_by_metadata(test_df, checks)

    expected = spark_session.createDataFrame(
        [[1, 3, 3, 1, None, None], [2, None, 4, 1, {"col_col2_is_null": "Column col2 is null"}, None]],
        schema + ", _errors: map<string,string>, _warnings: map<string,string>",
    )

    assert_df_equality(actual, expected, ignore_nullable=True)


def test_apply_checks_from_yml_file_by_metadata(spark_session: SparkSession):
    schema = "col1: int, col2: int, col3: int, col4 int"
    test_df = spark_session.createDataFrame([[1, 3, 3, 1], [2, None, 4, 1]], schema)

    base_path = str(Path(__file__).resolve().parent.parent)
    checks = load_checks_from_local_file(base_path + "/test_data/checks.yml")

    actual = apply_checks_by_metadata(test_df, checks)

    expected = spark_session.createDataFrame(
        [[1, 3, 3, 1, None, None], [2, None, 4, 1, {"col_col2_is_null": "Column col2 is null"}, None]],
        schema + ", _errors: map<string,string>, _warnings: map<string,string>",
    )

    assert_df_equality(actual, expected, ignore_nullable=True)


def col_test_check_func(col_name: str) -> Column:
    check_col = F.col(col_name)
    condition = check_col.isNull() | (check_col == "") | (check_col == "null")
    return make_condition(condition, "new check failed", f"{col_name}_is_null_or_empty")


def test_apply_checks_by_metadata_with_func_defined_outside_framework(spark_session: SparkSession):
    test_df = spark_session.createDataFrame([[1, 3, 3], [2, None, 4], [None, 4, None], [None, None, None]], SCHEMA)

    checks = [{"criticality": "warn", "check": {"function": "col_test_check_func", "arguments": {"col_name": "a"}}}]

    checked = apply_checks_by_metadata(test_df, checks, globals())

    expected = spark_session.createDataFrame(
        [
            [1, 3, 3, None, None],
            [2, None, 4, None, None],
            [None, 4, None, None, {"col_a_is_null_or_empty": "new check failed"}],
            [None, None, None, None, {"col_a_is_null_or_empty": "new check failed"}],
        ],
        EXPECTED_SCHEMA,
    )

    assert_df_equality(checked, expected, ignore_nullable=True)
